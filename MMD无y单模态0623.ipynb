{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 18:24:55.110017: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-23 18:24:55.152159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-23 18:24:55.826511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib.ticker as ticker\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys, Descriptors, AllChem\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect as Morgan\n",
    "from rdkit.Chem.QED import qed\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.Chem import RDConfig\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "import sascorer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_vocab(file, args):\n",
    "    \"\"\"\n",
    "    Load vocabulary and labels，并添加智能分词功能\n",
    "    \"\"\"\n",
    "    f = open(file)\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "    lines = [l.split() for l in lines]\n",
    "    lines = [l for l in lines]\n",
    "    labels = [l[1:] for l in lines][1:]\n",
    "\n",
    "    if args.grammar == 'smiles':\n",
    "        vocab={'#': 0, '(': 1, ')': 2, '-': 3, '.': 4, '/': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '=': 13, 'B': 14, 'Br': 15, 'C': 16, 'Cl': 17, 'E': 18, 'F': 19, 'I': 20, 'N': 21, 'O': 22, 'P': 23, 'S': 24, 'X': 25, '[ Co +2]': 26, '[ Co +]': 27, '[ Cr ]': 28, '[ Fe ]': 29, '[ Ir +]': 30, '[ Ni +2]': 31, '[ Pd +2]': 32, '[ Pd +]': 33, '[ Rh +]': 34, '[ Rh ]': 35, '[ Ru +2]': 36, '[ Ru +]': 37, '[B-1]': 38, '[B-]': 39, '[BH3-]': 40, '[C@@H]': 41, '[C@@]': 42, '[C@H]': 43, '[C@]': 44, '[C]': 45, '[H]': 46, '[K+]': 47, '[N+]': 48, '[N-1]': 49, '[N-]': 50, '[NH3+]': 51, '[N]': 52, '[Na+]': 53, '[O-1]': 54, '[O-]': 55, '[P+]': 56, '[P@@]': 57, '[P@H]': 58, '[P@]': 59, '[PH+]': 60, '[PH]': 61, '[S@@]': 62, '[S@]': 63, '[Si]': 64, '[c-]': 65, '[n+]': 66, '[nH]': 67, '\\\\': 68, 'c': 69, 'n': 70, 'o': 71, 'p': 72, 's': 73,'*': 74}\n",
    "\n",
    "        \n",
    "        # 添加无空格版本的金属元素token\n",
    "        for token, idx in list(vocab.items()):\n",
    "            if isinstance(token, str) and token.startswith('[') and ' ' in token:\n",
    "                no_space_token = token.replace(' ', '')\n",
    "                if no_space_token not in vocab:\n",
    "                    vocab[no_space_token] = idx\n",
    "    else:\n",
    "        raise ValueError(\"Unknown grammar type\")\n",
    "    \n",
    "    chars = list(vocab)\n",
    "    chars += ('E',)  # End of smiles末尾添加 E\n",
    "    chars += ('X',)  # Start of smiles开头添加 X\n",
    "    \n",
    "    # 定义SMILES分词函数\n",
    "    def tokenize(smiles):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        smiles = smiles.strip()\n",
    "        while i < len(smiles):\n",
    "            # 优先匹配方括号里的整个 token\n",
    "            if smiles[i] == '[':\n",
    "                m = re.match(r\"\\[[^\\[\\]]+\\]\", smiles[i:])\n",
    "                if m:\n",
    "                    tok = m.group(0)\n",
    "                    tokens.append(tok)\n",
    "                    i += len(tok)\n",
    "                    continue\n",
    "            # 尝试两字符 token（如 Cl、Br）\n",
    "            if i + 1 < len(smiles) and smiles[i:i+2] in vocab:\n",
    "                tokens.append(smiles[i:i+2])\n",
    "                i += 2\n",
    "                continue\n",
    "            # 单字符 token\n",
    "            if smiles[i] in vocab:\n",
    "                tokens.append(smiles[i])\n",
    "                i += 1\n",
    "                continue\n",
    "            # 未知字符，打 warning 并跳过\n",
    "            print(f\"警告: '{smiles[i]}' 不在词汇表中。\")\n",
    "            i += 1\n",
    "        return tokens\n",
    "    \n",
    "    # token 列表 → 索引列表\n",
    "    def map_tokens_to_ids(tokens):\n",
    "        return [vocab.get(tok, vocab['X']) for tok in tokens]\n",
    "\n",
    "    return vocab, chars, labels, tokenize, map_tokens_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_smiles_to_sequence(smiles_list, vocab, max_len=165):\n",
    "    \"\"\"\n",
    "    将 SMILES 字符串映射为对应的符号序列，并填充到统一长度\n",
    "    使用更智能的分词方法处理SMILES\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    # 如果没有指定最大长度，则计算 SMILES 中的最大长度\n",
    "    if max_len is None:\n",
    "        max_len = max(len(smiles) for smiles in smiles_list)\n",
    "    \n",
    "    # 定义SMILES分词函数\n",
    "    def tokenize(smiles):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        smiles = smiles.strip()\n",
    "        while i < len(smiles):\n",
    "            # 优先匹配方括号里的整个 token\n",
    "            if smiles[i] == '[':\n",
    "                m = re.match(r\"\\[[^\\[\\]]+\\]\", smiles[i:])\n",
    "                if m:\n",
    "                    tok = m.group(0)\n",
    "                    tokens.append(tok)\n",
    "                    i += len(tok)\n",
    "                    continue\n",
    "            # 尝试两字符 token（如 Cl、Br）\n",
    "            if i + 1 < len(smiles) and smiles[i:i+2] in vocab:\n",
    "                tokens.append(smiles[i:i+2])\n",
    "                i += 2\n",
    "                continue\n",
    "            # 单字符 token\n",
    "            if smiles[i] in vocab:\n",
    "                tokens.append(smiles[i])\n",
    "                i += 1\n",
    "                continue\n",
    "            # 未知字符，打 warning 并跳过\n",
    "            print(f\"警告: '{smiles[i]}' 不在词汇表中。\")\n",
    "            tokens.append('X')  # 使用'X'替代未知字符\n",
    "            i += 1\n",
    "        return tokens\n",
    "    \n",
    "    # 遍历每个 SMILES 字符串\n",
    "    for smiles in smiles_list:\n",
    "        # 分词\n",
    "        tokens = tokenize(smiles)\n",
    "        \n",
    "        # 将token映射为ID\n",
    "        sequence = []\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                sequence.append(vocab[token])\n",
    "            else:\n",
    "                # 如果token不在词汇表中，尝试查找无空格版本\n",
    "                no_space_token = token.replace(' ', '')\n",
    "                if no_space_token in vocab:\n",
    "                    sequence.append(vocab[no_space_token])\n",
    "                else:\n",
    "                    print(f\"警告: {token} 不在词汇表中。\")\n",
    "                    sequence.append(vocab['X'])  # 使用'X'替代未知token\n",
    "        \n",
    "        # 填充序列到 max_len\n",
    "        sequence += [vocab['E']] * (max_len - len(sequence))  # 使用'E'填充至最大长度\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mol_from_smiles(smiles):\n",
    "    \"\"\"\n",
    "    This function takes a list of smiles and returns a list of molecules\n",
    "    \"\"\"\n",
    "    \n",
    "    mols = [Chem.MolFromSmiles(s) for s in smiles]\n",
    "    mols = [m for m in mols if m is not None]\n",
    "    return mols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smiles_from_mol(mols):\n",
    "    \"\"\"\n",
    "    This function takes a list of molecules and returns a list of smiles\n",
    "    \"\"\"\n",
    "    smiles = [Chem.MolToSmiles(m, canonical=True) for m in mols]\n",
    "    \n",
    "    return smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "def load_and_map_smiles(file, args):\n",
    "    \"\"\"\n",
    "    读取 CSV 文件并将 SMILES 映射为符号序列，同时计算每个 SMILES 对应的分子量\n",
    "    支持 Sub1 和 Sub2 两个替代分子列\n",
    "    \"\"\"\n",
    "    # 读取 CSV\n",
    "    df = pd.read_csv(file)\n",
    "    smiles_list = df['SMILES'].tolist()\n",
    "    \n",
    "    # 加载词表和字符、标签信息以及分词函数\n",
    "    vocab, chars, labels, tokenize, map_tokens_to_ids = load_vocab(file, args)\n",
    "    \n",
    "    # 将主 SMILES 映射为符号序列\n",
    "    sequences = []\n",
    "    for smiles in smiles_list:\n",
    "        tokens = tokenize(smiles)\n",
    "        ids = map_tokens_to_ids(tokens)\n",
    "        # 填充序列到统一长度\n",
    "        max_len = 165  # 或者其他想要的长度\n",
    "        ids += [vocab['E']] * (max_len - len(ids))\n",
    "        sequences.append(ids)\n",
    "    \n",
    "    # 计算每个 SMILES 的分子对象列表（可用于分子量等计算）\n",
    "    mols = mol_from_smiles(smiles_list)\n",
    "    \n",
    "    # 读取 Sub1、Sub2 和 OUTPUT\n",
    "    sub1_sequences, sub2_sequences, output_values = None, None, None\n",
    "    if {'Sub1', 'Sub2', 'OUTPUT'}.issubset(df.columns):\n",
    "        sub1_list = df['Sub1'].tolist()\n",
    "        sub2_list = df['Sub2'].tolist()\n",
    "        output_values = df['OUTPUT'].tolist()\n",
    "        \n",
    "        # 映射 Sub1 和 Sub2\n",
    "        sub1_sequences = []\n",
    "        for smiles in sub1_list:\n",
    "            tokens = tokenize(smiles)\n",
    "            ids = map_tokens_to_ids(tokens)\n",
    "            ids += [vocab['E']] * (max_len - len(ids))\n",
    "            sub1_sequences.append(ids)\n",
    "            \n",
    "        sub2_sequences = []\n",
    "        for smiles in sub2_list:\n",
    "            tokens = tokenize(smiles)\n",
    "            ids = map_tokens_to_ids(tokens)\n",
    "            ids += [vocab['E']] * (max_len - len(ids))\n",
    "            sub2_sequences.append(ids)\n",
    "    else:\n",
    "        missing = [col for col in ['Sub1', 'Sub2', 'OUTPUT'] if col not in df.columns]\n",
    "        print(f\"警告: CSV文件中缺少列: {', '.join(missing)}\")\n",
    "\n",
    "    # 转为 PyTorch Tensor\n",
    "    sequences_tensor = torch.tensor(sequences, dtype=torch.long)\n",
    "\n",
    "    # 返回主 SMILES 列表、主序列 Tensor、Sub1 、Sub2 、OUTPUT 值列表以及字符表\n",
    "    return smiles_list, sequences_tensor, sub1_sequences, sub2_sequences, output_values, chars, tokenize, map_tokens_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(smiles_list, sequences_tensor, sub1_seq, sub2_seq, output_values, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    将数据拆分为训练集和测试集，确保每个 SMILES 与其主序列、两个替代序列和输出值配对\n",
    "    接收替代序列列表而非 Tensor\n",
    "    \"\"\"\n",
    "    (train_smiles, test_smiles,\n",
    "     train_seq, test_seq,\n",
    "     train_sub1, test_sub1,\n",
    "     train_sub2, test_sub2,\n",
    "     train_output, test_output) = train_test_split(\n",
    "        smiles_list, sequences_tensor,\n",
    "        sub1_seq, sub2_seq,\n",
    "        output_values,\n",
    "        test_size=test_ratio,\n",
    "        random_state=42\n",
    "    )\n",
    "    return (train_smiles, test_smiles,\n",
    "            train_seq, test_seq,\n",
    "            train_sub1, test_sub1,\n",
    "            train_sub2, test_sub2,\n",
    "            train_output, test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 示例调用\n",
    "args = type('', (), {})()\n",
    "args.grammar = 'smiles'\n",
    "\n",
    "# 加载数据\n",
    "smiles_list, sequences_tensor, sub1_seq, sub2_seq, output_values, chars, tokenize, map_tokens_to_ids = load_and_map_smiles('AHO.csv', args)# 这个地方换成新数据集\n",
    "\n",
    "# 划分训练和测试集\n",
    "(train_smiles, test_smiles,\n",
    " train_seq, test_seq,\n",
    " train_sub1, test_sub1,\n",
    " train_sub2, test_sub2,\n",
    " train_output, test_output) = split_data(\n",
    "    smiles_list, sequences_tensor,\n",
    "    sub1_seq, sub2_seq,\n",
    "    output_values,\n",
    "    test_ratio=0.2\n",
    ")\n",
    "\n",
    "# # # 输出示例\n",
    "# # print(f\"训练集大小: {len(train_smiles)}, 测试集大小: {len(test_smiles)}\")\n",
    "# # for i in range(min(5, len(train_smiles))):\n",
    "# #     print(f\"SMILES: {train_smiles[i]}, 主序列: {train_seq[i]}, Sub1序列: {train_sub1[i]}, Sub2序列: {train_sub2[i]}, OUTPUT: {train_output[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def convert_to_smiles(vector, char):\n",
    "#     \"\"\"\n",
    "#     Convert a vector to a SMILES string\n",
    "#     \"\"\"\n",
    "#     list_char = list(char)\n",
    "#     vector = vector.astype(int)\n",
    "#     return \"\".join(map(lambda x: list_char[x], vector)).strip()\n",
    "\n",
    "def convert_to_smiles(vector, char):\n",
    "    \"\"\"\n",
    "    Convert a vector to a SMILES string\n",
    "    \"\"\"\n",
    "    list_char = list(char)\n",
    "    try:\n",
    "        vector = vector.numpy()\n",
    "    except AttributeError:\n",
    "        pass  # already a NumPy array\n",
    "    vector = vector.astype(int)\n",
    "    return \"\".join(map(lambda x: list_char[x], vector)).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.seq2seq import BasicDecoder, BahdanauAttention, AttentionWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, batch_size, unit_size, latent_size, vocab_size, n_rnn_layer, seq_length, embedding):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.unit_size = unit_size\n",
    "        self.latent_size = latent_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_rnn_layer = n_rnn_layer\n",
    "        self.seq_length = seq_length\n",
    "        self.embedding = embedding  # 共享 embedding 层（假设 X 和 C 都是 SMILES）\n",
    "\n",
    "        initializer = tf.keras.initializers.glorot_uniform(seed=42)\n",
    "        ker_reg = tf.keras.regularizers.l2(0.00001)\n",
    "\n",
    "        # 主序列编码器（X）\n",
    "        self.rnn_layer_encode = [\n",
    "            tf.keras.layers.LSTM(self.unit_size, return_sequences=True, return_state=True,\n",
    "                                 kernel_initializer=initializer, kernel_regularizer=ker_reg)\n",
    "            for _ in range(self.n_rnn_layer)\n",
    "        ]\n",
    "\n",
    "        # 条件序列编码器（C）\n",
    "        self.cond_encoder = tf.keras.layers.LSTM(self.unit_size, return_sequences=False, return_state=False,\n",
    "                                                 kernel_initializer=initializer, kernel_regularizer=ker_reg)\n",
    "\n",
    "        # 输出到均值/方差\n",
    "        self.dense_mean = tf.keras.layers.Dense(self.latent_size, name='mean',\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1),\n",
    "                                                kernel_regularizer=ker_reg)\n",
    "    def call(self, inputs_XCL, training=False):\n",
    "        # unpack inputs\n",
    "        X, C, L = inputs_XCL  # X, C: [batch, seq_len], L: [batch]\n",
    "\n",
    "        X = tf.convert_to_tensor(X.numpy(), dtype=tf.int32)  # 确保是整数类型\n",
    "        C = tf.convert_to_tensor(C.numpy(), dtype=tf.int32)  # 确保是整数类型\n",
    "        \n",
    "        # SMILES 嵌入（共享 embedding）\n",
    "        X_emb = self.embedding(X, training=training)       # shape: [batch, seq_len, emb_dim]\n",
    "        C_emb = self.embedding(C, training=training)       # shape: [batch, cond_seq_len, emb_dim]\n",
    "\n",
    "        # 条件 SMILES 编码\n",
    "        C_encoded = self.cond_encoder(C_emb, training=training)  # shape: [batch, unit_size]\n",
    "\n",
    "        # 将 C 的语义表示平铺到每个时间步\n",
    "        C_tiled = tf.tile(tf.expand_dims(C_encoded, 1), [1, self.seq_length, 1])  # [batch, seq_len, unit_size]\n",
    "\n",
    "        # 拼接条件向量到每个时间步\n",
    "        inputs = tf.concat([X_emb, C_tiled], axis=-1)  # [batch, seq_len, emb_dim + unit_size]\n",
    "\n",
    "        # 构建掩码（mask padding）\n",
    "        mask = tf.sequence_mask(L, maxlen=self.seq_length, dtype=tf.bool)\n",
    "\n",
    "        output_state = []\n",
    "        for i in range(self.n_rnn_layer):\n",
    "            inputs, *state_out = self.rnn_layer_encode[i](inputs, training=training, mask=mask)\n",
    "            output_state.append(state_out)\n",
    "\n",
    "        c, h = output_state[-1]\n",
    "\n",
    "        # 计算隐变量的均值\n",
    "        z_mean = self.dense_mean(h, training=training)\n",
    "\n",
    "        return z_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, batch_size, unit_size, latent_size, vocab_size, n_rnn_layer, embedding):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.unit_size = unit_size\n",
    "        self.latent_size = latent_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_rnn_layer = n_rnn_layer\n",
    "        self.embedding = embedding\n",
    "        # self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "        initializer = tf.keras.initializers.glorot_uniform(seed=42)\n",
    "        ker_reg = tf.keras.regularizers.l2(0.00001)\n",
    "\n",
    "        # 多层 LSTM 解码器\n",
    "        self.rnn_layer_decode = [\n",
    "            tf.keras.layers.LSTM(self.unit_size, return_sequences=True, return_state=True,\n",
    "                                 kernel_initializer=initializer, kernel_regularizer=ker_reg)\n",
    "            for _ in range(self.n_rnn_layer)\n",
    "        ]\n",
    "\n",
    "        # 条件 SMILES 序列编码器（和 encoder 的一致）\n",
    "        self.cond_encoder = tf.keras.layers.LSTM(\n",
    "            self.unit_size, return_sequences=False, return_state=False,\n",
    "            kernel_initializer=initializer, kernel_regularizer=ker_reg\n",
    "        )\n",
    "\n",
    "        # 输出层\n",
    "        self.dense = tf.keras.layers.Dense(self.vocab_size,\n",
    "                                           kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1),\n",
    "                                           kernel_regularizer=ker_reg)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def call(self, inputs_XZCL, state=None, training=False):\n",
    "        X, Z, C, L = inputs_XZCL  # X: SMILES序列，Z: latent，C: 条件SMILES，L: 序列长度\n",
    "\n",
    "        seq_length = tf.shape(X)[1]\n",
    "\n",
    "        # --- 嵌入 ---\n",
    "        X_emb = self.embedding(X, training=training)        # [batch, seq_len, emb_dim]\n",
    "        C_emb = self.embedding(C, training=training)        # [batch, cond_seq_len, emb_dim]\n",
    "\n",
    "        # --- 条件编码 ---\n",
    "        C_encoded = self.cond_encoder(C_emb, training=training)  # [batch, unit_size]\n",
    "        C_tiled = tf.tile(tf.expand_dims(C_encoded, 1), [1, seq_length, 1])  # [batch, seq_len, unit_size]\n",
    "\n",
    "        # --- Z 平铺 ---\n",
    "        Z = tf.tile(tf.expand_dims(Z, axis=1), [1, seq_length, 1])  # [batch, seq_len, latent_size]\n",
    "\n",
    "        # --- 拼接输入 ---\n",
    "        inputs = tf.concat([Z, X_emb, C_tiled], axis=-1)  # [batch, seq_len, latent + emb + cond]\n",
    "\n",
    "        # inputs = self.dropout(inputs, training=training)\n",
    "        mask = tf.sequence_mask(L, maxlen=seq_length, dtype=tf.bool)\n",
    "\n",
    "        # --- 解码 ---\n",
    "        output_state = []\n",
    "        if training:\n",
    "            for i in range(self.n_rnn_layer):\n",
    "                inputs, *state_out = self.rnn_layer_decode[i](inputs, training=training, mask=mask)\n",
    "                output_state.append(state_out)\n",
    "        else:\n",
    "            if state is None:\n",
    "                state = [[tf.zeros([tf.shape(X)[0], self.unit_size]), tf.zeros([tf.shape(X)[0], self.unit_size])]\n",
    "                         for _ in range(self.n_rnn_layer)]\n",
    "            for i in range(self.n_rnn_layer):\n",
    "                inputs, *state_out = self.rnn_layer_decode[i](inputs, training=training,\n",
    "                                                              initial_state=state[i], mask=mask)\n",
    "                output_state.append(state_out)\n",
    "\n",
    "        # --- 输出层 ---\n",
    "        X2 = tf.reshape(inputs, [-1, tf.shape(inputs)[-1]])\n",
    "        X3_logits = self.dense(X2, training=training)\n",
    "        X3_logits = tf.reshape(X3_logits, [-1, seq_length, tf.shape(X3_logits)[-1]])\n",
    "        X3 = self.softmax(X3_logits)\n",
    "\n",
    "        return X3, X3_logits, output_state\n",
    "    \n",
    "    def generate_random_sample(self, C, start_codon, char, max_len=165, return_smiles=True, stop_token_id=None):\n",
    "        \"\"\"\n",
    "        根据条件向量生成 SMILES 序列。\n",
    "\n",
    "        Args:\n",
    "            C: 条件向量，shape = [batch_size, cond_dim]\n",
    "            start_codon: 起始 token 的 ID，shape = [batch_size, 1]\n",
    "            char: id -> token 的列表（如 ['C', 'O', ...]）\n",
    "            max_len: 最长生成长度\n",
    "            return_smiles: 是否返回 SMILES 字符串\n",
    "            stop_token_id: 停止生成的 token ID（通常为 'E'）\n",
    "\n",
    "        Returns:\n",
    "            SMILES 序列列表或 token 序列\n",
    "        \"\"\"\n",
    "        batch_size = C.shape[0]\n",
    "        latent = tf.random.normal(shape=(batch_size, self.latent_size))  # 随机潜变量\n",
    "        X_pred = tf.cast(start_codon, tf.int32)  # 初始输入，形状 [batch, 1]\n",
    "        preds = []\n",
    "        state = None\n",
    "        L = tf.ones((batch_size,), dtype=tf.int32)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits, _, state = self([X_pred, latent, C, L], state=state, training=False)\n",
    "            last_logits = logits[:, -1, :]  # shape = [batch, vocab_size]\n",
    "            probs = tf.nn.softmax(last_logits)\n",
    "            sampled_token = tf.random.categorical(tf.math.log(probs), num_samples=1)\n",
    "            sampled_token = tf.squeeze(sampled_token, axis=-1)  # shape: [batch]\n",
    "            preds.append(sampled_token)\n",
    "            X_pred = tf.expand_dims(sampled_token, axis=1)\n",
    "\n",
    "        # 拼接生成序列\n",
    "        token_id_seqs = tf.stack(preds, axis=1).numpy()  # shape = [batch, max_len]\n",
    "\n",
    "        if not return_smiles:\n",
    "            return token_id_seqs\n",
    "\n",
    "        # 如果要求 SMILES 输出，进行解码 + 截断\n",
    "        generated_smiles = []\n",
    "        for seq in token_id_seqs:\n",
    "            if stop_token_id is not None:\n",
    "                try:\n",
    "                    end_idx = np.where(seq == stop_token_id)[0][0]\n",
    "                    seq = seq[:end_idx]\n",
    "                except IndexError:\n",
    "                    pass  # 没有 'E'，保留完整序列\n",
    "            if len(seq) == 0:\n",
    "                continue\n",
    "            smiles = convert_to_smiles(seq, char)\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                generated_smiles.append(smiles)\n",
    "\n",
    "        return generated_smiles\n",
    "    \n",
    "    def generate_topk_sampling(self, C, start_codon, char, k=5, max_len=165, return_smiles=True, stop_token_id=None):\n",
    "\n",
    "        batch_size = C.shape[0]\n",
    "        latent = tf.random.normal(shape=(batch_size, self.latent_size))  # 随机潜变量\n",
    "        X_pred = tf.cast(start_codon, tf.int32)  # 初始输入，形状 [batch, 1]\n",
    "        preds = []\n",
    "        state = None\n",
    "        L = tf.ones((batch_size,), dtype=tf.int32)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            logits, _, state = self([X_pred, latent, C, L], state=state, training=False)\n",
    "            last_logits = logits[:, -1, :]  # shape = [batch, vocab_size]\n",
    "            \n",
    "            # Top-k 采样\n",
    "            topk_values, topk_indices = tf.nn.top_k(last_logits, k=k)  # shape: [batch, k]\n",
    "            topk_probs = tf.nn.softmax(topk_values)  # 对 top-k 值进行 softmax\n",
    "            \n",
    "            # 从 top-k 中采样\n",
    "            sampled_indices = tf.random.categorical(tf.math.log(topk_probs), num_samples=1)  # shape: [batch, 1]\n",
    "            sampled_indices = tf.squeeze(sampled_indices, axis=-1)  # shape: [batch]\n",
    "            \n",
    "            # 获取实际的 token ID\n",
    "            sampled_token = tf.gather(topk_indices, sampled_indices, batch_dims=1)  # shape: [batch]\n",
    "            \n",
    "            preds.append(sampled_token)\n",
    "            X_pred = tf.expand_dims(sampled_token, axis=1)\n",
    "        \n",
    "        # 拼接生成序列\n",
    "        token_id_seqs = tf.stack(preds, axis=1).numpy()  # shape = [batch, max_len]\n",
    "        \n",
    "        if not return_smiles:\n",
    "            return token_id_seqs\n",
    "        \n",
    "        # 如果要求 SMILES 输出，进行解码 + 截断\n",
    "        generated_smiles = []\n",
    "        for seq in token_id_seqs:\n",
    "            if stop_token_id is not None:\n",
    "                try:\n",
    "                    end_idx = np.where(seq == stop_token_id)[0][0]\n",
    "                    seq = seq[:end_idx]\n",
    "                except IndexError:\n",
    "                    pass  # 没有 'E'，保留完整序列\n",
    "            if len(seq) == 0:\n",
    "                continue\n",
    "            smiles = convert_to_smiles(seq, char)\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                generated_smiles.append(smiles)\n",
    "        \n",
    "        return generated_smiles\n",
    "    \n",
    "    def generate_topp_sampling(self, C, start_codon, char, p=0.95, max_len=165, return_smiles=True, stop_token_id=None):\n",
    "        batch_size = C.shape[0]\n",
    "        latent = tf.random.normal(shape=(batch_size, self.latent_size))  \n",
    "        X_pred = tf.cast(start_codon, tf.int32)  \n",
    "        preds = []\n",
    "        state = None\n",
    "        L = tf.ones((batch_size,), dtype=tf.int32)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            logits, _, state = self([X_pred, latent, C, L], state=state, training=False)\n",
    "            last_logits = logits[:, -1, :]  \n",
    "            probs = tf.nn.softmax(last_logits)\n",
    "            \n",
    "            # 按概率降序排序\n",
    "            sorted_probs, sorted_indices = tf.nn.top_k(probs, k=tf.shape(probs)[-1])\n",
    "            \n",
    "            # 计算累积概率\n",
    "            cumulative_probs = tf.cumsum(sorted_probs, axis=1)\n",
    "            \n",
    "            # 创建掩码\n",
    "            mask = cumulative_probs <= p\n",
    "            \n",
    "            # 修改这里：先创建float类型的one-hot向量，再转换为bool\n",
    "            first_token_mask = tf.cast(\n",
    "                tf.one_hot(0, depth=tf.shape(mask)[1], dtype=tf.float32),\n",
    "                dtype=tf.bool\n",
    "            )\n",
    "            first_token_mask = tf.tile(tf.expand_dims(first_token_mask, 0), [batch_size, 1])\n",
    "            mask = tf.logical_or(mask, first_token_mask)\n",
    "            \n",
    "            # 应用掩码\n",
    "            filtered_probs = tf.where(mask, sorted_probs, 0.0)\n",
    "            \n",
    "            # 重新归一化\n",
    "            filtered_probs = filtered_probs / tf.reduce_sum(filtered_probs, axis=1, keepdims=True)\n",
    "            \n",
    "            # 从过滤后的概率中采样\n",
    "            sampled_indices = tf.random.categorical(tf.math.log(filtered_probs + 1e-8), num_samples=1)\n",
    "            sampled_indices = tf.squeeze(sampled_indices, axis=-1)\n",
    "            \n",
    "            # 获取实际的 token ID\n",
    "            sampled_token = tf.gather(sorted_indices, sampled_indices, batch_dims=1)\n",
    "            \n",
    "            preds.append(sampled_token)\n",
    "            X_pred = tf.expand_dims(sampled_token, axis=1)\n",
    "        \n",
    "        # 拼接生成序列\n",
    "        token_id_seqs = tf.stack(preds, axis=1).numpy()\n",
    "        \n",
    "        if not return_smiles:\n",
    "            return token_id_seqs\n",
    "        \n",
    "        # 转换为 SMILES\n",
    "        generated_smiles = []\n",
    "        for seq in token_id_seqs:\n",
    "            if stop_token_id is not None:\n",
    "                try:\n",
    "                    end_idx = np.where(seq == stop_token_id)[0][0]\n",
    "                    seq = seq[:end_idx]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            if len(seq) == 0:\n",
    "                continue\n",
    "            smiles = convert_to_smiles(seq, char)\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                generated_smiles.append(smiles)\n",
    "        \n",
    "        return generated_smiles\n",
    "        \n",
    "    # 贪婪生成\n",
    "    def generate(self, C, start_codon, seq_length=165):\n",
    "        # z = tf.keras.backend.random_normal(shape=(C.shape[0], self.latent_size), mean=0.0, stddev=1.0) # 从标准正态分布中随机采样潜在变量 z\n",
    "        z = tf.random.normal(shape=(C.shape[0], self.latent_size), mean=0.0, stddev=1.0) # 从标准正态分布中随机采样潜在变量 z\n",
    "        C = tf.cast(C, tf.int32)#将条件向量 C 转换为 float32 类型，确保与模型的其他部分兼容。\n",
    "        \n",
    "        \n",
    "        X_pred = tf.cast(start_codon, tf.int32)# start_codon是起始密码子\n",
    "        # X_pred = tf.expand_dims(tf.cast(start_codon, tf.int32), axis=1)\n",
    "\n",
    "        preds = []\n",
    "        state = None\n",
    "#         L = np.ones(shape=(C.shape[0],), dtype=np.int32)\n",
    "        L = tf.ones(shape=(tf.shape(C)[0],), dtype=tf.int32)\n",
    "        \n",
    "        for i in range(seq_length):\n",
    "\n",
    "            if i == 0:\n",
    "                Y_hat, _, state = self.call([X_pred, z, C, L], state=None, training=False) # traini·ng=False: 表示当前处于推理模式\n",
    "                Y_hat = tf.argmax(Y_hat, axis=-1)\n",
    "            else:\n",
    "                Y_hat, _, state = self.call([Y_hat, z, C, L], state=state, training=False)\n",
    "                Y_hat = tf.argmax(Y_hat, axis=-1)\n",
    "\n",
    "            preds.append(Y_hat)\n",
    "        \n",
    "        preds = tf.stack(preds, axis=1)\n",
    "        preds = tf.squeeze(preds, axis=2)\n",
    "        preds = tf.cast(preds, tf.int32)\n",
    "        return preds\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self,vocab):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = 512 #2048 initial; close to 100 for tuning\n",
    "        self.unit_size = 256\n",
    "        self.latent_size = 200 #200\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.n_rnn_layer = 3\n",
    "        self.seq_length = 165\n",
    "        self.lr = 1e-4 #1e-4 initial #1e-5 for tuning\n",
    "        self.kl_beta = 1 # 1 现在这个是mmd_loss的权重\n",
    "        self.vocab = vocab\n",
    "\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.latent_size, embeddings_initializer=tf.keras.initializers.random_uniform(minval=-0.1, maxval=0.1))\n",
    "        self.encoder = Encoder(self.batch_size, self.unit_size, self.latent_size, self.vocab_size, self.n_rnn_layer, self.seq_length,\n",
    "                                self.embedding)\n",
    "        self.decoder = Decoder(self.batch_size, self.unit_size, self.latent_size, self.vocab_size, self.n_rnn_layer, self.embedding)\n",
    "\n",
    "    def call(self, inputs, state=None, training=False):\n",
    "        X, C, L = inputs # 输入的X,C,L分别是SMILES,序列、条件向量、序列长度\n",
    "        z_mean = self.encoder([X, C, L], training=training)\n",
    "        # 编码器将输入的SMILES序列映射到潜在空间，得到潜在变量 z 及其均值 z_mean 和对数方差 z_logvar。\n",
    "        Y_hat, Y_hat_logits, state = self.decoder([X, z_mean, C, L], state=state, training=training)\n",
    "        # 解码器根据潜在变量 z 和条件向量 C 输出生成的分子序列预测 Y_hat 和对应的logits。\n",
    "        return Y_hat, Y_hat_logits, z_mean, state\n",
    "\n",
    "    def rbf_kernel(self, x, y, sigma=1.0):#高斯核函数，用于计算MMD\n",
    "        x_norm = tf.reduce_sum(tf.square(x), axis=1, keepdims=True)\n",
    "        y_norm = tf.reduce_sum(tf.square(y), axis=1, keepdims=True)\n",
    "        cross_term = tf.matmul(x, y, transpose_b=True)\n",
    "        dists = x_norm - 2 * cross_term + tf.transpose(y_norm)\n",
    "        return tf.exp(-dists / (2 * sigma ** 2))\n",
    "\n",
    "    def compute_mmd(self, z, prior_z=None, kernel='rbf', epsilon=1e-8):\n",
    "        \"\"\"计算编码器输出 z 与标准正态分布之间的 MMD\"\"\"\n",
    "        if prior_z is None:\n",
    "            prior_z = tf.random.normal(tf.shape(z))\n",
    "        Kxx = self.rbf_kernel(z, z)\n",
    "        Kyy = self.rbf_kernel(prior_z, prior_z)\n",
    "        Kxy = self.rbf_kernel(z, prior_z)\n",
    "    \n",
    "        mmd = tf.reduce_mean(Kxx) + tf.reduce_mean(Kyy) - 2 * tf.reduce_mean(Kxy)\n",
    "        return tf.maximum(mmd, epsilon) # 确保 MMD 至少为 epsilon，防止后续 log(0)\n",
    "   \n",
    "    def loss(self, Y, L, Y_hat, Y_hat_logits, z_mean):\n",
    "        weights = tf.sequence_mask(L, maxlen=self.seq_length, dtype=tf.float32)\n",
    "        z = z_mean\n",
    "        # print(f'Y_hat_logits shape: {Y_hat_logits.shape},Y shape: {Y.shape},weights shape: {weights.shape}')\n",
    "        \n",
    "        reconstruction_loss = tf.reduce_mean(tfa.seq2seq.sequence_loss(logits=Y_hat_logits, targets=Y, weights=weights))\n",
    "      \n",
    "        mmd_loss = self.compute_mmd(z_mean, prior_z=None, kernel='rbf', epsilon=1e-8)\n",
    "                \n",
    "        total_loss = reconstruction_loss + self.kl_beta*mmd_loss\n",
    "        # 最终的损失是重构损失和KL损失的加权和\n",
    "        return {'loss': total_loss, 'reconstruction': reconstruction_loss, 'mmd_loss': mmd_loss}   \n",
    "    \n",
    "\n",
    "    def train_batch(self, data):# 用于训练模型的一个批次数据\n",
    "        X, C, Y, L = data # X：由SMILES或者SELIES生成的序列\n",
    "        # Y: 真实的目标值（通常是分子的真实 SMILES 或 SELFIES 表示）。\n",
    "        X = tf.convert_to_tensor(X.numpy(), dtype=tf.int32)  # 确保是整数类型\n",
    "        Y = tf.convert_to_tensor(Y.numpy(), dtype=tf.int32)  # 确保是整数类型\n",
    "        # Y = tf.convert_to_tensor(Y, dtype=tf.string)\n",
    "        C = tf.convert_to_tensor(C, dtype=tf.float32)\n",
    "        L = tf.convert_to_tensor(L, dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:# 上下文管理器记录计算图中的操作，以便后续计算梯度。\n",
    "            Y_hat, Y_hat_logits, z_mean, state = self.call([X, C, L ], training=True)\n",
    "            # loss_batch 是一个字典，包含多个损失项\n",
    "            loss_batch = self.loss(Y, L, Y_hat, Y_hat_logits, z_mean)\n",
    "            # 计算正则化损失\n",
    "            loss_batch['Regularization'] = tf.reduce_mean(self.encoder.losses) + tf.reduce_mean(self.decoder.losses) # 编码器中的正则化损失（例如 L2 正则化）的平均值和解码器中的正则化损失的平均值\n",
    "            loss_batch['loss'] += loss_batch['Regularization'] # 将正则化损失添加到总损失中\n",
    "        gradients = tape.gradient(loss_batch['loss'], self.encoder.trainable_variables+self.decoder.trainable_variables) # 使用 tape.gradient 计算总损失对所有可训练变量的梯度\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.encoder.trainable_variables+self.decoder.trainable_variables)) # 使用优化器更新模型参数\n",
    "        return loss_batch # 现在这个loss_batch包括四项：loss总损失、reconstruction重构损失，KL 散度损失，正则化损失\n",
    "    \n",
    "    def train(self, dataset_train, dataset_val, epochs, patience):# patience: 早停机制的容忍度（连续多少轮验证损失不下降时停止训练）\n",
    "        history = {} # 一个字典，用于记录每轮训练和验证的损失值\n",
    "        best_val_loss = tf.constant(np.inf)\n",
    "        count = 0 # 记录连续验证损失未下降的轮数\n",
    "        X_train, Y_train, C_train, L_train = dataset_train # 训练数据\n",
    "        X_val, Y_val, C_val, L_val = dataset_val #  验证数据\n",
    "\n",
    "        if len(X_val) == 0:\n",
    "            raise ValueError(\"Validation dataset is empty. Please provide valid validation data.\")\n",
    "\n",
    "        from tqdm.auto import tqdm # 使用 tqdm 提供进度条功能，方便观察训练过程\n",
    "        for epoch in tqdm(range(epochs), desc='Epochs', leave=True):# 创建一个带有进度条的训练循环 设置进度条的描述信息为 \"Epochs\"。leave=True: 在训练完成后保留进度条\n",
    "            start = time.time() # 记录每轮训练的开始时间，用于计算每轮训练耗时\n",
    "            loss_batches = {} # 一个字典，用于存储每个批次的损失值\n",
    "\n",
    "            for i in range(len(X_train)// self.batch_size):\n",
    "                X_batch = X_train[i*self.batch_size:i*self.batch_size + self.batch_size]\n",
    "                Y_batch = Y_train[i*self.batch_size:i*self.batch_size + self.batch_size]\n",
    "                C_batch = C_train[i*self.batch_size:i*self.batch_size + self.batch_size]\n",
    "                L_batch = L_train[i*self.batch_size:i*self.batch_size + self.batch_size]\n",
    "\n",
    "                loss_batch = self.train_batch([X_batch, C_batch, Y_batch, L_batch])\n",
    "                for key, valu in loss_batch.items():\n",
    "                    loss_batches.setdefault(key, []).append(valu) #如果字典中不存在该键，则初始化为空列表。将当前批次的损失值追加到对应列表中。\n",
    "                \n",
    "            for i in range(len(X_val)//self.batch_size):\n",
    "                X_val_batch = X_val[i*self.batch_size:i*self.batch_size + self.batch_size]\n",
    "                Y_val_batch = Y_val[i*self.batch_size:i*self.batch_size + self.batch_size]\n",
    "                C_val_batch = C_val[i*self.batch_size:i*self.batch_size + self.batch_size]\n",
    "                L_val_batch = L_val[i*self.batch_size:i*self.batch_size + self.batch_size]\n",
    "                \n",
    "                Y_hat_val_batch, Y_hat_logits_batch, z_mean_val_batch, state_val_batch = self.call([X_val_batch, C_val_batch, L_val_batch], training=True)# add training true\n",
    "                \n",
    "                loss_val_batch = self.loss(Y_val_batch, L_val_batch, Y_hat_val_batch, Y_hat_logits_batch, z_mean_val_batch)# 调用 self.loss 方法计算验证损失\n",
    "                \n",
    "                \n",
    "                for key, valu in loss_val_batch.items():\n",
    "                    loss_batches.setdefault(key+'_val', []).append(valu)#将每个验证损失项存储到 loss_batches 字典中，键名加上后缀 _val 表示验证损失。\n",
    "\n",
    "            for key, valu in loss_batches.items():\n",
    "                history.setdefault(key, []).append(tf.reduce_mean(valu))# 对每个损失项取平均值，并将其添加到 history 字典中。\n",
    "\n",
    "            # print(\"Loss val batch:\", loss_val_batch)\n",
    "            # print(\"Loss batches:\", loss_batches)\n",
    "            #构造一条日志字符串 stampa，包含当前轮次、所有损失值以及训练耗时。\n",
    "            stampa = f'Epoch: {epoch + 1}/{epochs}, '\n",
    "            for key, valu in history.items():\n",
    "                stampa += f'{key}: {valu[-1].numpy():.4f}, '\n",
    "\n",
    "            stampa += f'Time: {time.time() - start:.2f}s'\n",
    "            print(stampa)\n",
    "\n",
    "            # 监控验证损失，并进行早停判断\n",
    "            if 'loss_val' in history and history['loss_val'][-1] < best_val_loss:\n",
    "                best_val_loss = history['loss_val'][-1]\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "    \n",
    "            # Early stopping\n",
    "            if count == patience:\n",
    "                print('Early stopping')\n",
    "                return history  # 如果触发早停机制，返回历史记录\n",
    "   \n",
    "        return history\n",
    "\n",
    "    \n",
    "    # def plot_loss(self, history, path):\n",
    "    def plot_loss(self, history):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history['loss'], label='Train')\n",
    "        plt.plot(history['loss_val'], label='Validation')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# def plot_loss_kl(self, history, path):\n",
    "    def plot_loss_mmd(self, history):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history['mmd_loss'], label='Train')\n",
    "        plt.plot(history['mmd_loss_val'], label='Validation')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('mmd_loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(args):\n",
    "    # 用于模型推理，也可在现有模型的基础上进行训练，也可以加载之前的权重并重新开始训练。\n",
    "    # 话可用于加载模型来评估性能\n",
    "    model = Model(args.vocab)\n",
    "    dummy_data = (tf.zeros((1, args.seq_length), dtype=tf.int32),\n",
    "                                tf.zeros((1, args.num_properties), dtype=tf.float32),\n",
    "                                tf.constant([args.seq_length]))\n",
    "    _ = model(dummy_data)\n",
    "\n",
    "    model.load_weights(str(args.save_dir) + f'/model_{args.grammar}_{args.model_type}_{args.target}_weights.h5')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 18:25:29.500180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22336 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:57:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([9069, 165]), C_train: (9069, 330)\n",
      "X_val:   torch.Size([2268, 165]),   C_val:   (2268, 330)\n"
     ]
    }
   ],
   "source": [
    "# 假设 seq_length 是序列的最大长度\n",
    "seq_length = train_seq.shape[1]\n",
    "\n",
    "# 将替代序列和输出转换为 numpy 数组\n",
    "train_sub1_arr = np.array(train_sub1, dtype=np.int32)  # (N, L_sub)\n",
    "train_sub2_arr = np.array(train_sub2, dtype=np.int32)  # (N, L_sub)\n",
    "test_sub1_arr  = np.array(test_sub1,  dtype=np.int32)  # (M, L_sub)\n",
    "test_sub2_arr  = np.array(test_sub2,  dtype=np.int32)  # (M, L_sub)\n",
    "# train_out_arr  = np.array(train_output, dtype=np.float32).reshape(-1, 1)  # (N,1)\n",
    "# test_out_arr   = np.array(test_output,  dtype=np.float32).reshape(-1, 1)  # (M,1)\n",
    "\n",
    "# 横向拼接 sub1, sub2, output\n",
    "# C_train_np = np.hstack((train_sub1_arr, train_sub2_arr, train_out_arr))  # (N, 2*L_sub+1)\n",
    "# C_val_np   = np.hstack((test_sub1_arr,  test_sub2_arr,  test_out_arr))   # (M, 2*L_sub+1)\n",
    "C_train_np = np.hstack((train_sub1_arr, train_sub2_arr))  # (N, 2*L_sub+1)\n",
    "C_val_np   = np.hstack((test_sub1_arr,  test_sub2_arr))   # (M, 2*L_sub+1)\n",
    "\n",
    "# 转成 TensorFlow 张量\n",
    "C_train = tf.convert_to_tensor(C_train_np, dtype=tf.float32)\n",
    "C_val   = tf.convert_to_tensor(C_val_np,   dtype=tf.float32)\n",
    "# --------- 修改 END ---------\n",
    "\n",
    "# X/Y/L 输入保持不变\n",
    "X_train = train_seq\n",
    "Y_train = train_seq\n",
    "L_train = np.full((len(train_seq),), seq_length, dtype=np.int32)\n",
    "\n",
    "X_val = test_seq\n",
    "Y_val = test_seq\n",
    "L_val = np.full((len(test_seq),), seq_length, dtype=np.int32)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, C_train: {C_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape},   C_val:   {C_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab={'#': 0, '(': 1, ')': 2, '-': 3, '.': 4, '/': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '=': 13, 'B': 14, 'Br': 15, 'C': 16, 'Cl': 17, 'E': 18, 'F': 19, 'I': 20, 'N': 21, 'O': 22, 'P': 23, 'S': 24, 'X': 25, '[ Co +2]': 26, '[ Co +]': 27, '[ Cr ]': 28, '[ Fe ]': 29, '[ Ir +]': 30, '[ Ni +2]': 31, '[ Pd +2]': 32, '[ Pd +]': 33, '[ Rh +]': 34, '[ Rh ]': 35, '[ Ru +2]': 36, '[ Ru +]': 37, '[B-1]': 38, '[B-]': 39, '[BH3-]': 40, '[C@@H]': 41, '[C@@]': 42, '[C@H]': 43, '[C@]': 44, '[C]': 45, '[H]': 46, '[K+]': 47, '[N+]': 48, '[N-1]': 49, '[N-]': 50, '[NH3+]': 51, '[N]': 52, '[Na+]': 53, '[O-1]': 54, '[O-]': 55, '[P+]': 56, '[P@@]': 57, '[P@H]': 58, '[P@]': 59, '[PH+]': 60, '[PH]': 61, '[S@@]': 62, '[S@]': 63, '[Si]': 64, '[c-]': 65, '[n+]': 66, '[nH]': 67, '\\\\': 68, 'c': 69, 'n': 70, 'o': 71, 'p': 72, 's': 73,'*': 74}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大值: 73.0\n",
      "最小值: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"最大值:\", np.max(C_val))\n",
    "print(\"最小值:\", np.min(C_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Y_train = tf.convert_to_tensor(Y_train, dtype=tf.string)\n",
    "Y_train = tf.convert_to_tensor(Y_train.numpy(), dtype=tf.int32)\n",
    "C_train = tf.convert_to_tensor(C_train, dtype=tf.float32)\n",
    "L_train = tf.convert_to_tensor(L_train, dtype=tf.int32)\n",
    "Y_val = tf.convert_to_tensor(Y_val.numpy(), dtype=tf.int32)\n",
    "C_val = tf.convert_to_tensor(C_val, dtype=tf.float32)\n",
    "L_val = tf.convert_to_tensor(L_val, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005839824676513672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epochs",
       "rate": null,
       "total": 300,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66a7f0ef4544cd58f1894c508cb1e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 18:27:10.641418: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b86a41be00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-06-23 18:27:10.641439: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2025-06-23 18:27:10.660778: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-06-23 18:27:10.920727: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f64c8221550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f64c8221550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch: 1/300, loss: 4.1349, reconstruction: 3.1364, mmd_loss: 0.9899, Regularization: 0.0086, loss_val: 2.9806, reconstruction_val: 2.0034, mmd_loss_val: 0.9772, Time: 11.65s\n",
      "Epoch: 2/300, loss: 2.8222, reconstruction: 1.8384, mmd_loss: 0.9752, Regularization: 0.0086, loss_val: 2.6080, reconstruction_val: 1.6774, mmd_loss_val: 0.9305, Time: 6.92s\n",
      "Epoch: 3/300, loss: 2.4298, reconstruction: 1.5837, mmd_loss: 0.8375, Regularization: 0.0086, loss_val: 1.6032, reconstruction_val: 1.4651, mmd_loss_val: 0.1381, Time: 7.00s\n",
      "Epoch: 4/300, loss: 1.3504, reconstruction: 1.2716, mmd_loss: 0.0702, Regularization: 0.0086, loss_val: 1.1951, reconstruction_val: 1.1402, mmd_loss_val: 0.0549, Time: 6.94s\n",
      "Epoch: 5/300, loss: 1.1122, reconstruction: 1.0485, mmd_loss: 0.0552, Regularization: 0.0086, loss_val: 1.0471, reconstruction_val: 0.9953, mmd_loss_val: 0.0518, Time: 7.06s\n",
      "Epoch: 6/300, loss: 1.0315, reconstruction: 0.9700, mmd_loss: 0.0530, Regularization: 0.0086, loss_val: 0.9796, reconstruction_val: 0.9280, mmd_loss_val: 0.0516, Time: 6.71s\n",
      "Epoch: 7/300, loss: 0.9979, reconstruction: 0.9371, mmd_loss: 0.0523, Regularization: 0.0086, loss_val: 0.9677, reconstruction_val: 0.9189, mmd_loss_val: 0.0488, Time: 6.97s\n",
      "Epoch: 8/300, loss: 0.9797, reconstruction: 0.9209, mmd_loss: 0.0503, Regularization: 0.0086, loss_val: 0.9536, reconstruction_val: 0.9079, mmd_loss_val: 0.0457, Time: 7.06s\n",
      "Epoch: 9/300, loss: 0.9630, reconstruction: 0.9057, mmd_loss: 0.0487, Regularization: 0.0086, loss_val: 0.9367, reconstruction_val: 0.8909, mmd_loss_val: 0.0458, Time: 6.96s\n",
      "Epoch: 10/300, loss: 0.9611, reconstruction: 0.9044, mmd_loss: 0.0482, Regularization: 0.0086, loss_val: 0.9363, reconstruction_val: 0.8863, mmd_loss_val: 0.0500, Time: 6.89s\n",
      "Epoch: 11/300, loss: 0.9549, reconstruction: 0.8981, mmd_loss: 0.0483, Regularization: 0.0085, loss_val: 0.9245, reconstruction_val: 0.8774, mmd_loss_val: 0.0471, Time: 6.73s\n",
      "Epoch: 12/300, loss: 0.9466, reconstruction: 0.8899, mmd_loss: 0.0481, Regularization: 0.0085, loss_val: 0.9167, reconstruction_val: 0.8707, mmd_loss_val: 0.0460, Time: 6.95s\n",
      "Epoch: 13/300, loss: 0.9384, reconstruction: 0.8818, mmd_loss: 0.0480, Regularization: 0.0085, loss_val: 0.9098, reconstruction_val: 0.8633, mmd_loss_val: 0.0465, Time: 7.01s\n",
      "Epoch: 14/300, loss: 0.9430, reconstruction: 0.8866, mmd_loss: 0.0479, Regularization: 0.0085, loss_val: 0.9131, reconstruction_val: 0.8672, mmd_loss_val: 0.0459, Time: 6.91s\n",
      "Epoch: 15/300, loss: 0.9274, reconstruction: 0.8726, mmd_loss: 0.0463, Regularization: 0.0085, loss_val: 0.9035, reconstruction_val: 0.8582, mmd_loss_val: 0.0453, Time: 7.02s\n",
      "Epoch: 16/300, loss: 0.9323, reconstruction: 0.8790, mmd_loss: 0.0448, Regularization: 0.0085, loss_val: 0.9030, reconstruction_val: 0.8559, mmd_loss_val: 0.0470, Time: 6.93s\n",
      "Epoch: 17/300, loss: 0.9231, reconstruction: 0.8683, mmd_loss: 0.0463, Regularization: 0.0085, loss_val: 0.8982, reconstruction_val: 0.8524, mmd_loss_val: 0.0458, Time: 7.14s\n",
      "Epoch: 18/300, loss: 0.9252, reconstruction: 0.8726, mmd_loss: 0.0441, Regularization: 0.0085, loss_val: 0.8945, reconstruction_val: 0.8500, mmd_loss_val: 0.0445, Time: 7.13s\n",
      "Epoch: 19/300, loss: 0.9154, reconstruction: 0.8636, mmd_loss: 0.0433, Regularization: 0.0085, loss_val: 0.8956, reconstruction_val: 0.8516, mmd_loss_val: 0.0441, Time: 6.82s\n",
      "Epoch: 20/300, loss: 0.9168, reconstruction: 0.8643, mmd_loss: 0.0440, Regularization: 0.0085, loss_val: 0.8919, reconstruction_val: 0.8517, mmd_loss_val: 0.0402, Time: 6.77s\n",
      "Epoch: 21/300, loss: 0.9159, reconstruction: 0.8644, mmd_loss: 0.0430, Regularization: 0.0085, loss_val: 0.8981, reconstruction_val: 0.8557, mmd_loss_val: 0.0424, Time: 7.05s\n",
      "Epoch: 22/300, loss: 0.9066, reconstruction: 0.8564, mmd_loss: 0.0417, Regularization: 0.0085, loss_val: 0.8900, reconstruction_val: 0.8468, mmd_loss_val: 0.0432, Time: 7.09s\n",
      "Epoch: 23/300, loss: 0.9187, reconstruction: 0.8677, mmd_loss: 0.0425, Regularization: 0.0085, loss_val: 0.8797, reconstruction_val: 0.8410, mmd_loss_val: 0.0388, Time: 7.08s\n",
      "Epoch: 24/300, loss: 0.9062, reconstruction: 0.8573, mmd_loss: 0.0403, Regularization: 0.0085, loss_val: 0.9032, reconstruction_val: 0.8613, mmd_loss_val: 0.0418, Time: 7.14s\n",
      "Epoch: 25/300, loss: 0.9021, reconstruction: 0.8541, mmd_loss: 0.0396, Regularization: 0.0085, loss_val: 0.8808, reconstruction_val: 0.8431, mmd_loss_val: 0.0377, Time: 7.23s\n",
      "Epoch: 26/300, loss: 0.9020, reconstruction: 0.8554, mmd_loss: 0.0381, Regularization: 0.0085, loss_val: 0.8919, reconstruction_val: 0.8535, mmd_loss_val: 0.0384, Time: 6.72s\n",
      "Epoch: 27/300, loss: 0.9027, reconstruction: 0.8555, mmd_loss: 0.0387, Regularization: 0.0085, loss_val: 0.8769, reconstruction_val: 0.8408, mmd_loss_val: 0.0361, Time: 6.98s\n",
      "Epoch: 28/300, loss: 0.8994, reconstruction: 0.8541, mmd_loss: 0.0368, Regularization: 0.0085, loss_val: 0.8738, reconstruction_val: 0.8378, mmd_loss_val: 0.0360, Time: 6.65s\n",
      "Epoch: 29/300, loss: 0.8925, reconstruction: 0.8494, mmd_loss: 0.0346, Regularization: 0.0085, loss_val: 0.8720, reconstruction_val: 0.8360, mmd_loss_val: 0.0359, Time: 6.91s\n",
      "Epoch: 30/300, loss: 0.8939, reconstruction: 0.8499, mmd_loss: 0.0355, Regularization: 0.0085, loss_val: 0.8666, reconstruction_val: 0.8299, mmd_loss_val: 0.0367, Time: 6.89s\n",
      "Epoch: 31/300, loss: 0.8891, reconstruction: 0.8483, mmd_loss: 0.0323, Regularization: 0.0085, loss_val: 0.8693, reconstruction_val: 0.8345, mmd_loss_val: 0.0348, Time: 7.03s\n",
      "Epoch: 32/300, loss: 0.8924, reconstruction: 0.8486, mmd_loss: 0.0354, Regularization: 0.0085, loss_val: 0.8664, reconstruction_val: 0.8328, mmd_loss_val: 0.0337, Time: 6.94s\n",
      "Epoch: 33/300, loss: 0.8815, reconstruction: 0.8398, mmd_loss: 0.0333, Regularization: 0.0085, loss_val: 0.8626, reconstruction_val: 0.8304, mmd_loss_val: 0.0322, Time: 7.09s\n",
      "Epoch: 34/300, loss: 0.8902, reconstruction: 0.8492, mmd_loss: 0.0326, Regularization: 0.0085, loss_val: 0.8616, reconstruction_val: 0.8295, mmd_loss_val: 0.0320, Time: 6.90s\n",
      "Epoch: 35/300, loss: 0.8815, reconstruction: 0.8406, mmd_loss: 0.0325, Regularization: 0.0084, loss_val: 0.8717, reconstruction_val: 0.8415, mmd_loss_val: 0.0302, Time: 7.00s\n",
      "Epoch: 36/300, loss: 0.8790, reconstruction: 0.8377, mmd_loss: 0.0329, Regularization: 0.0084, loss_val: 0.8622, reconstruction_val: 0.8253, mmd_loss_val: 0.0369, Time: 7.10s\n",
      "Epoch: 37/300, loss: 0.8812, reconstruction: 0.8409, mmd_loss: 0.0318, Regularization: 0.0084, loss_val: 0.8561, reconstruction_val: 0.8235, mmd_loss_val: 0.0327, Time: 7.17s\n",
      "Epoch: 38/300, loss: 0.8797, reconstruction: 0.8396, mmd_loss: 0.0316, Regularization: 0.0084, loss_val: 0.8798, reconstruction_val: 0.8475, mmd_loss_val: 0.0323, Time: 7.09s\n",
      "Epoch: 39/300, loss: 0.8803, reconstruction: 0.8379, mmd_loss: 0.0340, Regularization: 0.0084, loss_val: 0.8552, reconstruction_val: 0.8229, mmd_loss_val: 0.0323, Time: 7.01s\n",
      "Epoch: 40/300, loss: 0.8728, reconstruction: 0.8324, mmd_loss: 0.0319, Regularization: 0.0084, loss_val: 0.8546, reconstruction_val: 0.8226, mmd_loss_val: 0.0320, Time: 6.87s\n",
      "Epoch: 41/300, loss: 0.8742, reconstruction: 0.8331, mmd_loss: 0.0327, Regularization: 0.0084, loss_val: 0.8453, reconstruction_val: 0.8144, mmd_loss_val: 0.0308, Time: 7.02s\n",
      "Epoch: 42/300, loss: 0.8718, reconstruction: 0.8312, mmd_loss: 0.0321, Regularization: 0.0084, loss_val: 0.8613, reconstruction_val: 0.8209, mmd_loss_val: 0.0404, Time: 6.67s\n",
      "Epoch: 43/300, loss: 0.8730, reconstruction: 0.8338, mmd_loss: 0.0308, Regularization: 0.0084, loss_val: 0.8681, reconstruction_val: 0.8381, mmd_loss_val: 0.0301, Time: 7.08s\n",
      "Epoch: 44/300, loss: 0.8739, reconstruction: 0.8350, mmd_loss: 0.0305, Regularization: 0.0084, loss_val: 0.8434, reconstruction_val: 0.8145, mmd_loss_val: 0.0289, Time: 6.90s\n",
      "Epoch: 45/300, loss: 0.8652, reconstruction: 0.8268, mmd_loss: 0.0300, Regularization: 0.0084, loss_val: 0.8567, reconstruction_val: 0.8296, mmd_loss_val: 0.0272, Time: 7.11s\n",
      "Epoch: 46/300, loss: 0.8624, reconstruction: 0.8237, mmd_loss: 0.0303, Regularization: 0.0084, loss_val: 0.8423, reconstruction_val: 0.8094, mmd_loss_val: 0.0330, Time: 6.98s\n",
      "Epoch: 47/300, loss: 0.8749, reconstruction: 0.8349, mmd_loss: 0.0316, Regularization: 0.0084, loss_val: 0.8516, reconstruction_val: 0.8217, mmd_loss_val: 0.0298, Time: 7.01s\n",
      "Epoch: 48/300, loss: 0.8655, reconstruction: 0.8267, mmd_loss: 0.0305, Regularization: 0.0084, loss_val: 0.8477, reconstruction_val: 0.8185, mmd_loss_val: 0.0293, Time: 6.92s\n",
      "Epoch: 49/300, loss: 0.8624, reconstruction: 0.8237, mmd_loss: 0.0303, Regularization: 0.0084, loss_val: 0.8541, reconstruction_val: 0.8213, mmd_loss_val: 0.0328, Time: 7.05s\n",
      "Epoch: 50/300, loss: 0.8633, reconstruction: 0.8252, mmd_loss: 0.0296, Regularization: 0.0084, loss_val: 0.8348, reconstruction_val: 0.8057, mmd_loss_val: 0.0292, Time: 6.79s\n",
      "Epoch: 51/300, loss: 0.8561, reconstruction: 0.8183, mmd_loss: 0.0293, Regularization: 0.0084, loss_val: 0.8385, reconstruction_val: 0.8102, mmd_loss_val: 0.0283, Time: 7.17s\n",
      "Epoch: 52/300, loss: 0.8636, reconstruction: 0.8268, mmd_loss: 0.0284, Regularization: 0.0084, loss_val: 0.8406, reconstruction_val: 0.8103, mmd_loss_val: 0.0303, Time: 6.85s\n",
      "Epoch: 53/300, loss: 0.8595, reconstruction: 0.8204, mmd_loss: 0.0307, Regularization: 0.0084, loss_val: 0.8477, reconstruction_val: 0.8181, mmd_loss_val: 0.0296, Time: 7.04s\n",
      "Epoch: 54/300, loss: 0.8595, reconstruction: 0.8215, mmd_loss: 0.0296, Regularization: 0.0084, loss_val: 0.8382, reconstruction_val: 0.8057, mmd_loss_val: 0.0325, Time: 6.93s\n",
      "Epoch: 55/300, loss: 0.8614, reconstruction: 0.8228, mmd_loss: 0.0302, Regularization: 0.0084, loss_val: 0.8344, reconstruction_val: 0.8010, mmd_loss_val: 0.0334, Time: 6.98s\n",
      "Epoch: 56/300, loss: 0.8517, reconstruction: 0.8120, mmd_loss: 0.0313, Regularization: 0.0084, loss_val: 0.8244, reconstruction_val: 0.7964, mmd_loss_val: 0.0280, Time: 7.00s\n",
      "Epoch: 57/300, loss: 0.8549, reconstruction: 0.8173, mmd_loss: 0.0292, Regularization: 0.0084, loss_val: 0.8288, reconstruction_val: 0.7958, mmd_loss_val: 0.0330, Time: 6.94s\n",
      "Epoch: 58/300, loss: 0.8470, reconstruction: 0.8101, mmd_loss: 0.0286, Regularization: 0.0084, loss_val: 0.8273, reconstruction_val: 0.7977, mmd_loss_val: 0.0296, Time: 7.06s\n",
      "Epoch: 59/300, loss: 0.8522, reconstruction: 0.8150, mmd_loss: 0.0289, Regularization: 0.0084, loss_val: 0.8221, reconstruction_val: 0.7942, mmd_loss_val: 0.0279, Time: 7.09s\n",
      "Epoch: 60/300, loss: 0.8500, reconstruction: 0.8123, mmd_loss: 0.0293, Regularization: 0.0084, loss_val: 0.8236, reconstruction_val: 0.7952, mmd_loss_val: 0.0284, Time: 7.09s\n",
      "Epoch: 61/300, loss: 0.8445, reconstruction: 0.8073, mmd_loss: 0.0289, Regularization: 0.0084, loss_val: 0.8399, reconstruction_val: 0.8120, mmd_loss_val: 0.0279, Time: 7.20s\n",
      "Epoch: 62/300, loss: 0.8380, reconstruction: 0.8011, mmd_loss: 0.0285, Regularization: 0.0084, loss_val: 0.8217, reconstruction_val: 0.7904, mmd_loss_val: 0.0312, Time: 7.12s\n",
      "Epoch: 63/300, loss: 0.8425, reconstruction: 0.8042, mmd_loss: 0.0299, Regularization: 0.0084, loss_val: 0.8219, reconstruction_val: 0.7901, mmd_loss_val: 0.0318, Time: 7.00s\n",
      "Epoch: 64/300, loss: 0.8613, reconstruction: 0.8235, mmd_loss: 0.0294, Regularization: 0.0084, loss_val: 0.8498, reconstruction_val: 0.8221, mmd_loss_val: 0.0277, Time: 6.95s\n",
      "Epoch: 65/300, loss: 0.8623, reconstruction: 0.8260, mmd_loss: 0.0279, Regularization: 0.0084, loss_val: 0.8412, reconstruction_val: 0.8112, mmd_loss_val: 0.0300, Time: 7.12s\n",
      "Epoch: 66/300, loss: 0.8429, reconstruction: 0.8065, mmd_loss: 0.0281, Regularization: 0.0084, loss_val: 0.8144, reconstruction_val: 0.7879, mmd_loss_val: 0.0265, Time: 6.91s\n",
      "Epoch: 67/300, loss: 0.8365, reconstruction: 0.7996, mmd_loss: 0.0285, Regularization: 0.0084, loss_val: 0.8154, reconstruction_val: 0.7881, mmd_loss_val: 0.0273, Time: 7.09s\n",
      "Epoch: 68/300, loss: 0.8289, reconstruction: 0.7925, mmd_loss: 0.0281, Regularization: 0.0084, loss_val: 0.8009, reconstruction_val: 0.7726, mmd_loss_val: 0.0283, Time: 6.99s\n",
      "Epoch: 69/300, loss: 0.8212, reconstruction: 0.7838, mmd_loss: 0.0291, Regularization: 0.0084, loss_val: 0.7939, reconstruction_val: 0.7669, mmd_loss_val: 0.0270, Time: 6.94s\n",
      "Epoch: 70/300, loss: 0.8164, reconstruction: 0.7803, mmd_loss: 0.0278, Regularization: 0.0084, loss_val: 0.7889, reconstruction_val: 0.7613, mmd_loss_val: 0.0276, Time: 7.11s\n",
      "Epoch: 71/300, loss: 0.8092, reconstruction: 0.7732, mmd_loss: 0.0276, Regularization: 0.0084, loss_val: 0.7822, reconstruction_val: 0.7541, mmd_loss_val: 0.0281, Time: 7.14s\n",
      "Epoch: 72/300, loss: 0.7985, reconstruction: 0.7624, mmd_loss: 0.0277, Regularization: 0.0084, loss_val: 0.7827, reconstruction_val: 0.7525, mmd_loss_val: 0.0302, Time: 6.87s\n",
      "Epoch: 73/300, loss: 0.8017, reconstruction: 0.7639, mmd_loss: 0.0294, Regularization: 0.0084, loss_val: 0.7690, reconstruction_val: 0.7419, mmd_loss_val: 0.0271, Time: 7.16s\n",
      "Epoch: 74/300, loss: 0.7846, reconstruction: 0.7489, mmd_loss: 0.0272, Regularization: 0.0084, loss_val: 0.7633, reconstruction_val: 0.7346, mmd_loss_val: 0.0288, Time: 7.04s\n",
      "Epoch: 75/300, loss: 0.7790, reconstruction: 0.7427, mmd_loss: 0.0279, Regularization: 0.0084, loss_val: 0.7646, reconstruction_val: 0.7364, mmd_loss_val: 0.0282, Time: 7.19s\n",
      "Epoch: 76/300, loss: 0.7672, reconstruction: 0.7309, mmd_loss: 0.0279, Regularization: 0.0084, loss_val: 0.7557, reconstruction_val: 0.7261, mmd_loss_val: 0.0296, Time: 6.97s\n",
      "Epoch: 77/300, loss: 0.7625, reconstruction: 0.7265, mmd_loss: 0.0275, Regularization: 0.0084, loss_val: 0.7368, reconstruction_val: 0.7102, mmd_loss_val: 0.0265, Time: 7.27s\n",
      "Epoch: 78/300, loss: 0.7453, reconstruction: 0.7092, mmd_loss: 0.0277, Regularization: 0.0084, loss_val: 0.7460, reconstruction_val: 0.7175, mmd_loss_val: 0.0285, Time: 7.15s\n",
      "Epoch: 79/300, loss: 0.7415, reconstruction: 0.7063, mmd_loss: 0.0267, Regularization: 0.0084, loss_val: 0.7064, reconstruction_val: 0.6823, mmd_loss_val: 0.0241, Time: 7.13s\n",
      "Epoch: 80/300, loss: 0.7252, reconstruction: 0.6891, mmd_loss: 0.0276, Regularization: 0.0084, loss_val: 0.6962, reconstruction_val: 0.6697, mmd_loss_val: 0.0265, Time: 7.15s\n",
      "Epoch: 81/300, loss: 0.7146, reconstruction: 0.6791, mmd_loss: 0.0271, Regularization: 0.0084, loss_val: 0.6888, reconstruction_val: 0.6610, mmd_loss_val: 0.0278, Time: 7.16s\n",
      "Epoch: 82/300, loss: 0.7011, reconstruction: 0.6648, mmd_loss: 0.0279, Regularization: 0.0084, loss_val: 0.6816, reconstruction_val: 0.6548, mmd_loss_val: 0.0268, Time: 7.16s\n",
      "Epoch: 83/300, loss: 0.6866, reconstruction: 0.6504, mmd_loss: 0.0278, Regularization: 0.0084, loss_val: 0.6590, reconstruction_val: 0.6303, mmd_loss_val: 0.0287, Time: 7.07s\n",
      "Epoch: 84/300, loss: 0.6680, reconstruction: 0.6317, mmd_loss: 0.0279, Regularization: 0.0084, loss_val: 0.6471, reconstruction_val: 0.6173, mmd_loss_val: 0.0298, Time: 7.17s\n",
      "Epoch: 85/300, loss: 0.6592, reconstruction: 0.6236, mmd_loss: 0.0271, Regularization: 0.0085, loss_val: 0.6280, reconstruction_val: 0.6013, mmd_loss_val: 0.0268, Time: 7.08s\n",
      "Epoch: 86/300, loss: 0.6374, reconstruction: 0.6016, mmd_loss: 0.0273, Regularization: 0.0085, loss_val: 0.6119, reconstruction_val: 0.5857, mmd_loss_val: 0.0263, Time: 7.06s\n",
      "Epoch: 87/300, loss: 0.6334, reconstruction: 0.5981, mmd_loss: 0.0268, Regularization: 0.0085, loss_val: 0.6045, reconstruction_val: 0.5757, mmd_loss_val: 0.0287, Time: 6.90s\n",
      "Epoch: 88/300, loss: 0.6111, reconstruction: 0.5753, mmd_loss: 0.0273, Regularization: 0.0085, loss_val: 0.5880, reconstruction_val: 0.5620, mmd_loss_val: 0.0260, Time: 6.68s\n",
      "Epoch: 89/300, loss: 0.5962, reconstruction: 0.5602, mmd_loss: 0.0275, Regularization: 0.0085, loss_val: 0.5751, reconstruction_val: 0.5487, mmd_loss_val: 0.0264, Time: 7.02s\n",
      "Epoch: 90/300, loss: 0.5823, reconstruction: 0.5479, mmd_loss: 0.0259, Regularization: 0.0085, loss_val: 0.5697, reconstruction_val: 0.5434, mmd_loss_val: 0.0262, Time: 6.92s\n",
      "Epoch: 91/300, loss: 0.5815, reconstruction: 0.5465, mmd_loss: 0.0265, Regularization: 0.0085, loss_val: 0.5560, reconstruction_val: 0.5271, mmd_loss_val: 0.0288, Time: 6.93s\n",
      "Epoch: 92/300, loss: 0.5646, reconstruction: 0.5292, mmd_loss: 0.0269, Regularization: 0.0085, loss_val: 0.5456, reconstruction_val: 0.5192, mmd_loss_val: 0.0264, Time: 7.01s\n",
      "Epoch: 93/300, loss: 0.5529, reconstruction: 0.5179, mmd_loss: 0.0264, Regularization: 0.0085, loss_val: 0.5302, reconstruction_val: 0.5043, mmd_loss_val: 0.0259, Time: 7.00s\n",
      "Epoch: 94/300, loss: 0.5395, reconstruction: 0.5048, mmd_loss: 0.0262, Regularization: 0.0085, loss_val: 0.5214, reconstruction_val: 0.4937, mmd_loss_val: 0.0277, Time: 6.92s\n",
      "Epoch: 95/300, loss: 0.5393, reconstruction: 0.5042, mmd_loss: 0.0265, Regularization: 0.0085, loss_val: 0.5161, reconstruction_val: 0.4904, mmd_loss_val: 0.0257, Time: 6.97s\n",
      "Epoch: 96/300, loss: 0.5227, reconstruction: 0.4881, mmd_loss: 0.0260, Regularization: 0.0085, loss_val: 0.5031, reconstruction_val: 0.4769, mmd_loss_val: 0.0262, Time: 7.12s\n",
      "Epoch: 97/300, loss: 0.5111, reconstruction: 0.4764, mmd_loss: 0.0262, Regularization: 0.0085, loss_val: 0.4919, reconstruction_val: 0.4663, mmd_loss_val: 0.0256, Time: 6.76s\n",
      "Epoch: 98/300, loss: 0.5036, reconstruction: 0.4690, mmd_loss: 0.0261, Regularization: 0.0086, loss_val: 0.4850, reconstruction_val: 0.4588, mmd_loss_val: 0.0262, Time: 6.78s\n",
      "Epoch: 99/300, loss: 0.4930, reconstruction: 0.4585, mmd_loss: 0.0259, Regularization: 0.0086, loss_val: 0.4764, reconstruction_val: 0.4474, mmd_loss_val: 0.0290, Time: 7.01s\n",
      "Epoch: 100/300, loss: 0.5000, reconstruction: 0.4647, mmd_loss: 0.0267, Regularization: 0.0086, loss_val: 0.4784, reconstruction_val: 0.4523, mmd_loss_val: 0.0262, Time: 6.86s\n",
      "Epoch: 101/300, loss: 0.4797, reconstruction: 0.4448, mmd_loss: 0.0263, Regularization: 0.0086, loss_val: 0.4625, reconstruction_val: 0.4374, mmd_loss_val: 0.0252, Time: 7.13s\n",
      "Epoch: 102/300, loss: 0.4700, reconstruction: 0.4343, mmd_loss: 0.0271, Regularization: 0.0086, loss_val: 0.4508, reconstruction_val: 0.4229, mmd_loss_val: 0.0279, Time: 7.08s\n",
      "Epoch: 103/300, loss: 0.4598, reconstruction: 0.4248, mmd_loss: 0.0264, Regularization: 0.0086, loss_val: 0.4485, reconstruction_val: 0.4225, mmd_loss_val: 0.0261, Time: 6.92s\n",
      "Epoch: 104/300, loss: 0.4509, reconstruction: 0.4162, mmd_loss: 0.0261, Regularization: 0.0086, loss_val: 0.4321, reconstruction_val: 0.4053, mmd_loss_val: 0.0268, Time: 7.14s\n",
      "Epoch: 105/300, loss: 0.4480, reconstruction: 0.4143, mmd_loss: 0.0252, Regularization: 0.0086, loss_val: 0.4455, reconstruction_val: 0.4161, mmd_loss_val: 0.0294, Time: 6.97s\n",
      "Epoch: 106/300, loss: 0.4424, reconstruction: 0.4083, mmd_loss: 0.0254, Regularization: 0.0086, loss_val: 0.4209, reconstruction_val: 0.3933, mmd_loss_val: 0.0276, Time: 7.09s\n",
      "Epoch: 107/300, loss: 0.4276, reconstruction: 0.3936, mmd_loss: 0.0254, Regularization: 0.0086, loss_val: 0.4102, reconstruction_val: 0.3841, mmd_loss_val: 0.0262, Time: 7.22s\n",
      "Epoch: 108/300, loss: 0.4185, reconstruction: 0.3855, mmd_loss: 0.0244, Regularization: 0.0086, loss_val: 0.4044, reconstruction_val: 0.3801, mmd_loss_val: 0.0243, Time: 6.99s\n",
      "Epoch: 109/300, loss: 0.4140, reconstruction: 0.3805, mmd_loss: 0.0249, Regularization: 0.0086, loss_val: 0.3977, reconstruction_val: 0.3736, mmd_loss_val: 0.0241, Time: 6.91s\n",
      "Epoch: 110/300, loss: 0.4088, reconstruction: 0.3753, mmd_loss: 0.0249, Regularization: 0.0086, loss_val: 0.3923, reconstruction_val: 0.3658, mmd_loss_val: 0.0264, Time: 7.20s\n",
      "Epoch: 111/300, loss: 0.4024, reconstruction: 0.3678, mmd_loss: 0.0260, Regularization: 0.0086, loss_val: 0.3834, reconstruction_val: 0.3573, mmd_loss_val: 0.0261, Time: 7.20s\n",
      "Epoch: 112/300, loss: 0.3928, reconstruction: 0.3595, mmd_loss: 0.0247, Regularization: 0.0086, loss_val: 0.3829, reconstruction_val: 0.3559, mmd_loss_val: 0.0270, Time: 7.08s\n",
      "Epoch: 113/300, loss: 0.4045, reconstruction: 0.3704, mmd_loss: 0.0254, Regularization: 0.0086, loss_val: 0.3902, reconstruction_val: 0.3629, mmd_loss_val: 0.0273, Time: 6.94s\n",
      "Epoch: 114/300, loss: 0.3884, reconstruction: 0.3534, mmd_loss: 0.0264, Regularization: 0.0086, loss_val: 0.3702, reconstruction_val: 0.3446, mmd_loss_val: 0.0256, Time: 7.02s\n",
      "Epoch: 115/300, loss: 0.3771, reconstruction: 0.3437, mmd_loss: 0.0248, Regularization: 0.0086, loss_val: 0.3601, reconstruction_val: 0.3361, mmd_loss_val: 0.0239, Time: 7.08s\n",
      "Epoch: 116/300, loss: 0.3706, reconstruction: 0.3370, mmd_loss: 0.0249, Regularization: 0.0087, loss_val: 0.3536, reconstruction_val: 0.3307, mmd_loss_val: 0.0230, Time: 7.19s\n",
      "Epoch: 117/300, loss: 0.3656, reconstruction: 0.3327, mmd_loss: 0.0242, Regularization: 0.0087, loss_val: 0.3496, reconstruction_val: 0.3263, mmd_loss_val: 0.0233, Time: 6.94s\n",
      "Epoch: 118/300, loss: 0.3615, reconstruction: 0.3280, mmd_loss: 0.0248, Regularization: 0.0087, loss_val: 0.3455, reconstruction_val: 0.3209, mmd_loss_val: 0.0246, Time: 7.13s\n",
      "Epoch: 119/300, loss: 0.3672, reconstruction: 0.3345, mmd_loss: 0.0240, Regularization: 0.0087, loss_val: 0.3446, reconstruction_val: 0.3193, mmd_loss_val: 0.0254, Time: 6.88s\n",
      "Epoch: 120/300, loss: 0.3539, reconstruction: 0.3211, mmd_loss: 0.0241, Regularization: 0.0087, loss_val: 0.3386, reconstruction_val: 0.3133, mmd_loss_val: 0.0253, Time: 7.13s\n",
      "Epoch: 121/300, loss: 0.3469, reconstruction: 0.3143, mmd_loss: 0.0239, Regularization: 0.0087, loss_val: 0.3315, reconstruction_val: 0.3093, mmd_loss_val: 0.0222, Time: 7.35s\n",
      "Epoch: 122/300, loss: 0.3422, reconstruction: 0.3098, mmd_loss: 0.0237, Regularization: 0.0087, loss_val: 0.3272, reconstruction_val: 0.3030, mmd_loss_val: 0.0242, Time: 7.09s\n",
      "Epoch: 123/300, loss: 0.3379, reconstruction: 0.3056, mmd_loss: 0.0236, Regularization: 0.0087, loss_val: 0.3235, reconstruction_val: 0.2989, mmd_loss_val: 0.0246, Time: 6.83s\n",
      "Epoch: 124/300, loss: 0.3420, reconstruction: 0.3084, mmd_loss: 0.0249, Regularization: 0.0087, loss_val: 0.3272, reconstruction_val: 0.2983, mmd_loss_val: 0.0289, Time: 7.03s\n",
      "Epoch: 125/300, loss: 0.3339, reconstruction: 0.3010, mmd_loss: 0.0242, Regularization: 0.0087, loss_val: 0.3206, reconstruction_val: 0.2974, mmd_loss_val: 0.0232, Time: 7.10s\n",
      "Epoch: 126/300, loss: 0.3310, reconstruction: 0.2985, mmd_loss: 0.0238, Regularization: 0.0087, loss_val: 0.3133, reconstruction_val: 0.2882, mmd_loss_val: 0.0251, Time: 7.00s\n",
      "Epoch: 127/300, loss: 0.3267, reconstruction: 0.2946, mmd_loss: 0.0234, Regularization: 0.0087, loss_val: 0.3106, reconstruction_val: 0.2885, mmd_loss_val: 0.0221, Time: 7.14s\n",
      "Epoch: 128/300, loss: 0.3197, reconstruction: 0.2877, mmd_loss: 0.0233, Regularization: 0.0087, loss_val: 0.3097, reconstruction_val: 0.2807, mmd_loss_val: 0.0290, Time: 7.17s\n",
      "Epoch: 129/300, loss: 0.3139, reconstruction: 0.2823, mmd_loss: 0.0229, Regularization: 0.0087, loss_val: 0.3075, reconstruction_val: 0.2771, mmd_loss_val: 0.0305, Time: 7.22s\n",
      "Epoch: 130/300, loss: 0.3113, reconstruction: 0.2790, mmd_loss: 0.0236, Regularization: 0.0087, loss_val: 0.2976, reconstruction_val: 0.2748, mmd_loss_val: 0.0228, Time: 7.08s\n",
      "Epoch: 131/300, loss: 0.3077, reconstruction: 0.2758, mmd_loss: 0.0232, Regularization: 0.0087, loss_val: 0.2921, reconstruction_val: 0.2705, mmd_loss_val: 0.0216, Time: 6.99s\n",
      "Epoch: 132/300, loss: 0.3045, reconstruction: 0.2735, mmd_loss: 0.0223, Regularization: 0.0087, loss_val: 0.3129, reconstruction_val: 0.2890, mmd_loss_val: 0.0239, Time: 7.17s\n",
      "Epoch: 133/300, loss: 0.3108, reconstruction: 0.2786, mmd_loss: 0.0235, Regularization: 0.0087, loss_val: 0.2903, reconstruction_val: 0.2660, mmd_loss_val: 0.0243, Time: 7.13s\n",
      "Epoch: 134/300, loss: 0.2999, reconstruction: 0.2666, mmd_loss: 0.0246, Regularization: 0.0087, loss_val: 0.2834, reconstruction_val: 0.2608, mmd_loss_val: 0.0226, Time: 7.08s\n",
      "Epoch: 135/300, loss: 0.2939, reconstruction: 0.2625, mmd_loss: 0.0227, Regularization: 0.0087, loss_val: 0.2822, reconstruction_val: 0.2577, mmd_loss_val: 0.0245, Time: 6.44s\n",
      "Epoch: 136/300, loss: 0.2904, reconstruction: 0.2592, mmd_loss: 0.0224, Regularization: 0.0087, loss_val: 0.2781, reconstruction_val: 0.2543, mmd_loss_val: 0.0238, Time: 6.99s\n",
      "Epoch: 137/300, loss: 0.2888, reconstruction: 0.2562, mmd_loss: 0.0238, Regularization: 0.0087, loss_val: 0.2751, reconstruction_val: 0.2527, mmd_loss_val: 0.0224, Time: 6.95s\n",
      "Epoch: 138/300, loss: 0.2937, reconstruction: 0.2621, mmd_loss: 0.0228, Regularization: 0.0087, loss_val: 0.2768, reconstruction_val: 0.2500, mmd_loss_val: 0.0268, Time: 6.99s\n",
      "Epoch: 139/300, loss: 0.2832, reconstruction: 0.2513, mmd_loss: 0.0232, Regularization: 0.0087, loss_val: 0.2680, reconstruction_val: 0.2459, mmd_loss_val: 0.0221, Time: 7.08s\n",
      "Epoch: 140/300, loss: 0.2783, reconstruction: 0.2473, mmd_loss: 0.0222, Regularization: 0.0087, loss_val: 0.2705, reconstruction_val: 0.2425, mmd_loss_val: 0.0279, Time: 7.08s\n",
      "Epoch: 141/300, loss: 0.2756, reconstruction: 0.2444, mmd_loss: 0.0225, Regularization: 0.0087, loss_val: 0.2615, reconstruction_val: 0.2399, mmd_loss_val: 0.0216, Time: 7.04s\n",
      "Epoch: 142/300, loss: 0.2717, reconstruction: 0.2415, mmd_loss: 0.0215, Regularization: 0.0087, loss_val: 0.2643, reconstruction_val: 0.2368, mmd_loss_val: 0.0275, Time: 7.12s\n",
      "Epoch: 143/300, loss: 0.2708, reconstruction: 0.2403, mmd_loss: 0.0217, Regularization: 0.0087, loss_val: 0.2949, reconstruction_val: 0.2722, mmd_loss_val: 0.0228, Time: 7.06s\n",
      "Epoch: 144/300, loss: 0.3040, reconstruction: 0.2727, mmd_loss: 0.0226, Regularization: 0.0087, loss_val: 0.2773, reconstruction_val: 0.2520, mmd_loss_val: 0.0253, Time: 7.08s\n",
      "Epoch: 145/300, loss: 0.2771, reconstruction: 0.2447, mmd_loss: 0.0237, Regularization: 0.0087, loss_val: 0.2585, reconstruction_val: 0.2336, mmd_loss_val: 0.0248, Time: 7.10s\n",
      "Epoch: 146/300, loss: 0.2665, reconstruction: 0.2333, mmd_loss: 0.0245, Regularization: 0.0087, loss_val: 0.2505, reconstruction_val: 0.2275, mmd_loss_val: 0.0230, Time: 6.84s\n",
      "Epoch: 147/300, loss: 0.2594, reconstruction: 0.2286, mmd_loss: 0.0221, Regularization: 0.0088, loss_val: 0.2435, reconstruction_val: 0.2242, mmd_loss_val: 0.0193, Time: 7.04s\n",
      "Epoch: 148/300, loss: 0.2505, reconstruction: 0.2260, mmd_loss: 0.0157, Regularization: 0.0088, loss_val: 0.2402, reconstruction_val: 0.2226, mmd_loss_val: 0.0176, Time: 6.95s\n",
      "Epoch: 149/300, loss: 0.2471, reconstruction: 0.2234, mmd_loss: 0.0150, Regularization: 0.0088, loss_val: 0.2338, reconstruction_val: 0.2188, mmd_loss_val: 0.0150, Time: 6.48s\n",
      "Epoch: 150/300, loss: 0.2434, reconstruction: 0.2199, mmd_loss: 0.0147, Regularization: 0.0088, loss_val: 0.2286, reconstruction_val: 0.2157, mmd_loss_val: 0.0130, Time: 7.05s\n",
      "Epoch: 151/300, loss: 0.2391, reconstruction: 0.2170, mmd_loss: 0.0133, Regularization: 0.0088, loss_val: 0.2256, reconstruction_val: 0.2128, mmd_loss_val: 0.0128, Time: 7.05s\n",
      "Epoch: 152/300, loss: 0.2365, reconstruction: 0.2139, mmd_loss: 0.0138, Regularization: 0.0088, loss_val: 0.2232, reconstruction_val: 0.2102, mmd_loss_val: 0.0130, Time: 6.90s\n",
      "Epoch: 153/300, loss: 0.2334, reconstruction: 0.2112, mmd_loss: 0.0134, Regularization: 0.0088, loss_val: 0.2208, reconstruction_val: 0.2071, mmd_loss_val: 0.0137, Time: 6.89s\n",
      "Epoch: 154/300, loss: 0.2312, reconstruction: 0.2089, mmd_loss: 0.0136, Regularization: 0.0088, loss_val: 0.2191, reconstruction_val: 0.2047, mmd_loss_val: 0.0144, Time: 6.96s\n",
      "Epoch: 155/300, loss: 0.2277, reconstruction: 0.2059, mmd_loss: 0.0130, Regularization: 0.0088, loss_val: 0.2140, reconstruction_val: 0.2022, mmd_loss_val: 0.0119, Time: 7.15s\n",
      "Epoch: 156/300, loss: 0.2245, reconstruction: 0.2032, mmd_loss: 0.0125, Regularization: 0.0088, loss_val: 0.2159, reconstruction_val: 0.2033, mmd_loss_val: 0.0126, Time: 7.12s\n",
      "Epoch: 157/300, loss: 0.2240, reconstruction: 0.2026, mmd_loss: 0.0126, Regularization: 0.0088, loss_val: 0.2120, reconstruction_val: 0.1991, mmd_loss_val: 0.0129, Time: 7.12s\n",
      "Epoch: 158/300, loss: 0.2199, reconstruction: 0.1983, mmd_loss: 0.0128, Regularization: 0.0088, loss_val: 0.2052, reconstruction_val: 0.1941, mmd_loss_val: 0.0111, Time: 7.12s\n",
      "Epoch: 159/300, loss: 0.2168, reconstruction: 0.1960, mmd_loss: 0.0120, Regularization: 0.0088, loss_val: 0.2044, reconstruction_val: 0.1917, mmd_loss_val: 0.0127, Time: 7.09s\n",
      "Epoch: 160/300, loss: 0.2136, reconstruction: 0.1930, mmd_loss: 0.0119, Regularization: 0.0088, loss_val: 0.2015, reconstruction_val: 0.1894, mmd_loss_val: 0.0121, Time: 6.75s\n",
      "Epoch: 161/300, loss: 0.2107, reconstruction: 0.1902, mmd_loss: 0.0118, Regularization: 0.0088, loss_val: 0.1990, reconstruction_val: 0.1862, mmd_loss_val: 0.0128, Time: 7.15s\n",
      "Epoch: 162/300, loss: 0.2082, reconstruction: 0.1875, mmd_loss: 0.0119, Regularization: 0.0088, loss_val: 0.1970, reconstruction_val: 0.1841, mmd_loss_val: 0.0129, Time: 6.99s\n",
      "Epoch: 163/300, loss: 0.2061, reconstruction: 0.1856, mmd_loss: 0.0117, Regularization: 0.0088, loss_val: 0.1933, reconstruction_val: 0.1812, mmd_loss_val: 0.0121, Time: 6.87s\n",
      "Epoch: 164/300, loss: 0.2038, reconstruction: 0.1835, mmd_loss: 0.0116, Regularization: 0.0088, loss_val: 0.1913, reconstruction_val: 0.1789, mmd_loss_val: 0.0124, Time: 6.86s\n",
      "Epoch: 165/300, loss: 0.2367, reconstruction: 0.2149, mmd_loss: 0.0130, Regularization: 0.0088, loss_val: 0.2567, reconstruction_val: 0.2357, mmd_loss_val: 0.0210, Time: 7.02s\n",
      "Epoch: 166/300, loss: 0.2489, reconstruction: 0.2230, mmd_loss: 0.0172, Regularization: 0.0088, loss_val: 0.2148, reconstruction_val: 0.2018, mmd_loss_val: 0.0130, Time: 6.87s\n",
      "Epoch: 167/300, loss: 0.2184, reconstruction: 0.1970, mmd_loss: 0.0125, Regularization: 0.0088, loss_val: 0.1979, reconstruction_val: 0.1860, mmd_loss_val: 0.0119, Time: 7.16s\n",
      "Epoch: 168/300, loss: 0.2048, reconstruction: 0.1841, mmd_loss: 0.0119, Regularization: 0.0088, loss_val: 0.1901, reconstruction_val: 0.1780, mmd_loss_val: 0.0121, Time: 7.22s\n",
      "Epoch: 169/300, loss: 0.1989, reconstruction: 0.1778, mmd_loss: 0.0123, Regularization: 0.0088, loss_val: 0.1854, reconstruction_val: 0.1733, mmd_loss_val: 0.0121, Time: 7.06s\n",
      "Epoch: 170/300, loss: 0.1948, reconstruction: 0.1740, mmd_loss: 0.0121, Regularization: 0.0088, loss_val: 0.1814, reconstruction_val: 0.1701, mmd_loss_val: 0.0114, Time: 7.15s\n",
      "Epoch: 171/300, loss: 0.1917, reconstruction: 0.1711, mmd_loss: 0.0117, Regularization: 0.0088, loss_val: 0.1784, reconstruction_val: 0.1675, mmd_loss_val: 0.0109, Time: 7.11s\n",
      "Epoch: 172/300, loss: 0.1891, reconstruction: 0.1686, mmd_loss: 0.0118, Regularization: 0.0088, loss_val: 0.1781, reconstruction_val: 0.1652, mmd_loss_val: 0.0129, Time: 7.11s\n",
      "Epoch: 173/300, loss: 0.1867, reconstruction: 0.1661, mmd_loss: 0.0117, Regularization: 0.0088, loss_val: 0.1751, reconstruction_val: 0.1628, mmd_loss_val: 0.0123, Time: 7.34s\n",
      "Epoch: 174/300, loss: 0.1854, reconstruction: 0.1639, mmd_loss: 0.0127, Regularization: 0.0088, loss_val: 0.1715, reconstruction_val: 0.1609, mmd_loss_val: 0.0107, Time: 7.09s\n",
      "Epoch: 175/300, loss: 0.1823, reconstruction: 0.1621, mmd_loss: 0.0114, Regularization: 0.0088, loss_val: 0.1733, reconstruction_val: 0.1593, mmd_loss_val: 0.0140, Time: 7.10s\n",
      "Epoch: 176/300, loss: 0.1805, reconstruction: 0.1601, mmd_loss: 0.0115, Regularization: 0.0088, loss_val: 0.1694, reconstruction_val: 0.1579, mmd_loss_val: 0.0115, Time: 7.02s\n",
      "Epoch: 177/300, loss: 0.1785, reconstruction: 0.1581, mmd_loss: 0.0115, Regularization: 0.0088, loss_val: 0.1650, reconstruction_val: 0.1545, mmd_loss_val: 0.0105, Time: 7.11s\n",
      "Epoch: 178/300, loss: 0.1755, reconstruction: 0.1555, mmd_loss: 0.0112, Regularization: 0.0088, loss_val: 0.1631, reconstruction_val: 0.1525, mmd_loss_val: 0.0106, Time: 6.80s\n",
      "Epoch: 179/300, loss: 0.1736, reconstruction: 0.1535, mmd_loss: 0.0113, Regularization: 0.0088, loss_val: 0.1613, reconstruction_val: 0.1509, mmd_loss_val: 0.0104, Time: 6.99s\n",
      "Epoch: 180/300, loss: 0.1716, reconstruction: 0.1516, mmd_loss: 0.0112, Regularization: 0.0088, loss_val: 0.1612, reconstruction_val: 0.1490, mmd_loss_val: 0.0122, Time: 7.00s\n",
      "Epoch: 181/300, loss: 0.1702, reconstruction: 0.1501, mmd_loss: 0.0113, Regularization: 0.0088, loss_val: 0.1590, reconstruction_val: 0.1471, mmd_loss_val: 0.0120, Time: 6.94s\n",
      "Epoch: 182/300, loss: 0.1687, reconstruction: 0.1482, mmd_loss: 0.0116, Regularization: 0.0088, loss_val: 0.1557, reconstruction_val: 0.1454, mmd_loss_val: 0.0103, Time: 7.03s\n",
      "Epoch: 183/300, loss: 0.1663, reconstruction: 0.1464, mmd_loss: 0.0111, Regularization: 0.0088, loss_val: 0.1545, reconstruction_val: 0.1436, mmd_loss_val: 0.0109, Time: 6.94s\n",
      "Epoch: 184/300, loss: 0.1645, reconstruction: 0.1445, mmd_loss: 0.0112, Regularization: 0.0088, loss_val: 0.1534, reconstruction_val: 0.1419, mmd_loss_val: 0.0115, Time: 6.88s\n",
      "Epoch: 185/300, loss: 0.1632, reconstruction: 0.1432, mmd_loss: 0.0111, Regularization: 0.0088, loss_val: 0.1527, reconstruction_val: 0.1403, mmd_loss_val: 0.0124, Time: 7.05s\n",
      "Epoch: 186/300, loss: 0.1622, reconstruction: 0.1420, mmd_loss: 0.0114, Regularization: 0.0088, loss_val: 0.1518, reconstruction_val: 0.1405, mmd_loss_val: 0.0113, Time: 6.95s\n",
      "Epoch: 187/300, loss: 0.1598, reconstruction: 0.1400, mmd_loss: 0.0110, Regularization: 0.0088, loss_val: 0.1482, reconstruction_val: 0.1378, mmd_loss_val: 0.0103, Time: 6.67s\n",
      "Epoch: 188/300, loss: 0.1605, reconstruction: 0.1401, mmd_loss: 0.0116, Regularization: 0.0088, loss_val: 0.1507, reconstruction_val: 0.1393, mmd_loss_val: 0.0114, Time: 6.82s\n",
      "Epoch: 189/300, loss: 0.1582, reconstruction: 0.1380, mmd_loss: 0.0113, Regularization: 0.0089, loss_val: 0.1469, reconstruction_val: 0.1357, mmd_loss_val: 0.0112, Time: 7.11s\n",
      "Epoch: 190/300, loss: 0.1554, reconstruction: 0.1357, mmd_loss: 0.0109, Regularization: 0.0089, loss_val: 0.1432, reconstruction_val: 0.1335, mmd_loss_val: 0.0097, Time: 6.94s\n",
      "Epoch: 191/300, loss: 0.1539, reconstruction: 0.1341, mmd_loss: 0.0110, Regularization: 0.0089, loss_val: 0.1444, reconstruction_val: 0.1338, mmd_loss_val: 0.0105, Time: 7.23s\n",
      "Epoch: 192/300, loss: 0.1907, reconstruction: 0.1705, mmd_loss: 0.0114, Regularization: 0.0089, loss_val: 0.1750, reconstruction_val: 0.1645, mmd_loss_val: 0.0105, Time: 7.15s\n",
      "Epoch: 193/300, loss: 0.1710, reconstruction: 0.1503, mmd_loss: 0.0118, Regularization: 0.0089, loss_val: 0.1496, reconstruction_val: 0.1387, mmd_loss_val: 0.0109, Time: 7.09s\n",
      "Epoch: 194/300, loss: 0.1560, reconstruction: 0.1360, mmd_loss: 0.0111, Regularization: 0.0089, loss_val: 0.1411, reconstruction_val: 0.1305, mmd_loss_val: 0.0106, Time: 7.06s\n",
      "Epoch: 195/300, loss: 0.1504, reconstruction: 0.1305, mmd_loss: 0.0111, Regularization: 0.0089, loss_val: 0.1388, reconstruction_val: 0.1273, mmd_loss_val: 0.0115, Time: 6.65s\n",
      "Epoch: 196/300, loss: 0.1481, reconstruction: 0.1278, mmd_loss: 0.0113, Regularization: 0.0089, loss_val: 0.1375, reconstruction_val: 0.1254, mmd_loss_val: 0.0121, Time: 6.82s\n",
      "Epoch: 197/300, loss: 0.1461, reconstruction: 0.1262, mmd_loss: 0.0111, Regularization: 0.0089, loss_val: 0.1350, reconstruction_val: 0.1238, mmd_loss_val: 0.0111, Time: 7.10s\n",
      "Epoch: 198/300, loss: 0.1444, reconstruction: 0.1246, mmd_loss: 0.0109, Regularization: 0.0089, loss_val: 0.1346, reconstruction_val: 0.1225, mmd_loss_val: 0.0121, Time: 6.99s\n",
      "Epoch: 199/300, loss: 0.1433, reconstruction: 0.1233, mmd_loss: 0.0112, Regularization: 0.0089, loss_val: 0.1322, reconstruction_val: 0.1212, mmd_loss_val: 0.0110, Time: 6.99s\n",
      "Epoch: 200/300, loss: 0.1416, reconstruction: 0.1219, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.1305, reconstruction_val: 0.1198, mmd_loss_val: 0.0107, Time: 6.99s\n",
      "Epoch: 201/300, loss: 0.1400, reconstruction: 0.1205, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.1295, reconstruction_val: 0.1186, mmd_loss_val: 0.0110, Time: 7.13s\n",
      "Epoch: 202/300, loss: 0.1393, reconstruction: 0.1192, mmd_loss: 0.0112, Regularization: 0.0089, loss_val: 0.1285, reconstruction_val: 0.1174, mmd_loss_val: 0.0112, Time: 6.96s\n",
      "Epoch: 203/300, loss: 0.1379, reconstruction: 0.1181, mmd_loss: 0.0109, Regularization: 0.0089, loss_val: 0.1276, reconstruction_val: 0.1162, mmd_loss_val: 0.0114, Time: 6.78s\n",
      "Epoch: 204/300, loss: 0.1363, reconstruction: 0.1168, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.1265, reconstruction_val: 0.1151, mmd_loss_val: 0.0115, Time: 7.00s\n",
      "Epoch: 205/300, loss: 0.1352, reconstruction: 0.1156, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.1236, reconstruction_val: 0.1139, mmd_loss_val: 0.0097, Time: 6.85s\n",
      "Epoch: 206/300, loss: 0.1342, reconstruction: 0.1144, mmd_loss: 0.0109, Regularization: 0.0089, loss_val: 0.1233, reconstruction_val: 0.1127, mmd_loss_val: 0.0106, Time: 7.04s\n",
      "Epoch: 207/300, loss: 0.1328, reconstruction: 0.1132, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.1218, reconstruction_val: 0.1116, mmd_loss_val: 0.0103, Time: 7.03s\n",
      "Epoch: 208/300, loss: 0.1316, reconstruction: 0.1121, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.1211, reconstruction_val: 0.1105, mmd_loss_val: 0.0106, Time: 6.90s\n",
      "Epoch: 209/300, loss: 0.1311, reconstruction: 0.1111, mmd_loss: 0.0110, Regularization: 0.0089, loss_val: 0.1208, reconstruction_val: 0.1097, mmd_loss_val: 0.0111, Time: 7.17s\n",
      "Epoch: 210/300, loss: 0.1298, reconstruction: 0.1104, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.1212, reconstruction_val: 0.1096, mmd_loss_val: 0.0116, Time: 7.19s\n",
      "Epoch: 211/300, loss: 0.1354, reconstruction: 0.1154, mmd_loss: 0.0112, Regularization: 0.0089, loss_val: 0.1214, reconstruction_val: 0.1093, mmd_loss_val: 0.0121, Time: 6.97s\n",
      "Epoch: 212/300, loss: 0.1312, reconstruction: 0.1111, mmd_loss: 0.0112, Regularization: 0.0089, loss_val: 0.1175, reconstruction_val: 0.1071, mmd_loss_val: 0.0105, Time: 7.14s\n",
      "Epoch: 213/300, loss: 0.1942, reconstruction: 0.1741, mmd_loss: 0.0111, Regularization: 0.0089, loss_val: 0.1850, reconstruction_val: 0.1736, mmd_loss_val: 0.0114, Time: 7.18s\n",
      "Epoch: 214/300, loss: 0.1672, reconstruction: 0.1456, mmd_loss: 0.0127, Regularization: 0.0089, loss_val: 0.1423, reconstruction_val: 0.1306, mmd_loss_val: 0.0117, Time: 7.00s\n",
      "Epoch: 215/300, loss: 0.1418, reconstruction: 0.1215, mmd_loss: 0.0114, Regularization: 0.0089, loss_val: 0.1257, reconstruction_val: 0.1143, mmd_loss_val: 0.0114, Time: 7.06s\n",
      "Epoch: 216/300, loss: 0.1323, reconstruction: 0.1123, mmd_loss: 0.0111, Regularization: 0.0089, loss_val: 0.1200, reconstruction_val: 0.1086, mmd_loss_val: 0.0114, Time: 6.71s\n",
      "Epoch: 217/300, loss: 0.1278, reconstruction: 0.1080, mmd_loss: 0.0109, Regularization: 0.0089, loss_val: 0.1162, reconstruction_val: 0.1054, mmd_loss_val: 0.0108, Time: 6.99s\n",
      "Epoch: 218/300, loss: 0.1252, reconstruction: 0.1055, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.1142, reconstruction_val: 0.1035, mmd_loss_val: 0.0107, Time: 7.14s\n",
      "Epoch: 219/300, loss: 0.1234, reconstruction: 0.1037, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.1132, reconstruction_val: 0.1019, mmd_loss_val: 0.0113, Time: 7.14s\n",
      "Epoch: 220/300, loss: 0.1217, reconstruction: 0.1022, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.1117, reconstruction_val: 0.1007, mmd_loss_val: 0.0111, Time: 6.87s\n",
      "Epoch: 221/300, loss: 0.1204, reconstruction: 0.1008, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.1099, reconstruction_val: 0.0994, mmd_loss_val: 0.0104, Time: 7.08s\n",
      "Epoch: 222/300, loss: 0.1194, reconstruction: 0.0996, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.1087, reconstruction_val: 0.0982, mmd_loss_val: 0.0104, Time: 7.05s\n",
      "Epoch: 223/300, loss: 0.1181, reconstruction: 0.0985, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.1069, reconstruction_val: 0.0971, mmd_loss_val: 0.0098, Time: 7.09s\n",
      "Epoch: 224/300, loss: 0.1170, reconstruction: 0.0974, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.1069, reconstruction_val: 0.0959, mmd_loss_val: 0.0110, Time: 7.05s\n",
      "Epoch: 225/300, loss: 0.1163, reconstruction: 0.0963, mmd_loss: 0.0111, Regularization: 0.0089, loss_val: 0.1052, reconstruction_val: 0.0948, mmd_loss_val: 0.0103, Time: 7.32s\n",
      "Epoch: 226/300, loss: 0.1147, reconstruction: 0.0952, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.1049, reconstruction_val: 0.0939, mmd_loss_val: 0.0110, Time: 6.11s\n",
      "Epoch: 227/300, loss: 0.1138, reconstruction: 0.0942, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.1034, reconstruction_val: 0.0930, mmd_loss_val: 0.0104, Time: 6.22s\n",
      "Epoch: 228/300, loss: 0.1129, reconstruction: 0.0933, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.1031, reconstruction_val: 0.0921, mmd_loss_val: 0.0110, Time: 6.51s\n",
      "Epoch: 229/300, loss: 0.1123, reconstruction: 0.0923, mmd_loss: 0.0111, Regularization: 0.0089, loss_val: 0.1016, reconstruction_val: 0.0911, mmd_loss_val: 0.0105, Time: 6.94s\n",
      "Epoch: 230/300, loss: 0.1108, reconstruction: 0.0913, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.1004, reconstruction_val: 0.0901, mmd_loss_val: 0.0103, Time: 7.01s\n",
      "Epoch: 231/300, loss: 0.1102, reconstruction: 0.0906, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.1011, reconstruction_val: 0.0898, mmd_loss_val: 0.0112, Time: 7.01s\n",
      "Epoch: 232/300, loss: 0.1095, reconstruction: 0.0900, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0985, reconstruction_val: 0.0885, mmd_loss_val: 0.0100, Time: 7.15s\n",
      "Epoch: 233/300, loss: 0.1086, reconstruction: 0.0889, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.1004, reconstruction_val: 0.0877, mmd_loss_val: 0.0127, Time: 7.11s\n",
      "Epoch: 234/300, loss: 0.1076, reconstruction: 0.0880, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.0978, reconstruction_val: 0.0868, mmd_loss_val: 0.0109, Time: 7.09s\n",
      "Epoch: 235/300, loss: 0.1067, reconstruction: 0.0870, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.0960, reconstruction_val: 0.0861, mmd_loss_val: 0.0099, Time: 7.06s\n",
      "Epoch: 236/300, loss: 0.1065, reconstruction: 0.0866, mmd_loss: 0.0110, Regularization: 0.0089, loss_val: 0.0975, reconstruction_val: 0.0863, mmd_loss_val: 0.0112, Time: 7.05s\n",
      "Epoch: 237/300, loss: 0.1084, reconstruction: 0.0886, mmd_loss: 0.0109, Regularization: 0.0089, loss_val: 0.0994, reconstruction_val: 0.0892, mmd_loss_val: 0.0102, Time: 7.02s\n",
      "Epoch: 238/300, loss: 0.1614, reconstruction: 0.1415, mmd_loss: 0.0109, Regularization: 0.0089, loss_val: 0.1305, reconstruction_val: 0.1201, mmd_loss_val: 0.0103, Time: 7.07s\n",
      "Epoch: 239/300, loss: 0.1304, reconstruction: 0.1104, mmd_loss: 0.0110, Regularization: 0.0089, loss_val: 0.1111, reconstruction_val: 0.1002, mmd_loss_val: 0.0109, Time: 6.93s\n",
      "Epoch: 240/300, loss: 0.1163, reconstruction: 0.0964, mmd_loss: 0.0109, Regularization: 0.0089, loss_val: 0.1023, reconstruction_val: 0.0917, mmd_loss_val: 0.0106, Time: 6.84s\n",
      "Epoch: 241/300, loss: 0.1100, reconstruction: 0.0901, mmd_loss: 0.0110, Regularization: 0.0089, loss_val: 0.0980, reconstruction_val: 0.0876, mmd_loss_val: 0.0105, Time: 6.94s\n",
      "Epoch: 242/300, loss: 0.1063, reconstruction: 0.0868, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0956, reconstruction_val: 0.0849, mmd_loss_val: 0.0106, Time: 6.67s\n",
      "Epoch: 243/300, loss: 0.1045, reconstruction: 0.0847, mmd_loss: 0.0108, Regularization: 0.0089, loss_val: 0.0948, reconstruction_val: 0.0833, mmd_loss_val: 0.0114, Time: 6.19s\n",
      "Epoch: 244/300, loss: 0.1029, reconstruction: 0.0832, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.0918, reconstruction_val: 0.0819, mmd_loss_val: 0.0099, Time: 6.93s\n",
      "Epoch: 245/300, loss: 0.1016, reconstruction: 0.0819, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.0925, reconstruction_val: 0.0810, mmd_loss_val: 0.0115, Time: 7.21s\n",
      "Epoch: 246/300, loss: 0.1006, reconstruction: 0.0810, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.0903, reconstruction_val: 0.0801, mmd_loss_val: 0.0102, Time: 6.93s\n",
      "Epoch: 247/300, loss: 0.0995, reconstruction: 0.0800, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0895, reconstruction_val: 0.0792, mmd_loss_val: 0.0103, Time: 6.94s\n",
      "Epoch: 248/300, loss: 0.0986, reconstruction: 0.0791, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0886, reconstruction_val: 0.0781, mmd_loss_val: 0.0105, Time: 7.04s\n",
      "Epoch: 249/300, loss: 0.0977, reconstruction: 0.0781, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.0880, reconstruction_val: 0.0772, mmd_loss_val: 0.0108, Time: 7.01s\n",
      "Epoch: 250/300, loss: 0.0969, reconstruction: 0.0772, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.0866, reconstruction_val: 0.0766, mmd_loss_val: 0.0100, Time: 7.11s\n",
      "Epoch: 251/300, loss: 0.0959, reconstruction: 0.0765, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0860, reconstruction_val: 0.0757, mmd_loss_val: 0.0103, Time: 7.06s\n",
      "Epoch: 252/300, loss: 0.0953, reconstruction: 0.0758, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0856, reconstruction_val: 0.0750, mmd_loss_val: 0.0106, Time: 6.73s\n",
      "Epoch: 253/300, loss: 0.0947, reconstruction: 0.0752, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0849, reconstruction_val: 0.0747, mmd_loss_val: 0.0102, Time: 6.78s\n",
      "Epoch: 254/300, loss: 0.0939, reconstruction: 0.0745, mmd_loss: 0.0104, Regularization: 0.0089, loss_val: 0.0842, reconstruction_val: 0.0738, mmd_loss_val: 0.0104, Time: 6.82s\n",
      "Epoch: 255/300, loss: 0.0932, reconstruction: 0.0738, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0837, reconstruction_val: 0.0730, mmd_loss_val: 0.0106, Time: 7.09s\n",
      "Epoch: 256/300, loss: 0.0926, reconstruction: 0.0731, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0833, reconstruction_val: 0.0723, mmd_loss_val: 0.0110, Time: 7.03s\n",
      "Epoch: 257/300, loss: 0.0920, reconstruction: 0.0724, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0829, reconstruction_val: 0.0718, mmd_loss_val: 0.0111, Time: 6.56s\n",
      "Epoch: 258/300, loss: 0.0911, reconstruction: 0.0716, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0811, reconstruction_val: 0.0712, mmd_loss_val: 0.0099, Time: 7.09s\n",
      "Epoch: 259/300, loss: 0.0904, reconstruction: 0.0710, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0812, reconstruction_val: 0.0705, mmd_loss_val: 0.0107, Time: 7.03s\n",
      "Epoch: 260/300, loss: 0.0898, reconstruction: 0.0704, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0804, reconstruction_val: 0.0698, mmd_loss_val: 0.0106, Time: 7.02s\n",
      "Epoch: 261/300, loss: 0.0893, reconstruction: 0.0697, mmd_loss: 0.0107, Regularization: 0.0089, loss_val: 0.0795, reconstruction_val: 0.0693, mmd_loss_val: 0.0102, Time: 7.07s\n",
      "Epoch: 262/300, loss: 0.0884, reconstruction: 0.0690, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0794, reconstruction_val: 0.0690, mmd_loss_val: 0.0104, Time: 6.92s\n",
      "Epoch: 263/300, loss: 0.0877, reconstruction: 0.0685, mmd_loss: 0.0103, Regularization: 0.0089, loss_val: 0.0786, reconstruction_val: 0.0681, mmd_loss_val: 0.0105, Time: 7.25s\n",
      "Epoch: 264/300, loss: 0.0871, reconstruction: 0.0678, mmd_loss: 0.0104, Regularization: 0.0089, loss_val: 0.0784, reconstruction_val: 0.0677, mmd_loss_val: 0.0107, Time: 7.07s\n",
      "Epoch: 265/300, loss: 0.0866, reconstruction: 0.0671, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0764, reconstruction_val: 0.0670, mmd_loss_val: 0.0094, Time: 6.94s\n",
      "Epoch: 266/300, loss: 0.0858, reconstruction: 0.0666, mmd_loss: 0.0103, Regularization: 0.0089, loss_val: 0.0757, reconstruction_val: 0.0661, mmd_loss_val: 0.0096, Time: 7.17s\n",
      "Epoch: 267/300, loss: 0.0854, reconstruction: 0.0659, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0767, reconstruction_val: 0.0657, mmd_loss_val: 0.0110, Time: 7.09s\n",
      "Epoch: 268/300, loss: 0.0847, reconstruction: 0.0653, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0755, reconstruction_val: 0.0652, mmd_loss_val: 0.0104, Time: 7.02s\n",
      "Epoch: 269/300, loss: 0.0844, reconstruction: 0.0649, mmd_loss: 0.0106, Regularization: 0.0089, loss_val: 0.0767, reconstruction_val: 0.0652, mmd_loss_val: 0.0115, Time: 7.06s\n",
      "Epoch: 270/300, loss: 0.0839, reconstruction: 0.0645, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0744, reconstruction_val: 0.0640, mmd_loss_val: 0.0104, Time: 6.73s\n",
      "Epoch: 271/300, loss: 0.0833, reconstruction: 0.0639, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0736, reconstruction_val: 0.0634, mmd_loss_val: 0.0102, Time: 7.07s\n",
      "Epoch: 272/300, loss: 0.0824, reconstruction: 0.0630, mmd_loss: 0.0104, Regularization: 0.0089, loss_val: 0.0726, reconstruction_val: 0.0627, mmd_loss_val: 0.0099, Time: 6.88s\n",
      "Epoch: 273/300, loss: 0.0816, reconstruction: 0.0623, mmd_loss: 0.0104, Regularization: 0.0089, loss_val: 0.0724, reconstruction_val: 0.0621, mmd_loss_val: 0.0102, Time: 6.79s\n",
      "Epoch: 274/300, loss: 0.0812, reconstruction: 0.0619, mmd_loss: 0.0103, Regularization: 0.0089, loss_val: 0.0735, reconstruction_val: 0.0620, mmd_loss_val: 0.0116, Time: 7.20s\n",
      "Epoch: 275/300, loss: 0.0808, reconstruction: 0.0614, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0719, reconstruction_val: 0.0612, mmd_loss_val: 0.0107, Time: 7.03s\n",
      "Epoch: 276/300, loss: 0.0803, reconstruction: 0.0609, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0711, reconstruction_val: 0.0608, mmd_loss_val: 0.0104, Time: 7.38s\n",
      "Epoch: 277/300, loss: 0.0798, reconstruction: 0.0603, mmd_loss: 0.0105, Regularization: 0.0089, loss_val: 0.0706, reconstruction_val: 0.0604, mmd_loss_val: 0.0102, Time: 6.95s\n",
      "Epoch: 278/300, loss: 0.0788, reconstruction: 0.0596, mmd_loss: 0.0103, Regularization: 0.0089, loss_val: 0.0698, reconstruction_val: 0.0596, mmd_loss_val: 0.0103, Time: 6.79s\n",
      "Epoch: 279/300, loss: 0.0784, reconstruction: 0.0591, mmd_loss: 0.0104, Regularization: 0.0089, loss_val: 0.0689, reconstruction_val: 0.0589, mmd_loss_val: 0.0100, Time: 6.84s\n",
      "Epoch: 280/300, loss: 0.0778, reconstruction: 0.0586, mmd_loss: 0.0103, Regularization: 0.0089, loss_val: 0.0692, reconstruction_val: 0.0585, mmd_loss_val: 0.0108, Time: 6.97s\n",
      "Epoch: 281/300, loss: 0.0772, reconstruction: 0.0580, mmd_loss: 0.0102, Regularization: 0.0089, loss_val: 0.0685, reconstruction_val: 0.0579, mmd_loss_val: 0.0106, Time: 6.94s\n",
      "Epoch: 282/300, loss: 0.0773, reconstruction: 0.0580, mmd_loss: 0.0103, Regularization: 0.0089, loss_val: 0.0679, reconstruction_val: 0.0580, mmd_loss_val: 0.0099, Time: 6.78s\n",
      "Epoch: 283/300, loss: 0.0764, reconstruction: 0.0570, mmd_loss: 0.0104, Regularization: 0.0089, loss_val: 0.0678, reconstruction_val: 0.0570, mmd_loss_val: 0.0108, Time: 6.61s\n",
      "Epoch: 284/300, loss: 0.0759, reconstruction: 0.0564, mmd_loss: 0.0106, Regularization: 0.0090, loss_val: 0.0673, reconstruction_val: 0.0567, mmd_loss_val: 0.0106, Time: 6.93s\n",
      "Epoch: 285/300, loss: 0.0752, reconstruction: 0.0559, mmd_loss: 0.0104, Regularization: 0.0090, loss_val: 0.0659, reconstruction_val: 0.0560, mmd_loss_val: 0.0099, Time: 7.22s\n",
      "Epoch: 286/300, loss: 0.0745, reconstruction: 0.0552, mmd_loss: 0.0103, Regularization: 0.0090, loss_val: 0.0656, reconstruction_val: 0.0555, mmd_loss_val: 0.0100, Time: 7.13s\n",
      "Epoch: 287/300, loss: 0.0742, reconstruction: 0.0548, mmd_loss: 0.0104, Regularization: 0.0090, loss_val: 0.0658, reconstruction_val: 0.0550, mmd_loss_val: 0.0108, Time: 7.24s\n",
      "Epoch: 288/300, loss: 0.0739, reconstruction: 0.0544, mmd_loss: 0.0105, Regularization: 0.0090, loss_val: 0.0652, reconstruction_val: 0.0544, mmd_loss_val: 0.0107, Time: 7.06s\n",
      "Epoch: 289/300, loss: 0.0731, reconstruction: 0.0538, mmd_loss: 0.0104, Regularization: 0.0090, loss_val: 0.0640, reconstruction_val: 0.0539, mmd_loss_val: 0.0101, Time: 6.93s\n",
      "Epoch: 290/300, loss: 0.0723, reconstruction: 0.0532, mmd_loss: 0.0102, Regularization: 0.0090, loss_val: 0.0647, reconstruction_val: 0.0532, mmd_loss_val: 0.0115, Time: 6.88s\n",
      "Epoch: 291/300, loss: 0.0724, reconstruction: 0.0530, mmd_loss: 0.0104, Regularization: 0.0090, loss_val: 0.0649, reconstruction_val: 0.0546, mmd_loss_val: 0.0103, Time: 6.92s\n",
      "Epoch: 292/300, loss: 0.1341, reconstruction: 0.1138, mmd_loss: 0.0114, Regularization: 0.0090, loss_val: 0.1223, reconstruction_val: 0.1050, mmd_loss_val: 0.0173, Time: 6.97s\n",
      "Epoch: 293/300, loss: 0.1277, reconstruction: 0.1040, mmd_loss: 0.0147, Regularization: 0.0090, loss_val: 0.1010, reconstruction_val: 0.0880, mmd_loss_val: 0.0130, Time: 7.16s\n",
      "Epoch: 294/300, loss: 0.1030, reconstruction: 0.0816, mmd_loss: 0.0125, Regularization: 0.0090, loss_val: 0.0863, reconstruction_val: 0.0746, mmd_loss_val: 0.0117, Time: 7.09s\n",
      "Epoch: 295/300, loss: 0.0921, reconstruction: 0.0715, mmd_loss: 0.0116, Regularization: 0.0090, loss_val: 0.0778, reconstruction_val: 0.0672, mmd_loss_val: 0.0106, Time: 7.12s\n",
      "Epoch: 296/300, loss: 0.0863, reconstruction: 0.0656, mmd_loss: 0.0118, Regularization: 0.0090, loss_val: 0.0737, reconstruction_val: 0.0629, mmd_loss_val: 0.0107, Time: 6.85s\n",
      "Epoch: 297/300, loss: 0.0819, reconstruction: 0.0614, mmd_loss: 0.0115, Regularization: 0.0090, loss_val: 0.0706, reconstruction_val: 0.0595, mmd_loss_val: 0.0111, Time: 7.09s\n",
      "Epoch: 298/300, loss: 0.0784, reconstruction: 0.0584, mmd_loss: 0.0110, Regularization: 0.0090, loss_val: 0.0673, reconstruction_val: 0.0570, mmd_loss_val: 0.0103, Time: 6.79s\n",
      "Epoch: 299/300, loss: 0.0760, reconstruction: 0.0563, mmd_loss: 0.0107, Regularization: 0.0090, loss_val: 0.0657, reconstruction_val: 0.0554, mmd_loss_val: 0.0104, Time: 7.18s\n",
      "Epoch: 300/300, loss: 0.0747, reconstruction: 0.0548, mmd_loss: 0.0109, Regularization: 0.0090, loss_val: 0.0648, reconstruction_val: 0.0541, mmd_loss_val: 0.0107, Time: 6.94s\n"
     ]
    }
   ],
   "source": [
    "# 使用已有的 vocab 初始化模型\n",
    "model = Model(vocab=vocab)\n",
    "# 将 PyTorch 张量转换为 TensorFlow 张量\n",
    "X_train = tf.convert_to_tensor(X_train.numpy(), dtype=tf.int32)  # 确保是整数类型\n",
    "X_val = tf.convert_to_tensor(X_val.numpy(), dtype=tf.int32)\n",
    "\n",
    "# # 训练模型\n",
    "history = model.train(\n",
    "    dataset_train=(X_train, Y_train, C_train, L_train),\n",
    "    dataset_val=(X_val, Y_val, C_val, L_val),\n",
    "    epochs=300,  # 设置训练轮数\n",
    "    patience=50  # 设置早停机制的容忍度\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 假设你已经训练好了模型并得到了 history\n",
    "# model.plot_loss(history, path=None)  # 不保存图像时，可以传 None 或者不传 path 参数\n",
    "model.plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 假设你已经训练好了模型并得到了 history\n",
    "model.plot_loss_kl(history, path=None)  # 不保存图像时，可以传 None 或者不传 path 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重已保存至 model_pre_weights1.h5\n"
     ]
    }
   ],
   "source": [
    "# 定义保存路径（保存模型的权重）\n",
    "save_path = 'model_pre_weights1.h5'  # HDF5 格式保存权重文件\n",
    "\n",
    "# 保存模型权重\n",
    "model.save_weights(save_path)\n",
    "print(f\"模型权重已保存至 {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 重新加载模型\n",
    "# 创建虚拟数据以构建模型\n",
    "seq_length = model.seq_length  # 模型定义的序列长度\n",
    "batch_size = 1  # 用于测试的批次大小\n",
    "\n",
    "# 创建三个输入:\n",
    "# 1. X: SMILES序列\n",
    "# 2. C: 两个条件SMILES序列拼接\n",
    "# 3. L: 序列长度\n",
    "dummy_data = (\n",
    "    tf.zeros((batch_size, seq_length), dtype=tf.int32),  # X \n",
    "    tf.zeros((batch_size, seq_length * 2), dtype=tf.float32),  # C: 两个SMILES序列拼接\n",
    "    tf.constant([seq_length], dtype=tf.int32)  # L\n",
    ")\n",
    "\n",
    "# 调用模型以完成构建\n",
    "_ = model(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重已从 model_pre_weights1.h5 加载\n"
     ]
    }
   ],
   "source": [
    "# 3. 加载权重\n",
    "load_path = 'model_pre_weights1.h5'\n",
    "model.load_weights(load_path)\n",
    "print(f\"模型权重已从 {load_path} 加载\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载预训练权重，微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import time\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def transfer_vocab_weights(old_vocab, new_vocab, old_embedding_weights):\n",
    "#     \"\"\"\n",
    "#     将预训练模型的嵌入层权重转移到新的词汇表上\n",
    "    \n",
    "#     Args:\n",
    "#         old_vocab: 预训练模型的词汇表字典\n",
    "#         new_vocab: 新数据集的词汇表字典\n",
    "#         old_embedding_weights: 预训练模型的嵌入层权重\n",
    "    \n",
    "#     Returns:\n",
    "#         new_embedding_weights: 新的嵌入层权重\n",
    "#     \"\"\"\n",
    "#     # 获取嵌入维度\n",
    "#     embedding_dim = old_embedding_weights.shape[1]\n",
    "    \n",
    "#     # 初始化新的嵌入权重\n",
    "#     new_embedding_weights = np.random.uniform(\n",
    "#         low=-0.1, \n",
    "#         high=0.1, \n",
    "#         size=(len(new_vocab), embedding_dim)\n",
    "#     ).astype(np.float32)\n",
    "    \n",
    "#     # 统计重叠词汇\n",
    "#     overlap_count = 0\n",
    "    \n",
    "#     # 对于重叠的词汇，复制预训练权重\n",
    "#     for token, new_idx in new_vocab.items():\n",
    "#         if token in old_vocab:\n",
    "#             old_idx = old_vocab[token]\n",
    "#             new_embedding_weights[new_idx] = old_embedding_weights[old_idx]\n",
    "#             overlap_count += 1\n",
    "    \n",
    "#     print(f\"词汇表重叠统计:\")\n",
    "#     print(f\"  预训练词汇表大小: {len(old_vocab)}\")\n",
    "#     print(f\"  新词汇表大小: {len(new_vocab)}\")\n",
    "#     print(f\"  重叠词汇数量: {overlap_count}\")\n",
    "#     print(f\"  重叠比例: {overlap_count/len(new_vocab)*100:.2f}%\")\n",
    "    \n",
    "#     return new_embedding_weights\n",
    "\n",
    "# def load_and_fine_tune_model(old_vocab, new_vocab, pretrained_weights_path,\n",
    "#                              X_sample, C_sample, L_sample, X_new_sample, C_new_sample, L_new_sample):\n",
    "#     \"\"\"\n",
    "#     1. 实例化旧模型并“构建”变量\n",
    "#     2. 加载 H5 权重\n",
    "#     3. 实例化新模型并“构建”变量\n",
    "#     4. 迁移嵌入层权重 + 冻结/释放层\n",
    "#     5. 返回新模型\n",
    "#     \"\"\"\n",
    "#     # --- 1. 构建旧模型并创建变量 ---\n",
    "#     old_model = Model(vocab=old_vocab)\n",
    "#     # 假跑一次，确保 subclassed Model 的所有权重都创建好了\n",
    "#     _ = old_model((X_sample, C_sample, L_sample), training=True)\n",
    "    \n",
    "#     # --- 2. 加载预训练权重 ---\n",
    "#     old_model.load_weights(pretrained_weights_path)\n",
    "#     print(\"✅ 旧模型权重加载完成\")\n",
    "\n",
    "#     # 提取旧 embedding 权重\n",
    "#     old_embedding_weights = old_model.embedding.get_weights()[0]\n",
    "\n",
    "#     # --- 3. 构建新模型并创建变量 ---\n",
    "#     new_model = Model(vocab=new_vocab)\n",
    "#     _ = new_model((X_new_sample, C_new_sample, L_new_sample), training=True)\n",
    "\n",
    "#     # --- 4. 迁移嵌入层权重 ---\n",
    "#     new_emb_w = transfer_vocab_weights(old_vocab, new_vocab, old_embedding_weights)\n",
    "#     new_model.embedding.set_weights([new_emb_w])\n",
    "#     print(\"✅ 嵌入层权重迁移完成\")\n",
    "\n",
    "#     # --- 5. 加载其余共享层权重 ---\n",
    "#     try:\n",
    "#         new_model.encoder.set_weights(old_model.encoder.get_weights())\n",
    "#         print(\"✅ Encoder 权重加载完成\")\n",
    "#     except ValueError as e:\n",
    "#         print(f\"⚠️ 警告: 无法迁移所有的encoder权重: {str(e)}\")\n",
    "#         print(\"Proceeding with partial weight transfer...\")\n",
    "    \n",
    "#     # --- 5.1 迁移 decoder 中间层（逐层迁移，跳过输出层） ---\n",
    "#     # for old_layer, new_layer in zip(old_model.decoder.layers, new_model.decoder.layers):\n",
    "#     #     try:\n",
    "#     #         new_layer.set_weights(old_layer.get_weights())\n",
    "#     #         print(f\"✅ 迁移 decoder 子层：{new_layer.name}\")\n",
    "#     #     except ValueError:\n",
    "#     #         print(f\"⚠️ 跳过不兼容的 decoder 子层：{new_layer.name}\")\n",
    "#     #     print(\"✅ Decoder 中间层权重已迁移（不包括输出层）\")\n",
    "#     # --- 6. Transfer decoder weights (excluding output layer) ---\n",
    "#     for old_layer, new_layer in zip(old_model.decoder.layers, new_model.decoder.layers):\n",
    "#         if isinstance(new_layer, tf.keras.layers.Dense) and new_layer.units == new_model.vocab_size:\n",
    "#             print(f\"⚠️ 跳过输出层: {new_layer.name}\")\n",
    "#             continue\n",
    "#         try:\n",
    "#             new_layer.set_weights(old_layer.get_weights())\n",
    "#             print(f\"✅ 迁移 decoder 层: {new_layer.name}\")\n",
    "#         except ValueError as e:\n",
    "#             print(f\"⚠️ 警告: 无法迁移decoder层 {new_layer.name}: {str(e)}\")# 这个代码跟上边以上，都跳过了输出层\n",
    "\n",
    "\n",
    "#     # --- 6. 冻结共享权重，仅微调嵌入与输出层 ---\n",
    "#     # 冻结 encoder + quantizer + decoder\n",
    "#     new_model.encoder.trainable = True\n",
    "#     new_model.decoder.trainable = True\n",
    "\n",
    "#     # 释放 embedding 和 decoder 最后一层（假设叫 `decoder.dense` 或 `output_layer`）\n",
    "#     new_model.embedding.trainable = True\n",
    "#     try:\n",
    "#         new_model.decoder.dense.trainable = True\n",
    "#     except:\n",
    "#         # 如果你 decoder 输出层名字不同，请改为 new_model.decoder.output_layer\n",
    "#         new_model.decoder.output_layer.trainable = True\n",
    "\n",
    "#     return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 定义词汇表\n",
    "# old_vocab = {'#': 0, '(': 1, ')': 2, '-': 3, '.': 4, '/': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '=': 13, 'B': 14, 'Br': 15, 'C': 16, 'Cl': 17, 'E': 18, 'F': 19, 'I': 20, 'N': 21, 'O': 22, 'P': 23, 'S': 24, 'X': 25, '[ Co +2]': 26, '[ Co +]': 27, '[ Cr ]': 28, '[ Fe ]': 29, '[ Ir +]': 30, '[ Ni +2]': 31, '[ Pd +2]': 32, '[ Pd +]': 33, '[ Rh +]': 34, '[ Rh ]': 35, '[ Ru +2]': 36, '[ Ru +]': 37, '[B-1]': 38, '[B-]': 39, '[BH3-]': 40, '[C@@H]': 41, '[C@@]': 42, '[C@H]': 43, '[C@]': 44, '[C]': 45, '[H]': 46, '[K+]': 47, '[N+]': 48, '[N-1]': 49, '[N-]': 50, '[NH3+]': 51, '[N]': 52, '[Na+]': 53, '[O-1]': 54, '[O-]': 55, '[P+]': 56, '[P@@]': 57, '[P@H]': 58, '[P@]': 59, '[PH+]': 60, '[PH]': 61, '[S@@]': 62, '[S@]': 63, '[Si]': 64, '[c-]': 65, '[n+]': 66, '[nH]': 67, '\\\\': 68, 'c': 69, 'n': 70, 'o': 71, 'p': 72, 's': 73}\n",
    "\n",
    "# new_vocab = {\n",
    "#     '(': 0, ')': 1, '*': 2, '-': 3, '.': 4, '/': 5, '1': 6, '2': 7, '3': 8,\n",
    "#     '4': 9, '=': 10, 'Br': 11, 'C': 12, 'Cl': 13, 'E': 14, 'F': 15,\n",
    "#     'N': 16, 'O': 17, 'P': 18, 'S': 19, 'X': 20, 'c': 21, '5': 22\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重已从 model_pre_weights1.h5 加载\n"
     ]
    }
   ],
   "source": [
    "seq_len = 165  # 必须与预训练一致\n",
    "cond_dim = 2   # 从报错中看你用的是 shape=(1, 2)\n",
    "X = tf.constant([[0]*seq_len], dtype=tf.int32)\n",
    "C = tf.constant([[0.0]*cond_dim], dtype=tf.float32)\n",
    "L = tf.constant([seq_len], dtype=tf.int32)  # batch_size=1\n",
    "\n",
    "model((X, C, L))  # ✅ 现在提供三个输入\n",
    "model.load_weights('model_pre_weights1.h5')\n",
    "print(f\"模型权重已从 {load_path} 加载\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 新数据，新vocab映射\n",
    "def load_new_vocab(file, args):\n",
    "    \"\"\"\n",
    "    Load new_vocabulary and labels，并添加智能分词功能\n",
    "    \"\"\"\n",
    "    f = open(file)\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "    lines = [l.split() for l in lines]\n",
    "    lines = [l for l in lines]\n",
    "    labels = [l[1:] for l in lines][1:]\n",
    "\n",
    "    if args.grammar == 'smiles':\n",
    "        new_vocab={'#': 0, '(': 1, ')': 2, '-': 3, '.': 4, '/': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '=': 13, 'B': 14, 'Br': 15, 'C': 16, 'Cl': 17, 'E': 18, 'F': 19, 'I': 20, 'N': 21, 'O': 22, 'P': 23, 'S': 24, 'X': 25, '[ Co +2]': 26, '[ Co +]': 27, '[ Cr ]': 28, '[ Fe ]': 29, '[ Ir +]': 30, '[ Ni +2]': 31, '[ Pd +2]': 32, '[ Pd +]': 33, '[ Rh +]': 34, '[ Rh ]': 35, '[ Ru +2]': 36, '[ Ru +]': 37, '[B-1]': 38, '[B-]': 39, '[BH3-]': 40, '[C@@H]': 41, '[C@@]': 42, '[C@H]': 43, '[C@]': 44, '[C]': 45, '[H]': 46, '[K+]': 47, '[N+]': 48, '[N-1]': 49, '[N-]': 50, '[NH3+]': 51, '[N]': 52, '[Na+]': 53, '[O-1]': 54, '[O-]': 55, '[P+]': 56, '[P@@]': 57, '[P@H]': 58, '[P@]': 59, '[PH+]': 60, '[PH]': 61, '[S@@]': 62, '[S@]': 63, '[Si]': 64, '[c-]': 65, '[n+]': 66, '[nH]': 67, '\\\\': 68, 'c': 69, 'n': 70, 'o': 71, 'p': 72, 's': 73,'*': 74}                \n",
    "        \n",
    "        # 添加无空格版本的金属元素token\n",
    "        for token, idx in list(new_vocab.items()):\n",
    "            if isinstance(token, str) and token.startswith('[') and ' ' in token:\n",
    "                no_space_token = token.replace(' ', '')\n",
    "                if no_space_token not in new_vocab:\n",
    "                    new_vocab[no_space_token] = idx\n",
    "    else:\n",
    "        raise ValueError(\"Unknown grammar type\")\n",
    "    \n",
    "    chars = list(new_vocab)\n",
    "    chars += ('E',)  # End of smiles末尾添加 E\n",
    "    chars += ('X',)  # Start of smiles开头添加 X\n",
    "    \n",
    "    # 定义SMILES分词函数\n",
    "    def tokenize(smiles):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        smiles = smiles.strip()\n",
    "        while i < len(smiles):\n",
    "            # 优先匹配方括号里的整个 token\n",
    "            if smiles[i] == '[':\n",
    "                m = re.match(r\"\\[[^\\[\\]]+\\]\", smiles[i:])\n",
    "                if m:\n",
    "                    tok = m.group(0)\n",
    "                    tokens.append(tok)\n",
    "                    i += len(tok)\n",
    "                    continue\n",
    "            # 尝试两字符 token（如 Cl、Br）\n",
    "            if i + 1 < len(smiles) and smiles[i:i+2] in new_vocab:\n",
    "                tokens.append(smiles[i:i+2])\n",
    "                i += 2\n",
    "                continue\n",
    "            # 单字符 token\n",
    "            if smiles[i] in new_vocab:\n",
    "                tokens.append(smiles[i])\n",
    "                i += 1\n",
    "                continue\n",
    "            # 未知字符，打 warning 并跳过\n",
    "            print(f\"警告: '{smiles[i]}' 不在词汇表中。\")\n",
    "            i += 1\n",
    "        return tokens\n",
    "    \n",
    "    # token 列表 → 索引列表\n",
    "    def map_tokens_to_ids(tokens):\n",
    "        return [new_vocab.get(tok, new_vocab['X']) for tok in tokens]\n",
    "\n",
    "    return new_vocab, chars, labels, tokenize, map_tokens_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "def load_and_map_new_smiles(file, args):\n",
    "    \"\"\"\n",
    "    读取 CSV 文件并将 SMILES 映射为符号序列，同时计算每个 SMILES 对应的分子量\n",
    "    支持 Sub1 和 Sub2 两个替代分子列\n",
    "    \"\"\"\n",
    "    # 读取 CSV\n",
    "    df = pd.read_csv(file)\n",
    "    smiles_list = df['SMILES'].tolist()\n",
    "    \n",
    "    # 加载词表和字符、标签信息以及分词函数\n",
    "    new_vocab, chars, labels, tokenize, map_tokens_to_ids = load_new_vocab(file, args)\n",
    "    \n",
    "    # 将主 SMILES 映射为符号序列\n",
    "    sequences = []\n",
    "    for smiles in smiles_list:\n",
    "        tokens = tokenize(smiles)\n",
    "        ids = map_tokens_to_ids(tokens)\n",
    "        # 填充序列到统一长度\n",
    "        max_len = 165  # 或者其他您想要的长度\n",
    "        ids += [new_vocab['E']] * (max_len - len(ids))\n",
    "        sequences.append(ids)\n",
    "    \n",
    "    # 计算每个 SMILES 的分子对象列表（可用于分子量等计算）\n",
    "    mols = mol_from_smiles(smiles_list)\n",
    "    \n",
    "    # 读取 Sub1、Sub2 和 OUTPUT\n",
    "    sub1_sequences, sub2_sequences, output_values = None, None, None\n",
    "    if {'Sub1', 'Sub2', 'OUTPUT'}.issubset(df.columns):\n",
    "        sub1_list = df['Sub1'].tolist()\n",
    "        sub2_list = df['Sub2'].tolist()\n",
    "        output_values = df['OUTPUT'].tolist()\n",
    "        \n",
    "        # 映射 Sub1 和 Sub2\n",
    "        sub1_sequences = []\n",
    "        for smiles in sub1_list:\n",
    "            tokens = tokenize(smiles)\n",
    "            ids = map_tokens_to_ids(tokens)\n",
    "            ids += [new_vocab['E']] * (max_len - len(ids))\n",
    "            sub1_sequences.append(ids)\n",
    "            \n",
    "        sub2_sequences = []\n",
    "        for smiles in sub2_list:\n",
    "            tokens = tokenize(smiles)\n",
    "            ids = map_tokens_to_ids(tokens)\n",
    "            ids += [new_vocab['E']] * (max_len - len(ids))\n",
    "            sub2_sequences.append(ids)\n",
    "    else:\n",
    "        missing = [col for col in ['Sub1', 'Sub2', 'OUTPUT'] if col not in df.columns]\n",
    "        print(f\"警告: CSV文件中缺少列: {', '.join(missing)}\")\n",
    "\n",
    "    # 转为 PyTorch Tensor\n",
    "    sequences_tensor = torch.tensor(sequences, dtype=torch.long)\n",
    "\n",
    "    # 返回主 SMILES 列表、主序列 Tensor、Sub1 、Sub2 、OUTPUT 值列表以及字符表\n",
    "    return smiles_list, sequences_tensor, sub1_sequences, sub2_sequences, output_values, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 示例调用\n",
    "args = type('', (), {})()\n",
    "args.grammar = 'smiles'\n",
    "\n",
    "# 加载数据\n",
    "new_smiles_list, new_sequences_tensor, new_sub1_seq, new_sub2_seq, new_output_values, new_chars = load_and_map_new_smiles('reactions_5.csv', args)\n",
    "\n",
    "# 划分训练和测试集\n",
    "(train_new_smiles, test_new_smiles,\n",
    " train_new_seq, test_new_seq,\n",
    " train_new_sub1, test_new_sub1,\n",
    " train_new_sub2, test_new_sub2,\n",
    " train_new_output, test_new_output) = split_data(\n",
    "    new_smiles_list, new_sequences_tensor,\n",
    "    new_sub1_seq, new_sub2_seq,\n",
    "    new_output_values,\n",
    "    test_ratio=0.2\n",
    ")\n",
    "\n",
    "# # # 输出示例\n",
    "# # print(f\"训练集大小: {len(train_smiles)}, 测试集大小: {len(test_smiles)}\")\n",
    "# # for i in range(min(5, len(train_smiles))):\n",
    "# #     print(f\"SMILES: {train_smiles[i]}, 主序列: {train_seq[i]}, Sub1序列: {train_sub1[i]}, Sub2序列: {train_sub2[i]}, OUTPUT: {train_output[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new_train: torch.Size([7360, 165]), C_new_train: (7360, 330)\n",
      "X_new_val:   torch.Size([1840, 165]),   C_new_val:   (1840, 330)\n"
     ]
    }
   ],
   "source": [
    "# 假设 seq_length 是序列的最大长度\n",
    "new_seq_length = train_new_seq.shape[1]\n",
    "\n",
    "# 将替代序列和输出转换为 numpy 数组\n",
    "train_new_sub1_arr = np.array(train_new_sub1, dtype=np.int32)  # (N, L_sub)\n",
    "train_new_sub2_arr = np.array(train_new_sub2, dtype=np.int32)  # (N, L_sub)\n",
    "test_new_sub1_arr  = np.array(test_new_sub1,  dtype=np.int32)  # (M, L_sub)\n",
    "test_new_sub2_arr  = np.array(test_new_sub2,  dtype=np.int32)  # (M, L_sub)\n",
    "\n",
    "# 横向拼接 sub1, sub2, output\n",
    "C_new_train_np = np.hstack((train_new_sub1_arr, train_new_sub2_arr))  # (N, 2*L_sub+1)\n",
    "C_new_val_np   = np.hstack((test_new_sub1_arr,  test_new_sub2_arr))   # (M, 2*L_sub+1)\n",
    "\n",
    "# 转成 TensorFlow 张量\n",
    "C_new_train = tf.convert_to_tensor(C_new_train_np, dtype=tf.float32)\n",
    "C_new_val   = tf.convert_to_tensor(C_new_val_np,   dtype=tf.float32)\n",
    "# --------- 修改 END ---------\n",
    "\n",
    "# X/Y/L 输入保持不变\n",
    "X_new_train = train_new_seq\n",
    "Y_new_train = train_new_seq\n",
    "L_new_train = np.full((len(train_new_seq),), new_seq_length, dtype=np.int32)\n",
    "\n",
    "X_new_val = test_new_seq\n",
    "Y_new_val = test_new_seq\n",
    "L_new_val = np.full((len(test_new_seq),), new_seq_length, dtype=np.int32)\n",
    "\n",
    "print(f\"X_new_train: {X_new_train.shape}, C_new_train: {C_new_train.shape}\")\n",
    "print(f\"X_new_val:   {X_new_val.shape},   C_new_val:   {C_new_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Y_train = tf.convert_to_tensor(Y_train, dtype=tf.string)\n",
    "Y_new_train = tf.convert_to_tensor(Y_new_train.numpy(), dtype=tf.int32)\n",
    "C_new_train = tf.convert_to_tensor(C_new_train, dtype=tf.float32)\n",
    "L_new_train = tf.convert_to_tensor(L_new_train, dtype=tf.int32)\n",
    "Y_new_val = tf.convert_to_tensor(Y_new_val.numpy(), dtype=tf.int32)\n",
    "C_new_val = tf.convert_to_tensor(C_new_val, dtype=tf.float32)\n",
    "L_new_val = tf.convert_to_tensor(L_new_val, dtype=tf.int32)\n",
    "\n",
    "X_new_train = tf.convert_to_tensor(X_new_train.numpy(), dtype=tf.int32)  # 确保是整数类型\n",
    "X_new_val = tf.convert_to_tensor(X_new_val.numpy(), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008279800415039062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epochs",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7edf2d578a45039dc846f16c2fba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, loss: 0.3005, reconstruction: 0.1992, mmd_loss: 0.0924, Regularization: 0.0090, loss_val: 0.1564, reconstruction_val: 0.1179, mmd_loss_val: 0.0385, Time: 6.02s\n",
      "Epoch: 2/10, loss: 0.1311, reconstruction: 0.0986, mmd_loss: 0.0235, Regularization: 0.0090, loss_val: 0.0992, reconstruction_val: 0.0798, mmd_loss_val: 0.0195, Time: 5.64s\n",
      "Epoch: 3/10, loss: 0.0956, reconstruction: 0.0703, mmd_loss: 0.0163, Regularization: 0.0090, loss_val: 0.0734, reconstruction_val: 0.0604, mmd_loss_val: 0.0131, Time: 5.77s\n",
      "Epoch: 4/10, loss: 0.0742, reconstruction: 0.0539, mmd_loss: 0.0113, Regularization: 0.0090, loss_val: 0.0579, reconstruction_val: 0.0472, mmd_loss_val: 0.0107, Time: 5.46s\n",
      "Epoch: 5/10, loss: 0.0630, reconstruction: 0.0434, mmd_loss: 0.0107, Regularization: 0.0090, loss_val: 0.0499, reconstruction_val: 0.0402, mmd_loss_val: 0.0097, Time: 5.26s\n",
      "Epoch: 6/10, loss: 0.0570, reconstruction: 0.0382, mmd_loss: 0.0099, Regularization: 0.0090, loss_val: 0.0462, reconstruction_val: 0.0365, mmd_loss_val: 0.0097, Time: 5.18s\n",
      "Epoch: 7/10, loss: 0.0539, reconstruction: 0.0352, mmd_loss: 0.0097, Regularization: 0.0090, loss_val: 0.0434, reconstruction_val: 0.0341, mmd_loss_val: 0.0094, Time: 5.25s\n",
      "Epoch: 8/10, loss: 0.0516, reconstruction: 0.0331, mmd_loss: 0.0095, Regularization: 0.0090, loss_val: 0.0422, reconstruction_val: 0.0323, mmd_loss_val: 0.0098, Time: 5.65s\n",
      "Epoch: 9/10, loss: 0.0499, reconstruction: 0.0315, mmd_loss: 0.0095, Regularization: 0.0090, loss_val: 0.0405, reconstruction_val: 0.0310, mmd_loss_val: 0.0095, Time: 5.56s\n",
      "Epoch: 10/10, loss: 0.0484, reconstruction: 0.0302, mmd_loss: 0.0092, Regularization: 0.0090, loss_val: 0.0389, reconstruction_val: 0.0298, mmd_loss_val: 0.0091, Time: 5.67s\n"
     ]
    }
   ],
   "source": [
    "# # 假设已经有 old_vocab, new_vocab, 以及 X_train, C_train, L_train\n",
    "# X_s, C_s, L_s = X_train[:1], C_train[:1], L_train[:1]\n",
    "# X_new_s, C_new_s, L_new_s = X_new_train[:1], C_new_train[:1], L_new_train[:1]\n",
    "\n",
    "# model = load_and_fine_tune_model(\n",
    "#     old_vocab, new_vocab, 'model_allgen_ mmd_weights17.h5',\n",
    "#     X_sample=X_s, C_sample=C_s, L_sample=L_s,\n",
    "#     X_new_sample=X_new_s, C_new_sample=C_new_s, L_new_sample=L_new_s,\n",
    "# )\n",
    "\n",
    "# # 验证模型已就绪\n",
    "# print(\"新模型可训练参数：\")\n",
    "# for layer in [model.embedding, model.decoder.dense]:\n",
    "#     print(layer.name, \"trainable:\", layer.trainable)\n",
    "# for layer in [model.encoder, model.decoder]:\n",
    "#     print(layer.name, \"trainable:\", layer.trainable)\n",
    "\n",
    "# 然后就可以直接微调了：\n",
    "history = model.train(\n",
    "    dataset_train=(X_new_train, Y_new_train, C_new_train, L_new_train),\n",
    "    dataset_val  =(X_new_val,   Y_new_val,   C_new_val,   L_new_val),\n",
    "    epochs=10,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重已保存至 NOTLmodel_weights1.h5\n"
     ]
    }
   ],
   "source": [
    "# 定义保存路径（保存模型的权重）\n",
    "save_path = 'NOTLmodel_weights1.h5'  # HDF5 格式保存权重文件\n",
    "\n",
    "# 保存模型权重\n",
    "model.save_weights(save_path)\n",
    "print(f\"模型权重已保存至 {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 重新加载模型\n",
    "# 创建虚拟数据以构建模型\n",
    "seq_length = model.seq_length  # 模型定义的序列长度\n",
    "batch_size = 1  # 用于测试的批次大小\n",
    "\n",
    "# 创建三个输入:\n",
    "# 1. X: SMILES序列\n",
    "# 2. C: 两个条件SMILES序列拼接\n",
    "# 3. L: 序列长度\n",
    "dummy_data = (\n",
    "    tf.zeros((batch_size, seq_length), dtype=tf.int32),  # X \n",
    "    tf.zeros((batch_size, seq_length * 2), dtype=tf.float32),  # C: 两个SMILES序列拼接\n",
    "    tf.constant([seq_length], dtype=tf.int32)  # L\n",
    ")\n",
    "\n",
    "# 调用模型以完成构建\n",
    "_ = model(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重已从 NOTLmodel_weights1.h5 加载\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01621103286743164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epochs",
       "rate": null,
       "total": 190,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8925ed41d0d54069a05a7459aa3c5aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/190, loss: 0.0472, reconstruction: 0.0291, mmd_loss: 0.0091, Regularization: 0.0090, loss_val: 0.0391, reconstruction_val: 0.0288, mmd_loss_val: 0.0103, Time: 5.96s\n",
      "Epoch: 2/190, loss: 0.0461, reconstruction: 0.0280, mmd_loss: 0.0091, Regularization: 0.0090, loss_val: 0.0369, reconstruction_val: 0.0276, mmd_loss_val: 0.0093, Time: 5.65s\n",
      "Epoch: 3/190, loss: 0.0447, reconstruction: 0.0270, mmd_loss: 0.0087, Regularization: 0.0090, loss_val: 0.0354, reconstruction_val: 0.0268, mmd_loss_val: 0.0086, Time: 5.59s\n",
      "Epoch: 4/190, loss: 0.0440, reconstruction: 0.0263, mmd_loss: 0.0088, Regularization: 0.0090, loss_val: 0.0357, reconstruction_val: 0.0266, mmd_loss_val: 0.0092, Time: 5.82s\n",
      "Epoch: 5/190, loss: 0.0434, reconstruction: 0.0257, mmd_loss: 0.0087, Regularization: 0.0090, loss_val: 0.0341, reconstruction_val: 0.0253, mmd_loss_val: 0.0088, Time: 5.69s\n",
      "Epoch: 6/190, loss: 0.0425, reconstruction: 0.0248, mmd_loss: 0.0087, Regularization: 0.0090, loss_val: 0.0342, reconstruction_val: 0.0252, mmd_loss_val: 0.0090, Time: 5.69s\n",
      "Epoch: 7/190, loss: 0.0419, reconstruction: 0.0243, mmd_loss: 0.0086, Regularization: 0.0090, loss_val: 0.0327, reconstruction_val: 0.0238, mmd_loss_val: 0.0089, Time: 5.82s\n",
      "Epoch: 8/190, loss: 0.0407, reconstruction: 0.0233, mmd_loss: 0.0085, Regularization: 0.0090, loss_val: 0.0318, reconstruction_val: 0.0232, mmd_loss_val: 0.0087, Time: 5.73s\n",
      "Epoch: 9/190, loss: 0.0399, reconstruction: 0.0226, mmd_loss: 0.0083, Regularization: 0.0090, loss_val: 0.0312, reconstruction_val: 0.0225, mmd_loss_val: 0.0087, Time: 5.76s\n",
      "Epoch: 10/190, loss: 0.0397, reconstruction: 0.0222, mmd_loss: 0.0085, Regularization: 0.0090, loss_val: 0.0306, reconstruction_val: 0.0220, mmd_loss_val: 0.0085, Time: 5.35s\n",
      "Epoch: 11/190, loss: 0.0390, reconstruction: 0.0216, mmd_loss: 0.0084, Regularization: 0.0090, loss_val: 0.0296, reconstruction_val: 0.0217, mmd_loss_val: 0.0079, Time: 5.64s\n",
      "Epoch: 12/190, loss: 0.0381, reconstruction: 0.0210, mmd_loss: 0.0082, Regularization: 0.0090, loss_val: 0.0298, reconstruction_val: 0.0208, mmd_loss_val: 0.0090, Time: 5.75s\n",
      "Epoch: 13/190, loss: 0.0376, reconstruction: 0.0204, mmd_loss: 0.0083, Regularization: 0.0090, loss_val: 0.0288, reconstruction_val: 0.0204, mmd_loss_val: 0.0084, Time: 5.66s\n",
      "Epoch: 14/190, loss: 0.0370, reconstruction: 0.0199, mmd_loss: 0.0081, Regularization: 0.0090, loss_val: 0.0279, reconstruction_val: 0.0199, mmd_loss_val: 0.0081, Time: 5.86s\n",
      "Epoch: 15/190, loss: 0.0365, reconstruction: 0.0196, mmd_loss: 0.0080, Regularization: 0.0090, loss_val: 0.0275, reconstruction_val: 0.0195, mmd_loss_val: 0.0081, Time: 5.76s\n",
      "Epoch: 16/190, loss: 0.0359, reconstruction: 0.0192, mmd_loss: 0.0078, Regularization: 0.0090, loss_val: 0.0275, reconstruction_val: 0.0192, mmd_loss_val: 0.0083, Time: 5.76s\n",
      "Epoch: 17/190, loss: 0.0356, reconstruction: 0.0186, mmd_loss: 0.0080, Regularization: 0.0090, loss_val: 0.0265, reconstruction_val: 0.0186, mmd_loss_val: 0.0080, Time: 5.80s\n",
      "Epoch: 18/190, loss: 0.0351, reconstruction: 0.0182, mmd_loss: 0.0079, Regularization: 0.0090, loss_val: 0.0261, reconstruction_val: 0.0182, mmd_loss_val: 0.0079, Time: 5.79s\n",
      "Epoch: 19/190, loss: 0.0347, reconstruction: 0.0179, mmd_loss: 0.0079, Regularization: 0.0090, loss_val: 0.0257, reconstruction_val: 0.0178, mmd_loss_val: 0.0079, Time: 5.78s\n",
      "Epoch: 20/190, loss: 0.0342, reconstruction: 0.0175, mmd_loss: 0.0078, Regularization: 0.0090, loss_val: 0.0256, reconstruction_val: 0.0175, mmd_loss_val: 0.0082, Time: 5.92s\n",
      "Epoch: 21/190, loss: 0.0340, reconstruction: 0.0171, mmd_loss: 0.0079, Regularization: 0.0089, loss_val: 0.0254, reconstruction_val: 0.0171, mmd_loss_val: 0.0083, Time: 5.54s\n",
      "Epoch: 22/190, loss: 0.0336, reconstruction: 0.0168, mmd_loss: 0.0078, Regularization: 0.0089, loss_val: 0.0244, reconstruction_val: 0.0168, mmd_loss_val: 0.0076, Time: 5.76s\n",
      "Epoch: 23/190, loss: 0.0333, reconstruction: 0.0166, mmd_loss: 0.0078, Regularization: 0.0089, loss_val: 0.0243, reconstruction_val: 0.0165, mmd_loss_val: 0.0079, Time: 5.75s\n",
      "Epoch: 24/190, loss: 0.0329, reconstruction: 0.0162, mmd_loss: 0.0077, Regularization: 0.0089, loss_val: 0.0237, reconstruction_val: 0.0163, mmd_loss_val: 0.0074, Time: 5.84s\n",
      "Epoch: 25/190, loss: 0.0328, reconstruction: 0.0159, mmd_loss: 0.0079, Regularization: 0.0089, loss_val: 0.0238, reconstruction_val: 0.0159, mmd_loss_val: 0.0080, Time: 5.91s\n",
      "Epoch: 26/190, loss: 0.0324, reconstruction: 0.0157, mmd_loss: 0.0077, Regularization: 0.0089, loss_val: 0.0234, reconstruction_val: 0.0156, mmd_loss_val: 0.0077, Time: 5.73s\n",
      "Epoch: 27/190, loss: 0.0320, reconstruction: 0.0153, mmd_loss: 0.0078, Regularization: 0.0089, loss_val: 0.0230, reconstruction_val: 0.0152, mmd_loss_val: 0.0078, Time: 5.94s\n",
      "Epoch: 28/190, loss: 0.0314, reconstruction: 0.0150, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0227, reconstruction_val: 0.0150, mmd_loss_val: 0.0077, Time: 5.93s\n",
      "Epoch: 29/190, loss: 0.0313, reconstruction: 0.0147, mmd_loss: 0.0076, Regularization: 0.0089, loss_val: 0.0229, reconstruction_val: 0.0148, mmd_loss_val: 0.0081, Time: 5.85s\n",
      "Epoch: 30/190, loss: 0.0310, reconstruction: 0.0144, mmd_loss: 0.0077, Regularization: 0.0089, loss_val: 0.0222, reconstruction_val: 0.0145, mmd_loss_val: 0.0076, Time: 5.88s\n",
      "Epoch: 31/190, loss: 0.0306, reconstruction: 0.0142, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0218, reconstruction_val: 0.0141, mmd_loss_val: 0.0077, Time: 5.85s\n",
      "Epoch: 32/190, loss: 0.0303, reconstruction: 0.0139, mmd_loss: 0.0076, Regularization: 0.0089, loss_val: 0.0215, reconstruction_val: 0.0139, mmd_loss_val: 0.0076, Time: 5.80s\n",
      "Epoch: 33/190, loss: 0.0303, reconstruction: 0.0138, mmd_loss: 0.0076, Regularization: 0.0089, loss_val: 0.0217, reconstruction_val: 0.0140, mmd_loss_val: 0.0078, Time: 5.93s\n",
      "Epoch: 34/190, loss: 0.0301, reconstruction: 0.0136, mmd_loss: 0.0077, Regularization: 0.0089, loss_val: 0.0213, reconstruction_val: 0.0136, mmd_loss_val: 0.0077, Time: 5.91s\n",
      "Epoch: 35/190, loss: 0.0298, reconstruction: 0.0134, mmd_loss: 0.0076, Regularization: 0.0089, loss_val: 0.0213, reconstruction_val: 0.0135, mmd_loss_val: 0.0077, Time: 5.90s\n",
      "Epoch: 36/190, loss: 0.0297, reconstruction: 0.0132, mmd_loss: 0.0077, Regularization: 0.0089, loss_val: 0.0209, reconstruction_val: 0.0130, mmd_loss_val: 0.0078, Time: 5.87s\n",
      "Epoch: 37/190, loss: 0.0293, reconstruction: 0.0128, mmd_loss: 0.0076, Regularization: 0.0089, loss_val: 0.0206, reconstruction_val: 0.0128, mmd_loss_val: 0.0078, Time: 5.83s\n",
      "Epoch: 38/190, loss: 0.0291, reconstruction: 0.0126, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0203, reconstruction_val: 0.0127, mmd_loss_val: 0.0076, Time: 5.77s\n",
      "Epoch: 39/190, loss: 0.0289, reconstruction: 0.0124, mmd_loss: 0.0076, Regularization: 0.0089, loss_val: 0.0200, reconstruction_val: 0.0124, mmd_loss_val: 0.0076, Time: 5.84s\n",
      "Epoch: 40/190, loss: 0.0285, reconstruction: 0.0121, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0198, reconstruction_val: 0.0121, mmd_loss_val: 0.0077, Time: 5.88s\n",
      "Epoch: 41/190, loss: 0.0285, reconstruction: 0.0120, mmd_loss: 0.0076, Regularization: 0.0089, loss_val: 0.0198, reconstruction_val: 0.0120, mmd_loss_val: 0.0078, Time: 5.89s\n",
      "Epoch: 42/190, loss: 0.0281, reconstruction: 0.0118, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0196, reconstruction_val: 0.0118, mmd_loss_val: 0.0078, Time: 5.87s\n",
      "Epoch: 43/190, loss: 0.0279, reconstruction: 0.0116, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0189, reconstruction_val: 0.0116, mmd_loss_val: 0.0073, Time: 5.89s\n",
      "Epoch: 44/190, loss: 0.0279, reconstruction: 0.0115, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0192, reconstruction_val: 0.0117, mmd_loss_val: 0.0074, Time: 5.73s\n",
      "Epoch: 45/190, loss: 0.0277, reconstruction: 0.0114, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0188, reconstruction_val: 0.0113, mmd_loss_val: 0.0075, Time: 5.78s\n",
      "Epoch: 46/190, loss: 0.0273, reconstruction: 0.0110, mmd_loss: 0.0074, Regularization: 0.0089, loss_val: 0.0184, reconstruction_val: 0.0111, mmd_loss_val: 0.0074, Time: 5.84s\n",
      "Epoch: 47/190, loss: 0.0276, reconstruction: 0.0112, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0190, reconstruction_val: 0.0112, mmd_loss_val: 0.0078, Time: 5.45s\n",
      "Epoch: 48/190, loss: 0.0276, reconstruction: 0.0113, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0186, reconstruction_val: 0.0111, mmd_loss_val: 0.0075, Time: 5.72s\n",
      "Epoch: 49/190, loss: 0.0274, reconstruction: 0.0109, mmd_loss: 0.0076, Regularization: 0.0089, loss_val: 0.0182, reconstruction_val: 0.0107, mmd_loss_val: 0.0074, Time: 5.86s\n",
      "Epoch: 50/190, loss: 0.0268, reconstruction: 0.0105, mmd_loss: 0.0075, Regularization: 0.0089, loss_val: 0.0178, reconstruction_val: 0.0105, mmd_loss_val: 0.0074, Time: 5.84s\n",
      "Epoch: 51/190, loss: 0.0264, reconstruction: 0.0102, mmd_loss: 0.0073, Regularization: 0.0089, loss_val: 0.0179, reconstruction_val: 0.0102, mmd_loss_val: 0.0077, Time: 5.74s\n",
      "Epoch: 52/190, loss: 0.0263, reconstruction: 0.0100, mmd_loss: 0.0074, Regularization: 0.0089, loss_val: 0.0175, reconstruction_val: 0.0101, mmd_loss_val: 0.0075, Time: 5.57s\n",
      "Epoch: 53/190, loss: 0.0261, reconstruction: 0.0098, mmd_loss: 0.0074, Regularization: 0.0089, loss_val: 0.0175, reconstruction_val: 0.0099, mmd_loss_val: 0.0075, Time: 5.75s\n",
      "Epoch: 54/190, loss: 0.0259, reconstruction: 0.0097, mmd_loss: 0.0073, Regularization: 0.0089, loss_val: 0.0173, reconstruction_val: 0.0098, mmd_loss_val: 0.0076, Time: 5.89s\n",
      "Epoch: 55/190, loss: 0.0259, reconstruction: 0.0096, mmd_loss: 0.0074, Regularization: 0.0089, loss_val: 0.0168, reconstruction_val: 0.0096, mmd_loss_val: 0.0072, Time: 5.52s\n",
      "Epoch: 56/190, loss: 0.0256, reconstruction: 0.0094, mmd_loss: 0.0074, Regularization: 0.0089, loss_val: 0.0172, reconstruction_val: 0.0095, mmd_loss_val: 0.0077, Time: 5.70s\n",
      "Epoch: 57/190, loss: 0.0255, reconstruction: 0.0093, mmd_loss: 0.0074, Regularization: 0.0089, loss_val: 0.0167, reconstruction_val: 0.0094, mmd_loss_val: 0.0072, Time: 5.80s\n",
      "Epoch: 58/190, loss: 0.0254, reconstruction: 0.0092, mmd_loss: 0.0074, Regularization: 0.0089, loss_val: 0.0166, reconstruction_val: 0.0092, mmd_loss_val: 0.0074, Time: 5.86s\n",
      "Epoch: 59/190, loss: 0.0252, reconstruction: 0.0090, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0164, reconstruction_val: 0.0090, mmd_loss_val: 0.0074, Time: 5.82s\n",
      "Epoch: 60/190, loss: 0.0251, reconstruction: 0.0088, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0164, reconstruction_val: 0.0091, mmd_loss_val: 0.0073, Time: 5.84s\n",
      "Epoch: 61/190, loss: 0.0250, reconstruction: 0.0087, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0164, reconstruction_val: 0.0087, mmd_loss_val: 0.0077, Time: 5.96s\n",
      "Epoch: 62/190, loss: 0.0249, reconstruction: 0.0086, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0160, reconstruction_val: 0.0087, mmd_loss_val: 0.0073, Time: 5.98s\n",
      "Epoch: 63/190, loss: 0.0247, reconstruction: 0.0085, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0160, reconstruction_val: 0.0085, mmd_loss_val: 0.0075, Time: 5.85s\n",
      "Epoch: 64/190, loss: 0.0245, reconstruction: 0.0083, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0159, reconstruction_val: 0.0085, mmd_loss_val: 0.0075, Time: 6.02s\n",
      "Epoch: 65/190, loss: 0.0244, reconstruction: 0.0083, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0157, reconstruction_val: 0.0083, mmd_loss_val: 0.0074, Time: 5.62s\n",
      "Epoch: 66/190, loss: 0.0242, reconstruction: 0.0081, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0157, reconstruction_val: 0.0082, mmd_loss_val: 0.0075, Time: 5.68s\n",
      "Epoch: 67/190, loss: 0.0241, reconstruction: 0.0079, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0155, reconstruction_val: 0.0080, mmd_loss_val: 0.0075, Time: 5.53s\n",
      "Epoch: 68/190, loss: 0.0241, reconstruction: 0.0079, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0153, reconstruction_val: 0.0078, mmd_loss_val: 0.0075, Time: 5.71s\n",
      "Epoch: 69/190, loss: 0.0238, reconstruction: 0.0077, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0150, reconstruction_val: 0.0078, mmd_loss_val: 0.0073, Time: 5.68s\n",
      "Epoch: 70/190, loss: 0.0237, reconstruction: 0.0076, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0150, reconstruction_val: 0.0077, mmd_loss_val: 0.0073, Time: 5.52s\n",
      "Epoch: 71/190, loss: 0.0235, reconstruction: 0.0074, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0148, reconstruction_val: 0.0075, mmd_loss_val: 0.0073, Time: 5.77s\n",
      "Epoch: 72/190, loss: 0.0235, reconstruction: 0.0073, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0150, reconstruction_val: 0.0076, mmd_loss_val: 0.0074, Time: 5.62s\n",
      "Epoch: 73/190, loss: 0.0240, reconstruction: 0.0077, mmd_loss: 0.0075, Regularization: 0.0088, loss_val: 0.0151, reconstruction_val: 0.0076, mmd_loss_val: 0.0074, Time: 5.57s\n",
      "Epoch: 74/190, loss: 0.0237, reconstruction: 0.0075, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0147, reconstruction_val: 0.0075, mmd_loss_val: 0.0072, Time: 5.75s\n",
      "Epoch: 75/190, loss: 0.0234, reconstruction: 0.0072, mmd_loss: 0.0075, Regularization: 0.0088, loss_val: 0.0147, reconstruction_val: 0.0072, mmd_loss_val: 0.0075, Time: 5.66s\n",
      "Epoch: 76/190, loss: 0.0232, reconstruction: 0.0070, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0146, reconstruction_val: 0.0072, mmd_loss_val: 0.0074, Time: 5.76s\n",
      "Epoch: 77/190, loss: 0.0230, reconstruction: 0.0069, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0144, reconstruction_val: 0.0069, mmd_loss_val: 0.0075, Time: 5.78s\n",
      "Epoch: 78/190, loss: 0.0227, reconstruction: 0.0067, mmd_loss: 0.0072, Regularization: 0.0088, loss_val: 0.0141, reconstruction_val: 0.0067, mmd_loss_val: 0.0074, Time: 5.67s\n",
      "Epoch: 79/190, loss: 0.0227, reconstruction: 0.0066, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0141, reconstruction_val: 0.0067, mmd_loss_val: 0.0074, Time: 5.73s\n",
      "Epoch: 80/190, loss: 0.0227, reconstruction: 0.0065, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0139, reconstruction_val: 0.0066, mmd_loss_val: 0.0073, Time: 5.88s\n",
      "Epoch: 81/190, loss: 0.0225, reconstruction: 0.0064, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0138, reconstruction_val: 0.0065, mmd_loss_val: 0.0073, Time: 5.94s\n",
      "Epoch: 82/190, loss: 0.0224, reconstruction: 0.0064, mmd_loss: 0.0072, Regularization: 0.0088, loss_val: 0.0138, reconstruction_val: 0.0065, mmd_loss_val: 0.0073, Time: 5.89s\n",
      "Epoch: 83/190, loss: 0.0223, reconstruction: 0.0063, mmd_loss: 0.0072, Regularization: 0.0088, loss_val: 0.0139, reconstruction_val: 0.0065, mmd_loss_val: 0.0073, Time: 5.79s\n",
      "Epoch: 84/190, loss: 0.0222, reconstruction: 0.0062, mmd_loss: 0.0072, Regularization: 0.0088, loss_val: 0.0137, reconstruction_val: 0.0063, mmd_loss_val: 0.0074, Time: 5.15s\n",
      "Epoch: 85/190, loss: 0.0223, reconstruction: 0.0063, mmd_loss: 0.0072, Regularization: 0.0088, loss_val: 0.0135, reconstruction_val: 0.0063, mmd_loss_val: 0.0072, Time: 5.66s\n",
      "Epoch: 86/190, loss: 0.0226, reconstruction: 0.0066, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0145, reconstruction_val: 0.0071, mmd_loss_val: 0.0074, Time: 5.93s\n",
      "Epoch: 87/190, loss: 0.0227, reconstruction: 0.0066, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0143, reconstruction_val: 0.0066, mmd_loss_val: 0.0076, Time: 5.56s\n",
      "Epoch: 88/190, loss: 0.0223, reconstruction: 0.0063, mmd_loss: 0.0073, Regularization: 0.0088, loss_val: 0.0135, reconstruction_val: 0.0061, mmd_loss_val: 0.0073, Time: 5.86s\n",
      "Epoch: 89/190, loss: 0.0221, reconstruction: 0.0060, mmd_loss: 0.0074, Regularization: 0.0088, loss_val: 0.0132, reconstruction_val: 0.0059, mmd_loss_val: 0.0073, Time: 5.57s\n",
      "Epoch: 90/190, loss: 0.0217, reconstruction: 0.0057, mmd_loss: 0.0073, Regularization: 0.0087, loss_val: 0.0129, reconstruction_val: 0.0058, mmd_loss_val: 0.0072, Time: 5.58s\n",
      "Epoch: 91/190, loss: 0.0215, reconstruction: 0.0056, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0130, reconstruction_val: 0.0057, mmd_loss_val: 0.0073, Time: 5.84s\n",
      "Epoch: 92/190, loss: 0.0215, reconstruction: 0.0055, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0130, reconstruction_val: 0.0056, mmd_loss_val: 0.0074, Time: 5.81s\n",
      "Epoch: 93/190, loss: 0.0214, reconstruction: 0.0055, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0129, reconstruction_val: 0.0055, mmd_loss_val: 0.0073, Time: 5.53s\n",
      "Epoch: 94/190, loss: 0.0213, reconstruction: 0.0054, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0130, reconstruction_val: 0.0055, mmd_loss_val: 0.0075, Time: 5.80s\n",
      "Epoch: 95/190, loss: 0.0213, reconstruction: 0.0053, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0126, reconstruction_val: 0.0054, mmd_loss_val: 0.0072, Time: 5.79s\n",
      "Epoch: 96/190, loss: 0.0211, reconstruction: 0.0052, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0125, reconstruction_val: 0.0053, mmd_loss_val: 0.0072, Time: 5.90s\n",
      "Epoch: 97/190, loss: 0.0210, reconstruction: 0.0051, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0125, reconstruction_val: 0.0052, mmd_loss_val: 0.0073, Time: 5.78s\n",
      "Epoch: 98/190, loss: 0.0211, reconstruction: 0.0051, mmd_loss: 0.0073, Regularization: 0.0087, loss_val: 0.0126, reconstruction_val: 0.0052, mmd_loss_val: 0.0074, Time: 5.62s\n",
      "Epoch: 99/190, loss: 0.0211, reconstruction: 0.0051, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0131, reconstruction_val: 0.0059, mmd_loss_val: 0.0072, Time: 5.61s\n",
      "Epoch: 100/190, loss: 0.0213, reconstruction: 0.0053, mmd_loss: 0.0073, Regularization: 0.0087, loss_val: 0.0130, reconstruction_val: 0.0055, mmd_loss_val: 0.0075, Time: 5.81s\n",
      "Epoch: 101/190, loss: 0.0213, reconstruction: 0.0053, mmd_loss: 0.0073, Regularization: 0.0087, loss_val: 0.0128, reconstruction_val: 0.0053, mmd_loss_val: 0.0074, Time: 5.82s\n",
      "Epoch: 102/190, loss: 0.0209, reconstruction: 0.0050, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0122, reconstruction_val: 0.0050, mmd_loss_val: 0.0073, Time: 5.80s\n",
      "Epoch: 103/190, loss: 0.0208, reconstruction: 0.0048, mmd_loss: 0.0073, Regularization: 0.0087, loss_val: 0.0121, reconstruction_val: 0.0048, mmd_loss_val: 0.0072, Time: 5.69s\n",
      "Epoch: 104/190, loss: 0.0207, reconstruction: 0.0047, mmd_loss: 0.0073, Regularization: 0.0087, loss_val: 0.0120, reconstruction_val: 0.0048, mmd_loss_val: 0.0072, Time: 5.78s\n",
      "Epoch: 105/190, loss: 0.0205, reconstruction: 0.0046, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0120, reconstruction_val: 0.0047, mmd_loss_val: 0.0073, Time: 5.84s\n",
      "Epoch: 106/190, loss: 0.0204, reconstruction: 0.0046, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0119, reconstruction_val: 0.0047, mmd_loss_val: 0.0072, Time: 5.84s\n",
      "Epoch: 107/190, loss: 0.0203, reconstruction: 0.0045, mmd_loss: 0.0071, Regularization: 0.0087, loss_val: 0.0119, reconstruction_val: 0.0047, mmd_loss_val: 0.0072, Time: 5.74s\n",
      "Epoch: 108/190, loss: 0.0204, reconstruction: 0.0045, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0120, reconstruction_val: 0.0046, mmd_loss_val: 0.0074, Time: 5.77s\n",
      "Epoch: 109/190, loss: 0.0203, reconstruction: 0.0044, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0117, reconstruction_val: 0.0045, mmd_loss_val: 0.0072, Time: 5.85s\n",
      "Epoch: 110/190, loss: 0.0203, reconstruction: 0.0044, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0121, reconstruction_val: 0.0046, mmd_loss_val: 0.0075, Time: 5.81s\n",
      "Epoch: 111/190, loss: 0.0204, reconstruction: 0.0044, mmd_loss: 0.0073, Regularization: 0.0087, loss_val: 0.0122, reconstruction_val: 0.0048, mmd_loss_val: 0.0074, Time: 5.73s\n",
      "Epoch: 112/190, loss: 0.0203, reconstruction: 0.0045, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0117, reconstruction_val: 0.0045, mmd_loss_val: 0.0072, Time: 5.79s\n",
      "Epoch: 113/190, loss: 0.0200, reconstruction: 0.0043, mmd_loss: 0.0071, Regularization: 0.0087, loss_val: 0.0116, reconstruction_val: 0.0043, mmd_loss_val: 0.0073, Time: 5.64s\n",
      "Epoch: 114/190, loss: 0.0200, reconstruction: 0.0042, mmd_loss: 0.0072, Regularization: 0.0087, loss_val: 0.0115, reconstruction_val: 0.0042, mmd_loss_val: 0.0073, Time: 5.71s\n",
      "Epoch: 115/190, loss: 0.0199, reconstruction: 0.0041, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0115, reconstruction_val: 0.0041, mmd_loss_val: 0.0074, Time: 5.73s\n",
      "Epoch: 116/190, loss: 0.0198, reconstruction: 0.0040, mmd_loss: 0.0071, Regularization: 0.0086, loss_val: 0.0113, reconstruction_val: 0.0041, mmd_loss_val: 0.0073, Time: 5.73s\n",
      "Epoch: 117/190, loss: 0.0198, reconstruction: 0.0040, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0113, reconstruction_val: 0.0041, mmd_loss_val: 0.0072, Time: 5.92s\n",
      "Epoch: 118/190, loss: 0.0198, reconstruction: 0.0039, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0114, reconstruction_val: 0.0040, mmd_loss_val: 0.0074, Time: 5.83s\n",
      "Epoch: 119/190, loss: 0.0197, reconstruction: 0.0039, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0111, reconstruction_val: 0.0039, mmd_loss_val: 0.0072, Time: 5.86s\n",
      "Epoch: 120/190, loss: 0.0197, reconstruction: 0.0039, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0112, reconstruction_val: 0.0040, mmd_loss_val: 0.0072, Time: 5.83s\n",
      "Epoch: 121/190, loss: 0.0196, reconstruction: 0.0038, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0111, reconstruction_val: 0.0039, mmd_loss_val: 0.0072, Time: 5.72s\n",
      "Epoch: 122/190, loss: 0.0195, reconstruction: 0.0038, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0111, reconstruction_val: 0.0039, mmd_loss_val: 0.0072, Time: 5.70s\n",
      "Epoch: 123/190, loss: 0.0195, reconstruction: 0.0037, mmd_loss: 0.0071, Regularization: 0.0086, loss_val: 0.0110, reconstruction_val: 0.0038, mmd_loss_val: 0.0072, Time: 5.88s\n",
      "Epoch: 124/190, loss: 0.0195, reconstruction: 0.0037, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0109, reconstruction_val: 0.0038, mmd_loss_val: 0.0071, Time: 5.86s\n",
      "Epoch: 125/190, loss: 0.0194, reconstruction: 0.0036, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0110, reconstruction_val: 0.0037, mmd_loss_val: 0.0073, Time: 5.82s\n",
      "Epoch: 126/190, loss: 0.0193, reconstruction: 0.0036, mmd_loss: 0.0071, Regularization: 0.0086, loss_val: 0.0108, reconstruction_val: 0.0036, mmd_loss_val: 0.0072, Time: 5.86s\n",
      "Epoch: 127/190, loss: 0.0193, reconstruction: 0.0035, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0108, reconstruction_val: 0.0036, mmd_loss_val: 0.0072, Time: 5.78s\n",
      "Epoch: 128/190, loss: 0.0192, reconstruction: 0.0035, mmd_loss: 0.0071, Regularization: 0.0086, loss_val: 0.0108, reconstruction_val: 0.0037, mmd_loss_val: 0.0072, Time: 5.83s\n",
      "Epoch: 129/190, loss: 0.0192, reconstruction: 0.0035, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0107, reconstruction_val: 0.0035, mmd_loss_val: 0.0072, Time: 5.88s\n",
      "Epoch: 130/190, loss: 0.0191, reconstruction: 0.0034, mmd_loss: 0.0071, Regularization: 0.0086, loss_val: 0.0108, reconstruction_val: 0.0036, mmd_loss_val: 0.0072, Time: 6.03s\n",
      "Epoch: 131/190, loss: 0.0191, reconstruction: 0.0034, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0106, reconstruction_val: 0.0035, mmd_loss_val: 0.0071, Time: 5.71s\n",
      "Epoch: 132/190, loss: 0.0190, reconstruction: 0.0033, mmd_loss: 0.0071, Regularization: 0.0086, loss_val: 0.0108, reconstruction_val: 0.0034, mmd_loss_val: 0.0074, Time: 5.86s\n",
      "Epoch: 133/190, loss: 0.0191, reconstruction: 0.0034, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0109, reconstruction_val: 0.0037, mmd_loss_val: 0.0072, Time: 5.79s\n",
      "Epoch: 134/190, loss: 0.0192, reconstruction: 0.0034, mmd_loss: 0.0072, Regularization: 0.0086, loss_val: 0.0106, reconstruction_val: 0.0034, mmd_loss_val: 0.0072, Time: 5.88s\n",
      "Epoch: 135/190, loss: 0.0190, reconstruction: 0.0033, mmd_loss: 0.0071, Regularization: 0.0086, loss_val: 0.0106, reconstruction_val: 0.0034, mmd_loss_val: 0.0072, Time: 5.48s\n",
      "Epoch: 136/190, loss: 0.0190, reconstruction: 0.0033, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0107, reconstruction_val: 0.0034, mmd_loss_val: 0.0072, Time: 5.61s\n",
      "Epoch: 137/190, loss: 0.0189, reconstruction: 0.0032, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0105, reconstruction_val: 0.0034, mmd_loss_val: 0.0071, Time: 5.66s\n",
      "Epoch: 138/190, loss: 0.0188, reconstruction: 0.0032, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0103, reconstruction_val: 0.0032, mmd_loss_val: 0.0071, Time: 5.18s\n",
      "Epoch: 139/190, loss: 0.0187, reconstruction: 0.0031, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0103, reconstruction_val: 0.0031, mmd_loss_val: 0.0072, Time: 5.81s\n",
      "Epoch: 140/190, loss: 0.0187, reconstruction: 0.0030, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0102, reconstruction_val: 0.0031, mmd_loss_val: 0.0071, Time: 4.96s\n",
      "Epoch: 141/190, loss: 0.0186, reconstruction: 0.0030, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0103, reconstruction_val: 0.0030, mmd_loss_val: 0.0072, Time: 4.84s\n",
      "Epoch: 142/190, loss: 0.0186, reconstruction: 0.0030, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0104, reconstruction_val: 0.0031, mmd_loss_val: 0.0073, Time: 5.09s\n",
      "Epoch: 143/190, loss: 0.0186, reconstruction: 0.0029, mmd_loss: 0.0072, Regularization: 0.0085, loss_val: 0.0100, reconstruction_val: 0.0030, mmd_loss_val: 0.0071, Time: 5.15s\n",
      "Epoch: 144/190, loss: 0.0185, reconstruction: 0.0029, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0101, reconstruction_val: 0.0029, mmd_loss_val: 0.0072, Time: 5.15s\n",
      "Epoch: 145/190, loss: 0.0184, reconstruction: 0.0028, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0099, reconstruction_val: 0.0029, mmd_loss_val: 0.0070, Time: 5.26s\n",
      "Epoch: 146/190, loss: 0.0186, reconstruction: 0.0029, mmd_loss: 0.0072, Regularization: 0.0085, loss_val: 0.0103, reconstruction_val: 0.0030, mmd_loss_val: 0.0072, Time: 5.36s\n",
      "Epoch: 147/190, loss: 0.0194, reconstruction: 0.0038, mmd_loss: 0.0072, Regularization: 0.0085, loss_val: 0.0117, reconstruction_val: 0.0044, mmd_loss_val: 0.0073, Time: 5.26s\n",
      "Epoch: 148/190, loss: 0.0201, reconstruction: 0.0044, mmd_loss: 0.0072, Regularization: 0.0085, loss_val: 0.0112, reconstruction_val: 0.0037, mmd_loss_val: 0.0075, Time: 5.26s\n",
      "Epoch: 149/190, loss: 0.0192, reconstruction: 0.0034, mmd_loss: 0.0073, Regularization: 0.0085, loss_val: 0.0101, reconstruction_val: 0.0031, mmd_loss_val: 0.0070, Time: 5.33s\n",
      "Epoch: 150/190, loss: 0.0186, reconstruction: 0.0030, mmd_loss: 0.0072, Regularization: 0.0085, loss_val: 0.0103, reconstruction_val: 0.0029, mmd_loss_val: 0.0074, Time: 5.33s\n",
      "Epoch: 151/190, loss: 0.0184, reconstruction: 0.0028, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0100, reconstruction_val: 0.0028, mmd_loss_val: 0.0072, Time: 5.09s\n",
      "Epoch: 152/190, loss: 0.0183, reconstruction: 0.0027, mmd_loss: 0.0072, Regularization: 0.0085, loss_val: 0.0099, reconstruction_val: 0.0028, mmd_loss_val: 0.0071, Time: 5.21s\n",
      "Epoch: 153/190, loss: 0.0183, reconstruction: 0.0026, mmd_loss: 0.0072, Regularization: 0.0085, loss_val: 0.0099, reconstruction_val: 0.0027, mmd_loss_val: 0.0073, Time: 5.14s\n",
      "Epoch: 154/190, loss: 0.0181, reconstruction: 0.0026, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0097, reconstruction_val: 0.0026, mmd_loss_val: 0.0071, Time: 5.03s\n",
      "Epoch: 155/190, loss: 0.0181, reconstruction: 0.0025, mmd_loss: 0.0071, Regularization: 0.0085, loss_val: 0.0097, reconstruction_val: 0.0026, mmd_loss_val: 0.0072, Time: 5.30s\n",
      "Epoch: 156/190, loss: 0.0181, reconstruction: 0.0025, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0097, reconstruction_val: 0.0025, mmd_loss_val: 0.0072, Time: 5.30s\n",
      "Epoch: 157/190, loss: 0.0180, reconstruction: 0.0025, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0097, reconstruction_val: 0.0025, mmd_loss_val: 0.0072, Time: 5.34s\n",
      "Epoch: 158/190, loss: 0.0180, reconstruction: 0.0024, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0096, reconstruction_val: 0.0025, mmd_loss_val: 0.0071, Time: 5.09s\n",
      "Epoch: 159/190, loss: 0.0179, reconstruction: 0.0024, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0096, reconstruction_val: 0.0024, mmd_loss_val: 0.0072, Time: 5.33s\n",
      "Epoch: 160/190, loss: 0.0179, reconstruction: 0.0024, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0095, reconstruction_val: 0.0024, mmd_loss_val: 0.0071, Time: 5.61s\n",
      "Epoch: 161/190, loss: 0.0179, reconstruction: 0.0023, mmd_loss: 0.0072, Regularization: 0.0084, loss_val: 0.0096, reconstruction_val: 0.0024, mmd_loss_val: 0.0072, Time: 5.71s\n",
      "Epoch: 162/190, loss: 0.0178, reconstruction: 0.0023, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0095, reconstruction_val: 0.0023, mmd_loss_val: 0.0072, Time: 5.73s\n",
      "Epoch: 163/190, loss: 0.0178, reconstruction: 0.0023, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0098, reconstruction_val: 0.0025, mmd_loss_val: 0.0073, Time: 5.78s\n",
      "Epoch: 164/190, loss: 0.0179, reconstruction: 0.0024, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0098, reconstruction_val: 0.0025, mmd_loss_val: 0.0073, Time: 5.71s\n",
      "Epoch: 165/190, loss: 0.0179, reconstruction: 0.0024, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0100, reconstruction_val: 0.0027, mmd_loss_val: 0.0073, Time: 5.88s\n",
      "Epoch: 166/190, loss: 0.0181, reconstruction: 0.0025, mmd_loss: 0.0072, Regularization: 0.0084, loss_val: 0.0098, reconstruction_val: 0.0025, mmd_loss_val: 0.0073, Time: 5.87s\n",
      "Epoch: 167/190, loss: 0.0178, reconstruction: 0.0023, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0095, reconstruction_val: 0.0023, mmd_loss_val: 0.0072, Time: 5.86s\n",
      "Epoch: 168/190, loss: 0.0177, reconstruction: 0.0022, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0094, reconstruction_val: 0.0022, mmd_loss_val: 0.0072, Time: 5.46s\n",
      "Epoch: 169/190, loss: 0.0176, reconstruction: 0.0021, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0095, reconstruction_val: 0.0022, mmd_loss_val: 0.0073, Time: 5.50s\n",
      "Epoch: 170/190, loss: 0.0176, reconstruction: 0.0021, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0093, reconstruction_val: 0.0021, mmd_loss_val: 0.0071, Time: 5.71s\n",
      "Epoch: 171/190, loss: 0.0175, reconstruction: 0.0021, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0092, reconstruction_val: 0.0021, mmd_loss_val: 0.0071, Time: 5.25s\n",
      "Epoch: 172/190, loss: 0.0175, reconstruction: 0.0020, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0096, reconstruction_val: 0.0023, mmd_loss_val: 0.0073, Time: 5.56s\n",
      "Epoch: 173/190, loss: 0.0178, reconstruction: 0.0023, mmd_loss: 0.0071, Regularization: 0.0084, loss_val: 0.0095, reconstruction_val: 0.0024, mmd_loss_val: 0.0072, Time: 5.69s\n",
      "Epoch: 174/190, loss: 0.0176, reconstruction: 0.0022, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0093, reconstruction_val: 0.0021, mmd_loss_val: 0.0072, Time: 5.81s\n",
      "Epoch: 175/190, loss: 0.0175, reconstruction: 0.0020, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0093, reconstruction_val: 0.0021, mmd_loss_val: 0.0072, Time: 5.64s\n",
      "Epoch: 176/190, loss: 0.0174, reconstruction: 0.0020, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0090, reconstruction_val: 0.0020, mmd_loss_val: 0.0070, Time: 5.60s\n",
      "Epoch: 177/190, loss: 0.0174, reconstruction: 0.0019, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0091, reconstruction_val: 0.0020, mmd_loss_val: 0.0071, Time: 5.71s\n",
      "Epoch: 178/190, loss: 0.0173, reconstruction: 0.0019, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0092, reconstruction_val: 0.0019, mmd_loss_val: 0.0073, Time: 5.91s\n",
      "Epoch: 179/190, loss: 0.0173, reconstruction: 0.0019, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0090, reconstruction_val: 0.0019, mmd_loss_val: 0.0071, Time: 5.91s\n",
      "Epoch: 180/190, loss: 0.0173, reconstruction: 0.0019, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0091, reconstruction_val: 0.0019, mmd_loss_val: 0.0072, Time: 5.58s\n",
      "Epoch: 181/190, loss: 0.0173, reconstruction: 0.0018, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0090, reconstruction_val: 0.0019, mmd_loss_val: 0.0072, Time: 5.80s\n",
      "Epoch: 182/190, loss: 0.0173, reconstruction: 0.0019, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0091, reconstruction_val: 0.0019, mmd_loss_val: 0.0071, Time: 5.86s\n",
      "Epoch: 183/190, loss: 0.0172, reconstruction: 0.0018, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0092, reconstruction_val: 0.0019, mmd_loss_val: 0.0073, Time: 5.82s\n",
      "Epoch: 184/190, loss: 0.0172, reconstruction: 0.0018, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0089, reconstruction_val: 0.0019, mmd_loss_val: 0.0071, Time: 5.77s\n",
      "Epoch: 185/190, loss: 0.0172, reconstruction: 0.0018, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0090, reconstruction_val: 0.0018, mmd_loss_val: 0.0072, Time: 5.63s\n",
      "Epoch: 186/190, loss: 0.0171, reconstruction: 0.0017, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0090, reconstruction_val: 0.0018, mmd_loss_val: 0.0072, Time: 5.73s\n",
      "Epoch: 187/190, loss: 0.0171, reconstruction: 0.0017, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0090, reconstruction_val: 0.0017, mmd_loss_val: 0.0072, Time: 5.61s\n",
      "Epoch: 188/190, loss: 0.0170, reconstruction: 0.0017, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0088, reconstruction_val: 0.0017, mmd_loss_val: 0.0071, Time: 5.52s\n",
      "Epoch: 189/190, loss: 0.0170, reconstruction: 0.0017, mmd_loss: 0.0071, Regularization: 0.0083, loss_val: 0.0087, reconstruction_val: 0.0017, mmd_loss_val: 0.0071, Time: 4.96s\n",
      "Epoch: 190/190, loss: 0.0170, reconstruction: 0.0016, mmd_loss: 0.0071, Regularization: 0.0082, loss_val: 0.0088, reconstruction_val: 0.0016, mmd_loss_val: 0.0071, Time: 5.76s\n",
      "模型训练完成！\n"
     ]
    }
   ],
   "source": [
    "# 3. 加载权重\n",
    "load_path = 'NOTLmodel_weights1.h5'\n",
    "model.load_weights(load_path)\n",
    "print(f\"模型权重已从 {load_path} 加载\")\n",
    "# # 2. 重新定义优化器（如果需要）\n",
    "# model.optimizer = tf.keras.optimize1rs.Adam(learning_rate=model.lr)\n",
    "\n",
    "# 然后就可以直接微调了：\n",
    "history = model.train(\n",
    "    dataset_train=(X_new_train, Y_new_train, C_new_train, L_new_train),\n",
    "    dataset_val  =(X_new_val,   Y_new_val,   C_new_val,   L_new_val),\n",
    "    epochs=190,\n",
    "    patience=50\n",
    ")\n",
    " \n",
    "print(\"模型训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重已保存至 NOTLmodel_weights1.h5\n"
     ]
    }
   ],
   "source": [
    "# 定义保存路径（保存模型的权重）\n",
    "save_path = 'NOTLmodel_weights1.h5'  # HDF5 格式保存权重文件\n",
    "\n",
    "# 保存模型权重\n",
    "model.save_weights(save_path)\n",
    "print(f\"模型权重已保存至 {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根据generate函数生成分子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_smiles(vector, char):\n",
    "    \"\"\"\n",
    "    Convert a vector to a SMILES string\n",
    "    \"\"\"\n",
    "    list_char = list(char)\n",
    "    try:\n",
    "        vector = vector.numpy()\n",
    "    except AttributeError:\n",
    "        pass  # already a NumPy array\n",
    "    vector = vector.astype(int)\n",
    "    return \"\".join(map(lambda x: list_char[x], vector)).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_condition(sub1, sub2, vocab, max_len, tokenize, map_tokens_to_ids):\n",
    "    \"\"\"\n",
    "    将两个 SMILES (sub1, sub2) 转换为拼接后的 token 条件向量 C\n",
    "    \"\"\"\n",
    "    # Tokenize 并转 ID\n",
    "    tokens1 = tokenize(sub1)\n",
    "    ids1 = map_tokens_to_ids(tokens1)\n",
    "    ids1 += [vocab['E']] * (max_len - len(ids1))  # padding\n",
    "\n",
    "    tokens2 = tokenize(sub2)\n",
    "    ids2 = map_tokens_to_ids(tokens2)\n",
    "    ids2 += [vocab['E']] * (max_len - len(ids2))\n",
    "\n",
    "    ids1_arr = np.array(ids1, dtype=np.int32)  # (N, L_sub)\n",
    "    ids2_arr = np.array(ids2, dtype=np.int32)  # (N, L_sub)\n",
    "\n",
    "    # 拼接两个序列\n",
    "    C = np.hstack((ids1_arr, ids2_arr))  # (N, 2*L_sub+1)\n",
    "    \n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_molecules(model, sub1, sub2, vocab, chars, tokenize, map_tokens_to_ids, num_samples=10, max_length=100):\n",
    "    \"\"\"\n",
    "    基于给定的底物SMILES生成新分子\n",
    "    \n",
    "    参数:\n",
    "    model: 训练好的CVAE模型\n",
    "    substrate_smiles: 底物SMILES字符串\n",
    "    vocab: 词汇表字典\n",
    "    inv_vocab: 反向词汇表字典\n",
    "    num_samples: 要生成的分子数量\n",
    "    max_length: 最大序列长度\n",
    "    \n",
    "    返回:\n",
    "    生成的SMILES列表\n",
    "    \"\"\"\n",
    "    # 将底物SMILES转换为序列\n",
    "    C = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "    \n",
    "    C = np.tile(C, (num_samples, 1))  # 复制num_samples次\n",
    "    \n",
    "    # 创建起始标记（使用'X'作为开始标记）\n",
    "    start_codon = np.array([[vocab['X']]] * num_samples)  # shape: (10, 1)\n",
    "   \n",
    "    # 生成分子\n",
    "    generated_tensors = model.generate(C, start_codon)\n",
    "   \n",
    "    # 将生成的张量转换回SMILES\n",
    "    generated_smiles = []\n",
    "    for tensor in generated_tensors:\n",
    "        token_ids = tensor.numpy()\n",
    "\n",
    "        # 找到第一个'E'的位置\n",
    "        try:\n",
    "            end_idx = np.where(token_ids == vocab['E'])[0][0]\n",
    "            # 截断到'E'之前\n",
    "            token_ids = token_ids[:end_idx]\n",
    "        except IndexError:\n",
    "            # 如果没有找到'E'，使用整个序列\n",
    "            pass\n",
    "\n",
    "        # 跳过空序列\n",
    "        if len(token_ids) == 0:\n",
    "            continue\n",
    "            \n",
    "        smiles = convert_to_smiles(token_ids, chars)\n",
    "       \n",
    "        # 验证生成的SMILES是否有效\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            generated_smiles.append(smiles)\n",
    "    \n",
    "    return generated_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "load_path = 'model_pre_weights1.h5'\n",
    "model.load_weights(load_path)\n",
    "print(f\"模型权重已从 {load_path} 加载\")\n",
    "\n",
    "sub1 = \"CCOC(=O)/C=C(\\C)c1ccccc1.ClCCl\"\n",
    "sub2 = \"[Ir+]\"\n",
    "\n",
    "# 生成分子\n",
    "generated_molecules = generate_molecules(\n",
    "    model.decoder, \n",
    "    sub1,\n",
    "    sub2, \n",
    "    vocab, \n",
    "    chars,\n",
    "    tokenize,\n",
    "    map_tokens_to_ids,\n",
    "    num_samples=100\n",
    ")\n",
    "\n",
    "# 打印生成的分子\n",
    "print(\"\\nGenerated molecules:\")\n",
    "for i, smiles in enumerate(generated_molecules, 1):\n",
    "    print(f\"{i}. {smiles}\")\n",
    "\n",
    "# 保存到 CSV 文件\n",
    "output_file = \"generate_SMILES17.csv\"\n",
    "df = pd.DataFrame({'Generated_SMILES': generated_molecules})\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nGenerated {len(generated_molecules)} valid molecules.\")\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根据generate_random_sample函数生成分子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random_sample函数\n",
    "def generate_random_molecules(model, sub1, sub2, vocab, chars, tokenize, map_tokens_to_ids, num_samples=100, max_length=165):\n",
    "    \"\"\"\n",
    "    基于给定底物 SMILES 生成新分子。\n",
    "    \"\"\"\n",
    "    C = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "    C = np.tile(C, (num_samples, 1))  # shape: [num_samples, cond_dim]\n",
    "\n",
    "    start_codon = np.array([[vocab['X']]] * num_samples)  # shape: [num_samples, 1]\n",
    "    stop_token_id = vocab['E']\n",
    "\n",
    "    # 调用 model 的方法生成\n",
    "    smiles_list = model.decoder.generate_random_sample(C, start_codon, chars, max_len=max_length, return_smiles=True, stop_token_id=stop_token_id)\n",
    "    return smiles_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_topk_molecules(model, sub1, sub2, vocab, chars, tokenize, map_tokens_to_ids, k=5, num_samples=100, max_length=165):\n",
    "    \"\"\"\n",
    "    基于给定底物 SMILES 使用 Top-k 采样生成新分子。\n",
    "    \n",
    "    Args:\n",
    "        model: 训练好的模型\n",
    "        sub1, sub2: 底物SMILES字符串\n",
    "        vocab: 词汇表字典\n",
    "        chars: token列表\n",
    "        tokenize: 分词函数\n",
    "        map_tokens_to_ids: token到ID的映射函数\n",
    "        k: Top-k采样的k值，只考虑概率最高的k个token\n",
    "        num_samples: 生成的分子数量\n",
    "        max_length: 最大序列长度\n",
    "    \n",
    "    Returns:\n",
    "        生成的SMILES字符串列表\n",
    "    \"\"\"\n",
    "    C = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "    C = np.tile(C, (num_samples, 1))  # shape: [num_samples, cond_dim]\n",
    "\n",
    "    start_codon = np.array([[vocab['X']]] * num_samples)  # shape: [num_samples, 1]\n",
    "    stop_token_id = vocab['E']\n",
    "\n",
    "    # 调用 model 的方法生成\n",
    "    smiles_list = model.decoder.generate_topk_sampling(C, start_codon, chars, k=k, max_len=max_length, return_smiles=True, stop_token_id=stop_token_id)\n",
    "    return smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_topp_molecules(model, sub1, sub2, vocab, chars, tokenize, map_tokens_to_ids, p=0.95, num_samples=100, max_length=165):\n",
    "    \"\"\"\n",
    "    基于给定底物 SMILES 使用 Top-p (nucleus) 采样生成新分子。\n",
    "    \n",
    "    Args:\n",
    "        model: 训练好的模型\n",
    "        sub1, sub2: 底物SMILES字符串\n",
    "        vocab: 词汇表字典\n",
    "        chars: token列表\n",
    "        tokenize: 分词函数\n",
    "        map_tokens_to_ids: token到ID的映射函数\n",
    "        p: Top-p采样的概率阈值，累积概率达到p时截断\n",
    "        num_samples: 生成的分子数量\n",
    "        max_length: 最大序列长度\n",
    "    \n",
    "    Returns:\n",
    "        生成的SMILES字符串列表\n",
    "    \"\"\"\n",
    "    C = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "    C = np.tile(C, (num_samples, 1))  # shape: [num_samples, cond_dim]\n",
    "\n",
    "    start_codon = np.array([[vocab['X']]] * num_samples)  # shape: [num_samples, 1]\n",
    "    stop_token_id = vocab['E']\n",
    "\n",
    "    # 调用 model 的方法生成\n",
    "    smiles_list = model.decoder.generate_topp_sampling(C, start_codon, chars, p=p, max_len=max_length, return_smiles=True, stop_token_id=stop_token_id)\n",
    "    return smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用示例代码\n",
    "def test_different_sampling_methods():\n",
    "    \"\"\"\n",
    "    测试不同采样方法的示例代码\n",
    "    \"\"\"\n",
    "    # 假设这些已经准备好了\n",
    "    sub1 = \"CCOC(=O)/C=C(\\C)c1ccccc1.ClCCl\"\n",
    "    sub2 = \"[Ir+]\"\n",
    "    num_samples = 100\n",
    "    max_length = 165\n",
    "    \n",
    "    # 创建字符映射\n",
    "    chars = [None] * len(vocab)\n",
    "    for k, v in vocab.items():\n",
    "        chars[v] = k\n",
    "    \n",
    "    print(\"=== 原始随机采样 ===\")\n",
    "    random_smiles = generate_random_molecules(model, sub1, sub2, vocab, chars, tokenize, map_tokens_to_ids,\n",
    "                                            num_samples=num_samples, max_length=max_length)\n",
    "    for i, smi in enumerate(random_smiles):\n",
    "        print(f\"{i+1}: {smi}\")\n",
    "    \n",
    "    print(\"\\n=== Top-k 采样 (k=5) ===\")\n",
    "    topk_smiles = generate_topk_molecules(model, sub1, sub2, vocab, chars, tokenize, map_tokens_to_ids,\n",
    "                                        k=5, num_samples=num_samples, max_length=max_length)\n",
    "    for i, smi in enumerate(topk_smiles):\n",
    "        print(f\"{i+1}: {smi}\")\n",
    "    \n",
    "    print(\"\\n=== Top-p 采样 (p=0.9) ===\")\n",
    "    topp_smiles = generate_topp_molecules(model, sub1, sub2, vocab, chars, tokenize, map_tokens_to_ids,\n",
    "                                        p=0.9, num_samples=num_samples, max_length=max_length)\n",
    "    for i, smi in enumerate(topp_smiles):\n",
    "        print(f\"{i+1}: {smi}\")\n",
    "    \n",
    "    # 保存结果到不同的CSV文件  \n",
    "    pd.DataFrame({'Random_Sampling': random_smiles}).to_csv('random_sampling_results.csv', index=False)\n",
    "    pd.DataFrame({'TopK_Sampling': topk_smiles}).to_csv('topk_sampling_results.csv', index=False)\n",
    "    pd.DataFrame({'TopP_Sampling': topp_smiles}).to_csv('topp_sampling_results.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n生成结果已保存到CSV文件\")\n",
    "    print(f\"随机采样: {len(random_smiles)} 个有效分子\")\n",
    "    print(f\"Top-k采样: {len(topk_smiles)} 个有效分子\")\n",
    "    print(f\"Top-p采样: {len(topp_smiles)} 个有效分子\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 原始随机采样 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:30:30] SMILES Parse Error: syntax error while parsing: cCl\\[B-][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'cCl\\[B-][' for input: 'cCl\\[B-]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: 1[NH3+][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '1[NH3+][' for input: '1[NH3+]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C@][' for input: '[C@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [P@]c1S[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[P@]c1S[' for input: '[P@]c1S['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C@H]X[PH+]NO[P+]6O[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C@H]X[PH+]NO[P+]6O[' for input: '[C@H]X[PH+]NO[P+]6O['\n",
      "[19:30:30] SMILES Parse Error: extra open parentheses for input: '[O-1]n[O-]([n+]n6'\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: poN2BrpC[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'poN2BrpC[' for input: 'poN2BrpC['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: =BrO[P@H][BH3-][S@][P@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '=BrO[P@H][BH3-][S@][P@][' for input: '=BrO[P@H][BH3-][S@][P@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [Si][O-]*[N-][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[Si][O-]*[N-][' for input: '[Si][O-]*[N-]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: 2[N+][O-]S[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '2[N+][O-]S[' for input: '2[N+][O-]S['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C]o1[O-1][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C]o1[O-1][' for input: '[C]o1[O-1]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: O21[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'O21[' for input: 'O21['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: n[BH3-][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'n[BH3-][' for input: 'n[BH3-]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [S@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[S@][' for input: '[S@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: #o[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '#o[' for input: '#o['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: X[O-1][P@][C]Cl[B-][S@]2P[B-1][c-][N+]co\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'X[O-1][P@][C]Cl[B-][S@]2P[B-1][c-][N+]co' for input: 'X[O-1][P@][C]Cl[B-][S@]2P[B-1][c-][N+]co'\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: ([n+][P@H]*[C@@]n[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '([n+][P@H]*[C@@]n[' for input: '([n+][P@H]*[C@@]n['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: P1[PH+][C@@]P[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'P1[PH+][C@@]P[' for input: 'P1[PH+][C@@]P['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [P+]1[C@@]-/[P@H]Br7[Na+]-[C]7[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[P+]1[C@@]-/[P@H]Br7[Na+]-[C]7[' for input: '[P+]1[C@@]-/[P@H]Br7[Na+]-[C]7['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: O[c-]I[S@@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'O[c-]I[S@@][' for input: 'O[c-]I[S@@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: .*)[nH]p[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '.*)[nH]p[' for input: '.*)[nH]p['\n",
      "[19:30:30] SMILES Parse Error: unclosed ring for input: '[C@H][PH]1s[S@]I[nH][C]6-[K+][PH]'\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: 5s=[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '5s=[' for input: '5s=['\n",
      "[19:30:30] SMILES Parse Error: extra close parentheses while parsing: [H])[PH+][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[H])[PH+][' for input: '[H])[PH+]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: 4[BH3-][N-1][H][C@H][P@@].N[N-1]n*[P@]#[H][C@H][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '4[BH3-][N-1][H][C@H][P@@].N[N-1]n*[P@]#[H][C@H][' for input: '4[BH3-][N-1][H][C@H][P@@].N[N-1]n*[P@]#[H][C@H]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: n[C@@][C@@][c-]*[K+](([\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'n[C@@][C@@][c-]*[K+](([' for input: 'n[C@@][C@@][c-]*[K+]((['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: X\\[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'X\\[' for input: 'X\\['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C@H][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C@H][' for input: '[C@H]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C@H]6[P@H][C][N-][H][P@@]1[H][P+][S@@][N+][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C@H]6[P@H][C][N-][H][P@@]1[H][P+][S@@][N+][' for input: '[C@H]6[P@H][C][N-][H][P@@]1[H][P+][S@@][N+]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: )So[PH]4#[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES ')So[PH]4#[' for input: ')So[PH]4#['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: =nO.[P@H][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '=nO.[P@H][' for input: '=nO.[P@H]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: P[O-][S@]6[PH]*[NH3+]Brp[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'P[O-][S@]6[PH]*[NH3+]Brp[' for input: 'P[O-][S@]6[PH]*[NH3+]Brp['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: (\\[NH3+])[H]O[PH][S@][nH][O-1][BH3-]4[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '(\\[NH3+])[H]O[PH][S@][nH][O-1][BH3-]4[' for input: '(\\[NH3+])[H]O[PH][S@][nH][O-1][BH3-]4['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [P+][N-][N][C@@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[P+][N-][N][C@@][' for input: '[P+][N-][N][C@@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [N+]N[P@H][C@@]O[n+]I[BH3-][N][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[N+]N[P@H][C@@]O[n+]I[BH3-][N][' for input: '[N+]N[P@H][C@@]O[n+]I[BH3-][N]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: P[N-1][Na+]#-o/[H]4[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'P[N-1][Na+]#-o/[H]4[' for input: 'P[N-1][Na+]#-o/[H]4['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [N-][C@@H][Si][C@@]Cl[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[N-][C@@H][Si][C@@]Cl[' for input: '[N-][C@@H][Si][C@@]Cl['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [c-]1*[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[c-]1*[' for input: '[c-]1*['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [B-]5/[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[B-]5/[' for input: '[B-]5/['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: )S[N-1]-n[C@][Si]o[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES ')S[N-1]-n[C@][Si]o[' for input: ')S[N-1]-n[C@][Si]o['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: XIP*[BH3-]=C[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'XIP*[BH3-]=C[' for input: 'XIP*[BH3-]=C['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: c[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'c[' for input: 'c['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: (s[P@@][C@H][N-1][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '(s[P@@][C@H][N-1][' for input: '(s[P@@][C@H][N-1]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C@H][S@@]//[P@@][P+][N-1][C@@H]O/[NH3+]X#[O-][PH+][N-1][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C@H][S@@]//[P@@][P+][N-1][C@@H]O/[NH3+]X#[O-][PH+][N-1][' for input: '[C@H][S@@]//[P@@][P+][N-1][C@@H]O/[NH3+]X#[O-][PH+][N-1]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [S@@]BrBr[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[S@@]BrBr[' for input: '[S@@]BrBr['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [N].N[P@H][Na+][C@]3[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[N].N[P@H][Na+][C@]3[' for input: '[N].N[P@H][Na+][C@]3['\n",
      "[19:30:30] SMILES Parse Error: extra close parentheses while parsing: I*3[P@H][nH]s[P@H]Cp[c-]7[BH3-])*[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'I*3[P@H][nH]s[P@H]Cp[c-]7[BH3-])*[' for input: 'I*3[P@H][nH]s[P@H]Cp[c-]7[BH3-])*['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: p[C@].[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'p[C@].[' for input: 'p[C@].['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C@@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C@@][' for input: '[C@@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: F[B-1]c[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'F[B-1]c[' for input: 'F[B-1]c['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [nH]n[S@@][K+].[H][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[nH]n[S@@][K+].[H][' for input: '[nH]n[S@@][K+].[H]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: I*[N][P@]o[Si]-.[nH][N-1][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'I*[N][P@]o[Si]-.[nH][N-1][' for input: 'I*[N][P@]o[Si]-.[nH][N-1]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [PH]\\No[C@H]4=[c-]#*p[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[PH]\\No[C@H]4=[c-]#*p[' for input: '[PH]\\No[C@H]4=[c-]#*p['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: =[Na+][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '=[Na+][' for input: '=[Na+]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [N][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[N][' for input: '[N]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: 7[B-1]B#\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '7[B-1]B#' for input: '7[B-1]B#'\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: I[S@][B-][C@]F[n+][c-][B-][P@@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'I[S@][B-][C@]F[n+][c-][B-][P@@][' for input: 'I[S@][B-][C@]F[n+][c-][B-][P@@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C@][' for input: '[C@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: -[n+][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '-[n+][' for input: '-[n+]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [P@][S@@][N+][N-1]731[C@@][N-1]6/#[C@H]=/[C@@H][C@H]72nO[H][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[P@][S@@][N+][N-1]731[C@@][N-1]6/#[C@H]=/[C@@H][C@H]72nO[H][' for input: '[P@][S@@][N+][N-1]731[C@@][N-1]6/#[C@H]=/[C@@H][C@H]72nO[H]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [O-1]PB6[PH+]\\[P+]BS[S@@][K+][n+]#[K+]6[C@@]=7[nH][C@H][P@@][Si]([C@H]#[c-][C@@H]-P[BH3-][PH+][PH]\\[O-1][c-][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[O-1]PB6[PH+]\\[P+]BS[S@@][K+][n+]#[K+]6[C@@]=7[nH][C@H][P@@][Si]([C@H]#[c-][C@@H]-P[BH3-][PH+][PH]\\[O-1][c-][' for input: '[O-1]PB6[PH+]\\[P+]BS[S@@][K+][n+]#[K+]6[C@@]=7[nH][C@H][P@@][Si]([C@H]#[c-][C@@H]-P[BH3-][PH+][PH]\\[O-1][c-]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: OBr[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'OBr[' for input: 'OBr['\n",
      "[19:30:30] Explicit valence for atom # 3 O, 3, is greater than permitted\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: o[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'o[' for input: 'o['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [S@]ClO[N-][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[S@]ClO[N-][' for input: '[S@]ClO[N-]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: 6[O-][BH3-]B[N-1]s[N][K+]s[O-]I[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '6[O-][BH3-]B[N-1]s[N][K+]s[O-]I[' for input: '6[O-][BH3-]B[N-1]s[N][K+]s[O-]I['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: .\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '.' for input: '.'\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [C]S[N+]p[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[C]S[N+]p[' for input: '[C]S[N+]p['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: C[C@]pn[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'C[C@]pn[' for input: 'C[C@]pn['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [P@H]X[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[P@H]X[' for input: '[P@H]X['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [H]/I[N+]/[N](*Oc[NH3+][O-][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[H]/I[N+]/[N](*Oc[NH3+][O-][' for input: '[H]/I[N+]/[N](*Oc[NH3+][O-]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: s4c[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 's4c[' for input: 's4c['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [n+][P@][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[n+][P@][' for input: '[n+][P@]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [Na+][Si][n+][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[Na+][Si][n+][' for input: '[Na+][Si][n+]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: C[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'C[' for input: 'C['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: O[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'O[' for input: 'O['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: .[S@@][N+].[C@]p[P@H][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '.[S@@][N+].[C@]p[P@H][' for input: '.[S@@][N+].[C@]p[P@H]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: p[N][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'p[N][' for input: 'p[N]['\n",
      "[19:30:30] Explicit valence for atom # 5 Na, 2, is greater than permitted\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [N-].4[S@][P+]BP/[C@@H][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[N-].4[S@][P+]BP/[C@@H][' for input: '[N-].4[S@][P+]BP/[C@@H]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: F[P@@]I[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'F[P@@]I[' for input: 'F[P@@]I['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: Br[\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES 'Br[' for input: 'Br['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [P@@][P+][N+]-[H]c2[B-1]O[S@][c-][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[P@@][P+][N+]-[H]c2[B-1]O[S@][c-][' for input: '[P@@][P+][N+]-[H]c2[B-1]O[S@][c-]['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:30] SMILES Parse Error: syntax error while parsing: [N-]N([B-1][\n",
      "[19:30:30] SMILES Parse Error: Failed parsing SMILES '[N-]N([B-1][' for input: '[N-]N([B-1]['\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top-k 采样 (k=5) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 4 to itself for input: '[C@H]cn[C@H]p222[C@@H][C@@H]4[C@@H]4n[C@H][C@@H]4424[C@@H][C@H]nncccccc[C@@H]c[C@@H]ncn4s442222O3=OO=2[C@@H][C@H]cc45sP[C@H]cc4s24occ[C@H]c22o4423424ns423O4c'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: 1(CC[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '1(CC[' for input: '1(CC['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: O[Si]=OO1O)11=))(((1FFO)(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'O[Si]=OO1O)11=))(((1FFO)(([' for input: 'O[Si]=OO1O)11=))(((1FFO)((['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@@H]ccnn233))[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]ccnn233))[' for input: '[C@@H]ccnn233))['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: F=)1OS=1=OON22O1[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'F=)1OS=1=OON22O1[' for input: 'F=)1OS=1=OON22O1['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: F[C@H]cpcnc[C@H]n23)(C(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'F[C@H]cpcnc[C@H]n23)(C(([' for input: 'F[C@H]cpcnc[C@H]n23)(C((['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: Pc[C@@H]pcn4[C@@H]n[C@@H]nnnon[C@@H][C@@H][C@@H][C@@H]4n[C@@H]o4[C@@H]no44oncccccccc[C@H]c45244[C@@H][C@H][C@H][C@@H][C@@H][C@@H][C@@H]o224P[C@@H]4425[C@@H]o[C@@H][C@H][C@H]cccnccc455OO[C@@H]453O=O[C@@H][C@H]p4[C@@H]4cncn224[C@H]4cc4c[C@H]c[C@H]53ONO==N[C@@H]N33N424o24[C@@H]24[C@@H]422424424\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'Pc[C@@H]pcn4[C@@H]n[C@@H]nnnon[C@@H][C@@H][C@@H][C@@H]4n[C@@H]o4[C@@H]no44oncccccccc[C@H]c45244[C@@H][C@H][C@H][C@@H][C@@H][C@@H][C@@H]o224P[C@@H]4425[C@@H]o[C@@H][C@H][C@H]cccnccc455OO[C@@H]453O=O[C@@H][C@H]p4[C@@H]4cncn224[C@H]4cc4c[C@H]c[C@H]53ONO==N[C@@H]N33N424o24[C@@H]24[C@@H]422424424' for input: 'Pc[C@@H]pcn4[C@@H]n[C@@H]nnnon[C@@H][C@@H][C@@H][C@@H]4n[C@@H]o4[C@@H]no44oncccccccc[C@H]c45244[C@@H][C@H][C@H][C@@H][C@@H][C@@H][C@@H]o224P[C@@H]4425[C@@H]o[C@@H][C@H][C@H]cccnccc455OO[C@@H]453O=O[C@@H][C@H]p4[C@@H]4cncn224[C@H]4cc4c[C@H]c[C@H]53ONO==N[C@@H]N33N424o24[C@@H]24[C@@H]422424424'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: F[C@H]cc[C@H]211(C))=NP2Oo[C@H]p[C@@H]2s[C@@H][C@H][C@H]ns42[C@@H]Pnnnnn42O=[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'F[C@H]cc[C@H]211(C))=NP2Oo[C@H]p[C@@H]2s[C@@H][C@H][C@H]ns42[C@@H]Pnnnnn42O=[' for input: 'F[C@H]cc[C@H]211(C))=NP2Oo[C@H]p[C@@H]2s[C@@H][C@H][C@H]ns42[C@@H]Pnnnnn42O=['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [Si]OO=(CC1(C)([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[Si]OO=(CC1(C)([' for input: '[Si]OO=(CC1(C)(['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@H]n[C@H]n222OO=Nopccp22[C@@H]o4p[C@@H]22OOOOS=NoN[C@@H]op[C@@H]44ppc[C@@H]25FO3==S)3O=ONNS=O)))((CC11([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H]n[C@H]n222OO=Nopccp22[C@@H]o4p[C@@H]22OOOOS=NoN[C@@H]op[C@@H]44ppc[C@@H]25FO3==S)3O=ONNS=O)))((CC11([' for input: '[C@H]n[C@H]n222OO=Nopccp22[C@@H]o4p[C@@H]22OOOOS=NoN[C@@H]op[C@@H]44ppc[C@@H]25FO3==S)3O=ONNS=O)))((CC11(['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@@H]cc[C@H]c[C@@H]4o[C@@H]2222O3O3)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]cc[C@H]c[C@@H]4o[C@@H]2222O3O3)[' for input: '[C@@H]cc[C@H]c[C@@H]4o[C@@H]2222O3O3)['\n",
      "[19:30:34] SMILES Parse Error: ring closure 2 duplicates bond between atom 28 and atom 29 for input: 'PcccccP[C@H]cc[C@H]c[C@H]nc[C@H]c[C@H]cc[C@@H]n[C@@H]cc42nc[C@@H]s22s2424c4cn[C@@H]cc[C@H]2nccnc[C@H]cc[C@H]4'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: 1O(11)([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '1O(11)([' for input: '1O(11)(['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@H][C@H]cpcP21O)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H][C@H]cpcP21O)[' for input: '[C@H][C@H]cpcP21O)['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@H][C@H]p[C@H]ccn24on22O)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H][C@H]p[C@H]ccn24on22O)[' for input: '[C@H][C@H]p[C@H]ccn24on22O)['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@H]Pcpcn24242223=O)O3O=O33NN[C@@H]o4ppnccc454np44c[C@H]n42s[C@@H]22o2522OO224o4242son\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H]Pcpcn24242223=O)O3O=O33NN[C@@H]o4ppnccc454np44c[C@H]n42s[C@@H]22o2522OO224o4242son' for input: '[C@H]Pcpcn24242223=O)O3O=O33NN[C@@H]o4ppnccc454np44c[C@H]n42s[C@@H]22o2522OO224o4242son'\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 10 to itself for input: '[C@@H][C@H]cc[C@H]Pnnsn[C@@H]22oO3OO'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@H][C@H][C@H][C@H]pp[C@@H]s3)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H][C@H][C@H][C@H]pp[C@@H]s3)[' for input: '[C@H][C@H][C@H][C@H]pp[C@@H]s3)['\n",
      "[19:30:34] SMILES Parse Error: ring closure 2 duplicates bond between atom 32 and atom 33 for input: 'FNOOFFNoopo24pcp4cn424[C@@H]4ccc55-OOOO3=ONo4po253OF[C@@H]p45522o24[C@@H]25F2F2[C@@H][C@@H]c[C@@H]nc[C@@H]44nn[C@@H]42222'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: ccccc[C@H]231[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'ccccc[C@H]231[' for input: 'ccccc[C@H]231['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 14 to itself for input: '[Si][C@@H]pp[C@H]nc[C@H]44n2n[C@H]4s2snn24422oooncn454n222o[C@@H][C@@H][C@@H]25o24[C@@H]4224n4ccc4scn2'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: CC)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'CC)[' for input: 'CC)['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: F=ONS=O/1O/=//=O==/O/)))[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'F=ONS=O/1O/=//=O==/O/)))[' for input: 'F=ONS=O/1O/=//=O==/O/)))['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H][C@@H]p[C@H]c[C@H]c[C@H]223=3OO3==N[C@@H]N3=3NN2NOOFO=)(1O==ONO))((([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H][C@@H]p[C@H]c[C@H]c[C@H]223=3OO3==N[C@@H]N3=3NN2NOOFO=)(1O==ONO))((([' for input: '[C@@H][C@@H]p[C@H]c[C@H]c[C@H]223=3OO3==N[C@@H]N3=3NN2NOOFO=)(1O==ONO))(((['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: nccccpcccp42nn24o22O==O=OOOONN[C@@H]pp4cc[C@@H]23-==33ONo[C@@H]44423OOON[C@@H]4423==)))3O==N44ccc44s4\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'nccccpcccp42nn24o22O==O=OOOONN[C@@H]pp4cc[C@@H]23-==33ONo[C@@H]44423OOON[C@@H]4423==)))3O==N44ccc44s4' for input: 'nccccpcccp42nn24o22O==O=OOOONN[C@@H]pp4cc[C@@H]23-==33ONo[C@@H]44423OOON[C@@H]4423==)))3O==N44ccc44s4'\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 23 to itself for input: 'ccccPPPc[C@H][C@H][C@@H]ncc[C@H]ccn4cn4nscn2444222ncccss'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: OO((C))1)OOSNFOO==([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'OO((C))1)OOSNFOO==([' for input: 'OO((C))1)OOSNFOO==(['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: OPPc[C@@H]2n[C@@H][C@@H][C@@H]n22[C@@H][C@@H]cc[C@H]24[C@@H]n[C@@H]cnccc[C@@H]n23OO==N22n2s23\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'OPPc[C@@H]2n[C@@H][C@@H][C@@H]n22[C@@H][C@@H]cc[C@H]24[C@@H]n[C@@H]cnccc[C@@H]n23OO==N22n2s23' for input: 'OPPc[C@@H]2n[C@@H][C@@H][C@@H]n22[C@@H][C@@H]cc[C@H]24[C@@H]n[C@@H]cnccc[C@@H]n23OO==N22n2s23'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: FC((((((()(1((((([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'FC((((((()(1((((([' for input: 'FC((((((()(1(((((['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: C)C[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'C)C[' for input: 'C)C['\n",
      "[19:30:34] SMILES Parse Error: ring closure 2 duplicates bond between atom 2 and atom 3 for input: 'Oc[C@@H]2s2on4ncc4s2n2n'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: CCCCCCCC1)=)11OOOOOO/=OONN[C@@H]pc[C@H]2sso[C@@H]23-NOONoocnsnPcP4244ccn4ccn44ccn23))O2N24Pccccns4PPnccP2s4cc4233OOONN4[C@@H]cccn244cPnc4nn4c[C@H]42s22s4cc222ss2nc4s44c4223)\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC1)=)11OOOOOO/=OONN[C@@H]pc[C@H]2sso[C@@H]23-NOONoocnsnPcP4244ccn4ccn44ccn23))O2N24Pccccns4PPnccP2s4cc4233OOONN4[C@@H]cccn244cPnc4nn4c[C@H]42s22s4cc222ss2nc4s44c4223)' for input: 'CCCCCCCC1)=)11OOOOOO/=OONN[C@@H]pc[C@H]2sso[C@@H]23-NOONoocnsnPcP4244ccn4ccn44ccn23))O2N24Pccccns4PPnccP2s4cc4233OOONN4[C@@H]cccn244cPnc4nn4c[C@H]42s22s4cc222ss2nc4s44c4223)'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: OOOO11O)O1OO1)OOOO/=O=NNOOO1)ONNpN2O2Noo[C@@H]2o222p[C@@H]4p2oooopon44opcnpo4oonnnoono4o4on222[C@@H]pp44oo4424o[C@@H][C@@H]4ncp244o244o4[C@@H]o4o[C@@H]n444423131)(1O\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'OOOO11O)O1OO1)OOOO/=O=NNOOO1)ONNpN2O2Noo[C@@H]2o222p[C@@H]4p2oooopon44opcnpo4oonnnoono4o4on222[C@@H]pp44oo4424o[C@@H][C@@H]4ncp244o244o4[C@@H]o4o[C@@H]n444423131)(1O' for input: 'OOOO11O)O1OO1)OOOO/=O=NNOOO1)ONNpN2O2Noo[C@@H]2o222p[C@@H]4p2oooopon44opcnpo4oonnnoono4o4on222[C@@H]pp44oo4424o[C@@H][C@@H]4ncp244o244o4[C@@H]o4o[C@@H]n444423131)(1O'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: F11(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'F11(([' for input: 'F11((['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: O[C@H]cPc4cnc[C@@H]44422O3O3)O=1(C(1O3So[C@@H]22[C@@H]npc44s222o[C@@H]44s[C@@H][C@@H]444n4n[C@@H]2sO22OO2ncc[C@@H]233=\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'O[C@H]cPc4cnc[C@@H]44422O3O3)O=1(C(1O3So[C@@H]22[C@@H]npc44s222o[C@@H]44s[C@@H][C@@H]444n4n[C@@H]2sO22OO2ncc[C@@H]233=' for input: 'O[C@H]cPc4cnc[C@@H]44422O3O3)O=1(C(1O3So[C@@H]22[C@@H]npc44s222o[C@@H]44s[C@@H][C@@H]444n4n[C@@H]2sO22OO2ncc[C@@H]233='\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: Fc[C@@H]44422on444222[C@@H][C@@H]25OONO3333OoO4o2oo[C@@H]cc4c4n44o424o25o42244p4ccc[C@H]n44452Ooo42oop44o22o242o[C@@H]pp2soo224pcpcn5333)OOo44424[C@@H]4ccpcn53)))=O))[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'Fc[C@@H]44422on444222[C@@H][C@@H]25OONO3333OoO4o2oo[C@@H]cc4c4n44o424o25o42244p4ccc[C@H]n44452Ooo42oop44o22o242o[C@@H]pp2soo224pcpcn5333)OOo44424[C@@H]4ccpcn53)))=O))[' for input: 'Fc[C@@H]44422on444222[C@@H][C@@H]25OONO3333OoO4o2oo[C@@H]cc4c4n44o424o25o42244p4ccc[C@H]n44452Ooo42oop44o22o242o[C@@H]pp2soo224pcpcn5333)OOo44424[C@@H]4ccpcn53)))=O))['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@@H]cn4ssnooo[C@@H]4soo4ono4n4224Pc4n444n4nos244o4n42O)(O))[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]cn4ssnooo[C@@H]4soo4ono4n4224Pc4n444n4nos244o4n42O)(O))[' for input: '[C@@H]cn4ssnooo[C@@H]4soo4ono4n4224Pc4n444n4nos244o4n42O)(O))['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: C)C()([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'C)C()([' for input: 'C)C()(['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [Si]OOO))1=)1(1)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[Si]OOO))1=)1(1)[' for input: '[Si]OOO))1=)1(1)['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: OO1C)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'OO1C)[' for input: 'OO1C)['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 32 to itself for input: 'cP44cnc[C@@H]ccccnn2n[C@@H][C@@H]n2n4[C@@H]nc453=N[C@@H][C@@H]4cns424cccc[C@@H]s2s2422[C@@H][C@@H]42nccc424c424c'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@@H]np4n2sooo22O)))OFO)))CC(())3ONo4p4p4oc[C@H]54P[C@@H]4oo24oncccc2444cc[C@H]s423\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]np4n2sooo22O)))OFO)))CC(())3ONo4p4p4oc[C@H]54P[C@@H]4oo24oncccc2444cc[C@H]s423' for input: '[C@@H]np4n2sooo22O)))OFO)))CC(())3ONo4p4p4oc[C@H]54P[C@@H]4oo24oncccc2444cc[C@H]s423'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H]nc[C@@H]233OFONO3==)(C)1O2NN4pc4n2223=)()[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]nc[C@@H]233OFONO3==)(C)1O2NN4pc4n2223=)()[' for input: '[C@@H]nc[C@@H]233OFONO3==)(C)1O2NN4pc4n2223=)()['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: F1=O==O)1)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'F1=O==O)1)[' for input: 'F1=O==O)1)['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H][C@H]pcn[C@H]c[C@H]c425O3==[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H][C@H]pcn[C@H]c[C@H]c425O3==[' for input: '[C@@H][C@H]pcn[C@H]c[C@H]c425O3==['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: (C[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '(C[' for input: '(C['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: FO1)(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'FO1)(([' for input: 'FO1)((['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: 1C[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '1C[' for input: '1C['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: FO1(C(1([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'FO1(C(1([' for input: 'FO1(C(1(['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: O==1=OO=S=3O=OO====3==O[C@@H][C@@H]n424[C@@H]n4444n22O3Oon[C@@H]4cc4c[C@H]nn522O3=)3)3O\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'O==1=OO=S=3O=OO====3==O[C@@H][C@@H]n424[C@@H]n4444n22O3Oon[C@@H]4cc4c[C@H]nn522O3=)3)3O' for input: 'O==1=OO=S=3O=OO====3==O[C@@H][C@@H]n424[C@@H]n4444n22O3Oon[C@@H]4cc4c[C@H]nn522O3=)3)3O'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@H][C@@H][C@H][C@@H]c[C@H]ps2[C@@H]nn[C@@H]nP2O))((C[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H][C@@H][C@H][C@@H]c[C@H]ps2[C@@H]nn[C@@H]nP2O))((C[' for input: '[C@H][C@@H][C@H][C@@H]c[C@H]ps2[C@@H]nn[C@@H]nP2O))((C['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: C))1)=FNFFOSSFO)OOFO=11OOO=)O)((111=FOO=)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'C))1)=FNFFOSSFO)OOFO=11OOO=)O)((111=FOO=)[' for input: 'C))1)=FNFFOSSFO)OOFO=11OOO=)O)((111=FOO=)['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: O[C@@H][C@@H][C@@H]4nnn4233F))C(()(1F))))OFFo\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'O[C@@H][C@@H][C@@H]4nnn4233F))C(()(1F))))OFFo' for input: 'O[C@@H][C@@H][C@@H]4nnn4233F))C(()(1F))))OFFo'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: Fc[C@H]p[C@H]cPs24on24nnc[C@H]ccn2n44[C@@H]nnccn2244[C@@H]4[C@@H]o2223O[C@@H]424n44224244[C@@H]24n24[C@@H][C@@H]4c[C@H]nnn23=3)=N[C@@H]ooo44n4[C@@H][C@@H]n[C@@H]42342n444c4232[C@@H][C@@H]2nn\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'Fc[C@H]p[C@H]cPs24on24nnc[C@H]ccn2n44[C@@H]nnccn2244[C@@H]4[C@@H]o2223O[C@@H]424n44224244[C@@H]24n24[C@@H][C@@H]4c[C@H]nnn23=3)=N[C@@H]ooo44n4[C@@H][C@@H]n[C@@H]42342n444c4232[C@@H][C@@H]2nn' for input: 'Fc[C@H]p[C@H]cPs24on24nnc[C@H]ccn2n44[C@@H]nnccn2244[C@@H]4[C@@H]o2223O[C@@H]424n44224244[C@@H]24n24[C@@H][C@@H]4c[C@H]nnn23=3)=N[C@@H]ooo44n4[C@@H][C@@H]n[C@@H]42342n444c4232[C@@H][C@@H]2nn'\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 2 to itself for input: '[C@H]cc422s4nccc[C@@H]424o44ncccc[C@H]255occc4cc4ccc[C@H]5s44c4c[C@@H]n54422o252o2[C@@H]ncc[C@H]csn44c4s4232224nc4nn2'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: O1)(CC1((C11==O=1([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'O1)(CC1((C11==O=1([' for input: 'O1)(CC1((C11==O=1(['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 9 to itself for input: 'O[C@@H]P[C@@H]n2ss42s4c[C@@H]224nnnsn442snc[C@H]c[C@@H]4cnsnccc44cncnsnnc[C@H]ccccccc[C@H]s4c4ccc[C@H]cc[C@H]4n4c[C@@H]c[C@@H]n'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@H]p[C@@H][C@H]442soN2[C@@H]pn[C@@H]ono24o[C@@H]nnnpp4n[C@@H]25FOFNNN[C@@H][C@@H][C@@H][C@@H][C@@H]p45O-NN5O3O3O)C([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H]p[C@@H][C@H]442soN2[C@@H]pn[C@@H]ono24o[C@@H]nnnpp4n[C@@H]25FOFNNN[C@@H][C@@H][C@@H][C@@H][C@@H]p45O-NN5O3O3O)C([' for input: '[C@H]p[C@@H][C@H]442soN2[C@@H]pn[C@@H]ono24o[C@@H]nnnpp4n[C@@H]25FOFNNN[C@@H][C@@H][C@@H][C@@H][C@@H]p45O-NN5O3O3O)C(['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: 1((([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '1((([' for input: '1(((['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@H]ccccn23)))())=NNOo[C@@H]ppnp4cp4np[C@@H]4cc[C@H]cn442OOopppn4cpcn422N4o4[C@@H]4onppnnnncc55-3)3NOOO=O=3)3No[C@@H]44nccp563=3))(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H]ccccn23)))())=NNOo[C@@H]ppnp4cp4np[C@@H]4cc[C@H]cn442OOopppn4cpcn422N4o4[C@@H]4onppnnnncc55-3)3NOOO=O=3)3No[C@@H]44nccp563=3))(([' for input: '[C@H]ccccn23)))())=NNOo[C@@H]ppnp4cp4np[C@@H]4cc[C@H]cn442OOopppn4cpcn422N4o4[C@@H]4onppnnnncc55-3)3NOOO=O=3)3No[C@@H]44nccp563=3))((['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@@H]Pnnp22oo[C@@H]23)O))))=FNO11))[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]Pnnp22oo[C@@H]23)O))))=FNO11))[' for input: '[C@@H]Pnnp22oo[C@@H]23)O))))=FNO11))['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H]Ppc[C@H][C@H]cc[C@@H]n2222=[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]Ppc[C@H][C@H]cc[C@@H]n2222=[' for input: '[C@@H]Ppc[C@H][C@H]cc[C@@H]n2222=['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: OC))(()OFSOFO==)OONoNOOFOONOO=O))===)=)=FF[C@@H][C@H][C@H]pc[C@@H]43O3FOONO))O=OON==ON[C@@H][C@@H]\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'OC))(()OFSOFO==)OONoNOOFOONOO=O))===)=)=FF[C@@H][C@H][C@H]pc[C@@H]43O3FOONO))O=OON==ON[C@@H][C@@H]' for input: 'OC))(()OFSOFO==)OONoNOOFOONOO=O))===)=)=FF[C@@H][C@H][C@H]pc[C@@H]43O3FOONO))O=OON==ON[C@@H][C@@H]'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H]P[C@H]ccccc[C@H][C@H]422o[C@H][C@H]c[C@@H]4s2[C@@H][C@H]cn24[C@@H][C@@H]2333OON[C@@H][C@H]c[C@H]23ON[C@H]c[C@@H]s44sn[C@H]444[C@@H]24[C@@H]4nnccccn54cc4c[C@@H]n4c[C@@H]4423==\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]P[C@H]ccccc[C@H][C@H]422o[C@H][C@H]c[C@@H]4s2[C@@H][C@H]cn24[C@@H][C@@H]2333OON[C@@H][C@H]c[C@H]23ON[C@H]c[C@@H]s44sn[C@H]444[C@@H]24[C@@H]4nnccccn54cc4c[C@@H]n4c[C@@H]4423==' for input: '[C@@H]P[C@H]ccccc[C@H][C@H]422o[C@H][C@H]c[C@@H]4s2[C@@H][C@H]cn24[C@@H][C@@H]2333OON[C@@H][C@H]c[C@H]23ON[C@H]c[C@@H]s44sn[C@H]444[C@@H]24[C@@H]4nnccccn54cc4c[C@@H]n4c[C@@H]4423=='\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: 1C1C[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '1C1C[' for input: '1C1C['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: 1C(11=(((([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '1C(11=(((([' for input: '1C(11=((((['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 6 to itself for input: '[C@H]p[C@H]cc4nn25-2O2N22O32O2O2[C@@H]n4c[C@@H]ns2o4[C@@H]4ccn4cn53OOO[C@@H]P254[C@@H]n4cccPn2nn4scnn2so4n4cnc[C@H]sn22'\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 10 to itself for input: 'O[C@@H]Ppcc4cPccc4224[C@@H][C@H]n[C@H][C@H]53OOO[C@@H][C@H]ncc[C@H]cnccc53O=O[C@@H][C@H]c4424224[C@@H]424o[C@@H]442s4c[C@H]ccnc422s4n[C@@H]nc422ncccc4nsnncc2ccccccc2'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: c[C@H]pcppn[C@@H][C@@H]oO3[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'c[C@H]pcppn[C@@H][C@@H]oO3[' for input: 'c[C@H]pcppn[C@@H][C@@H]oO3['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@H][C@H]Ppn23-)OF33)1=N[C@@H][C@@H]Poo23OOOO=)O[C@@H]Pp[C@H]c4nccn23-2O[C@@H]NO3NO2oO=OOO)===O[C@@H]PNNo2[C@@H]21OFONF11(11))(C1)OFFOFo\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H][C@H]Ppn23-)OF33)1=N[C@@H][C@@H]Poo23OOOO=)O[C@@H]Pp[C@H]c4nccn23-2O[C@@H]NO3NO2oO=OOO)===O[C@@H]PNNo2[C@@H]21OFONF11(11))(C1)OFFOFo' for input: '[C@H][C@H]Ppn23-)OF33)1=N[C@@H][C@@H]Poo23OOOO=)O[C@@H]Pp[C@H]c4nccn23-2O[C@@H]NO3NO2oO=OOO)===O[C@@H]PNNo2[C@@H]21OFONF11(11))(C1)OFFOFo'\n",
      "[19:30:34] SMILES Parse Error: unclosed ring for input: '[C@@H][C@@H]c[C@H]ccncnscp4sos4s2on4ss4n44'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [Si]NOSSNOFSN[C@@H]c[C@@H]54P[C@@H]2244[C@@H]44444n44c[C@@H]4553)=3ONN[C@@H][C@@H][C@@H]424nc[C@@H]4n24n44442233)(1()\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[Si]NOSSNOFSN[C@@H]c[C@@H]54P[C@@H]2244[C@@H]44444n44c[C@@H]4553)=3ONN[C@@H][C@@H][C@@H]424nc[C@@H]4n24n44442233)(1()' for input: '[Si]NOSSNOFSN[C@@H]c[C@@H]54P[C@@H]2244[C@@H]44444n44c[C@@H]4553)=3ONN[C@@H][C@@H][C@@H]424nc[C@@H]4n24n44442233)(1()'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H]cccn23OONN2NNN[C@@H][C@@H]22oO2[C@@H]pp425OFF=))())FF3))F333)(()OOFOFF33)==F))(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]cccn23OONN2NNN[C@@H][C@@H]22oO2[C@@H]pp425OFF=))())FF3))F333)(()OOFOFF33)==F))(([' for input: '[C@@H]cccn23OONN2NNN[C@@H][C@@H]22oO2[C@@H]pp425OFF=))())FF3))F333)(()OOFOFF33)==F))((['\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: O1)(1==FO1(1)1OFNFFNFF[C@@H][C@@H][C@@H][C@@H]4cn553OF[C@@H]o[C@H]cc4255NN[C@@H][C@H][C@H][C@H]44[C@@H]ccp56-[C@@H]ppcn4244ocp4c[C@H]cc4c[C@H]53OONNNN3))))3NN44[C@@H]n4n[C@@H]4n424n4nc444n44s4ccc[C@@H]42233=11O\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'O1)(1==FO1(1)1OFNFFNFF[C@@H][C@@H][C@@H][C@@H]4cn553OF[C@@H]o[C@H]cc4255NN[C@@H][C@H][C@H][C@H]44[C@@H]ccp56-[C@@H]ppcn4244ocp4c[C@H]cc4c[C@H]53OONNNN3))))3NN44[C@@H]n4n[C@@H]4n424n4nc444n44s4ccc[C@@H]42233=11O' for input: 'O1)(1==FO1(1)1OFNFFNFF[C@@H][C@@H][C@@H][C@@H]4cn553OF[C@@H]o[C@H]cc4255NN[C@@H][C@H][C@H][C@H]44[C@@H]ccp56-[C@@H]ppcn4244ocp4c[C@H]cc4c[C@H]53OONNNN3))))3NN44[C@@H]n4n[C@@H]4n424n4nc444n44s4ccc[C@@H]42233=11O'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: OC[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'OC[' for input: 'OC['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H]cc[C@H]ccPp4c[C@H]c[C@@H]cc4[C@@H]c[C@@H]cn4233===No4no442544c[C@@H]sc44ccn25socpc22\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]cc[C@H]ccPp4c[C@H]c[C@@H]cc4[C@@H]c[C@@H]cn4233===No4no442544c[C@@H]sc44ccn25socpc22' for input: '[C@@H]cc[C@H]ccPp4c[C@H]c[C@@H]cc4[C@@H]c[C@@H]cn4233===No4no442544c[C@@H]sc44ccn25socpc22'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@@H]ccpccpcccncPcnsncn2222O3ONO33)(1)(11FO2[C@@H]o[C@@H]21)(((((1(O\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]ccpccpcccncPcnsncn2222O3ONO33)(1)(11FO2[C@@H]o[C@@H]21)(((((1(O' for input: '[C@@H]ccpccpcccncPcnsncn2222O3ONO33)(1)(11FO2[C@@H]o[C@@H]21)(((((1(O'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: F11C([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'F11C([' for input: 'F11C(['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H][C@H]ccncccPnn[C@@H]2OO=O[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H][C@H]ccncccPnn[C@@H]2OO=O[' for input: '[C@@H][C@H]ccncccPnn[C@@H]2OO=O['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 4 bonds atom 9 to itself for input: 'OO2cn2nn4ns4c[C@@H]44424nccPc[C@H]cnc42so4cccccsn4c[C@H]44so42sncnn25'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: 1C)CC[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '1C)CC[' for input: '1C)CC['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 11 to itself for input: 'cP[C@H]pn[C@@H]c44cncc[C@H]4442424ooocc[C@H]4s2s22'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: O(1()((1(O=SOFFFO=))(C1)(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'O(1()((1(O=SOFFFO=))(C1)(([' for input: 'O(1()((1(O=SOFFFO=))(C1)((['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H][C@H]cc4cn222nn[C@@H]c[C@@H]n[C@@H][C@@H]nc4cc[C@@H]ncn5so[C@@H]2son4c44c[C@H]cc44s33==3)1)O11\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H][C@H]cc4cn222nn[C@@H]c[C@@H]n[C@@H][C@@H]nc4cc[C@@H]ncn5so[C@@H]2son4c44c[C@H]cc44s33==3)1)O11' for input: '[C@@H][C@H]cc4cn222nn[C@@H]c[C@@H]n[C@@H][C@@H]nc4cc[C@@H]ncn5so[C@@H]2son4c44c[C@H]cc44s33==3)1)O11'\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 16 to itself for input: '[C@H]cpccc[C@H]cccc44Pccc4cp4222Nn[C@@H]4cc444pccn2socc[C@H]n4444ncc[C@H]c44c4ccccP4554p422s23ON444424o2354cp234[C@@H]24nn4c[C@H]nn2322ooon4n4s22oc4s4c[C@@H]4c4snnc4c4244ccn4c44cccccnss2c4nc[C@H][C@H]4n4cccc[C@H]'\n",
      "[19:30:34] SMILES Parse Error: unclosed ring for input: 'OFNSNoFo[C@@H]n42oF4[C@@H][C@@H]233O33O=ONN2[C@@H]onc[C@@H]c[C@H]2snc[C@H]c[C@@H]s332[C@@H]nc422'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: CC[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'CC[' for input: 'CC['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: CCCC()(C)(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'CCCC()(C)(([' for input: 'CCCC()(C)((['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 22 to itself for input: '[C@@H][C@H]cccnc44n[C@@H]44ccnnn4[C@@H]ncc[C@H]4ccn4n2442222o4n2scc4cccc4n23244442'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: O[C@H]p[C@H][C@H]cccc[C@H]25O)O=NN2O=OFOONO13OONO2FOOo[C@@H]poccp44223)1O11F[C@@H]pc[C@H]444cc[C@H]cc44nc[C@H]4c[C@@H]cc4n232\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'O[C@H]p[C@H][C@H]cccc[C@H]25O)O=NN2O=OFOONO13OONO2FOOo[C@@H]poccp44223)1O11F[C@@H]pc[C@H]444cc[C@H]cc44nc[C@H]4c[C@@H]cc4n232' for input: 'O[C@H]p[C@H][C@H]cccc[C@H]25O)O=NN2O=OFOONO13OONO2FOOo[C@@H]poccp44223)1O11F[C@@H]pc[C@H]444cc[C@H]cc44nc[C@H]4c[C@@H]cc4n232'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@H]Pppns22-OOO))))(([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H]Pppns22-OOO))))(([' for input: '[C@H]Pppns22-OOO))))((['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 8 to itself for input: '[C@H]c[C@H]44c[C@H]n442oo4o2222o22[C@@H]o24oono24oo4nnccn44232N22oo'\n",
      "[19:30:34] SMILES Parse Error: extra close parentheses while parsing: [C@@H]PnccnP4n4244442O3NOO)O=O3)[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H]PnccnP4n4244442O3NOO)O=O3)[' for input: '[C@@H]PnccnP4n4244442O3NOO)O=O3)['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: C([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'C([' for input: 'C(['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@@H][C@H][C@@H]4nc[C@H][C@H]422O3[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@@H][C@H][C@@H]4nc[C@H][C@H]422O3[' for input: '[C@@H][C@H][C@@H]4nc[C@H][C@H]422O3['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: OC(1(()C[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES 'OC(1(()C[' for input: 'OC(1(()C['\n",
      "[19:30:34] SMILES Parse Error: duplicated ring closure 2 bonds atom 20 to itself for input: '[C@H]Pc[C@H][C@H]cccP44nnc42n4ccn5no2nc44o22[C@@H]4c[C@@H]2224ncc4244222[C@@H]44cccnn44ccc[C@H]44n4c4n444n[C@@H]444nc[C@H]4cncc4cc4cc[C@H]c44cc[C@H]c[C@@H]nc4c[C@@H]n22cc'\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [Si]CCC()CC1(1)1==1((1)1((CC(1OO=SO11()(CCC)(((((((CCCC(CC([\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[Si]CCC()CC1(1)1==1((1)1((CC(1OO=SO11()(CCC)(((((((CCCC(CC([' for input: '[Si]CCC()CC1(1)1==1((1)1((CC(1OO=SO11()(CCC)(((((((CCCC(CC(['\n",
      "[19:30:34] SMILES Parse Error: syntax error while parsing: [C@H]np[C@H][C@@H]c[C@@H]4on[C@@H]nn44[C@@H]22o[C@@H][C@@H]cc[C@H]cccPn422O2OOON[C@@H]P[C@H]cc[C@H][C@H]ccP455oPncPcP4c44n23==O=O==)((C[\n",
      "[19:30:34] SMILES Parse Error: Failed parsing SMILES '[C@H]np[C@H][C@@H]c[C@@H]4on[C@@H]nn44[C@@H]22o[C@@H][C@@H]cc[C@H]cccPn422O2OOON[C@@H]P[C@H]cc[C@H][C@H]ccP455oPncPcP4c44n23==O=O==)((C[' for input: '[C@H]np[C@H][C@@H]c[C@@H]4on[C@@H]nn44[C@@H]22o[C@@H][C@@H]cc[C@H]cccPn422O2OOON[C@@H]P[C@H]cc[C@H][C@H]ccP455oPncPcP4c44n23==O=O==)((C['\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top-p 采样 (p=0.9) ===\n",
      "1: [P@@]\n",
      "2: Cl[B-]CB\n",
      "\n",
      "生成结果已保存到CSV文件\n",
      "随机采样: 0 个有效分子\n",
      "Top-k采样: 0 个有效分子\n",
      "Top-p采样: 2 个有效分子\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:30:38] SMILES Parse Error: syntax error while parsing: n#.-[B-][Na+]4[C@@][nH]o[Si][H][S@@]oXBrOn[PH+][Na+][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'n#.-[B-][Na+]4[C@@][nH]o[Si][H][S@@]oXBrOn[PH+][Na+][' for input: 'n#.-[B-][Na+]4[C@@][nH]o[Si][H][S@@]oXBrOn[PH+][Na+]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: .6.[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '.6.[' for input: '.6.['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: B[O-1][C@H][BH3-][B-][O-1]=[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'B[O-1][C@H][BH3-][B-][O-1]=[' for input: 'B[O-1][C@H][BH3-][B-][O-1]=['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [P+][C@@H][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[P+][C@@H][' for input: '[P+][C@@H]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: \\cs[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '\\cs[' for input: '\\cs['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: P[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'P[' for input: 'P['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [Si][K+]3[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[Si][K+]3[' for input: '[Si][K+]3['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [B-]\\[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[B-]\\[' for input: '[B-]\\['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [C@H][BH3-][P@H][NH3+][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[C@H][BH3-][P@H][NH3+][' for input: '[C@H][BH3-][P@H][NH3+]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [nH][C][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[nH][C][' for input: '[nH][C]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [C@@][C@@][P@][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[C@@][C@@][P@][' for input: '[C@@][C@@][P@]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: -[nH]\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '-[nH]' for input: '-[nH]'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [NH3+][B-]o[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[NH3+][B-]o[' for input: '[NH3+][B-]o['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [S@][P@H][Na+]O[PH][C@@][S@]6[P+][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[S@][P@H][Na+]O[PH][C@@][S@]6[P+][' for input: '[S@][P@H][Na+]O[PH][C@@][S@]6[P+]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [N-]=[c-][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[N-]=[c-][' for input: '[N-]=[c-]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [P+][nH][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[P+][nH][' for input: '[P+][nH]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: #[BH3-][B-](3[c-]6[Si][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '#[BH3-][B-](3[c-]6[Si][' for input: '#[BH3-][B-](3[c-]6[Si]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [BH3-]P[B-1][B-1][H][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[BH3-]P[B-1][B-1][H][' for input: '[BH3-]P[B-1][B-1][H]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [P@H][C@@H][P@H][H](4[H]c[O-]Br[N+]1[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[P@H][C@@H][P@H][H](4[H]c[O-]Br[N+]1[' for input: '[P@H][C@@H][P@H][H](4[H]c[O-]Br[N+]1['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [nH][N+][O-][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[nH][N+][O-][' for input: '[nH][N+][O-]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: o[C@][B-1][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'o[C@][B-1][' for input: 'o[C@][B-1]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] non-ring atom 2 marked aromatic\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [S@@][P@@][C@@H][C@][C][c-][nH][K+]1[O-]s[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[S@@][P@@][C@@H][C@][C][c-][nH][K+]1[O-]s[' for input: '[S@@][P@@][C@@H][C@][C][c-][nH][K+]1[O-]s['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: O[N-1][H][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'O[N-1][H][' for input: 'O[N-1][H]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [P@@]Cl([\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[P@@]Cl([' for input: '[P@@]Cl(['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [PH+][N-]7[P@@]NcXNs[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[PH+][N-]7[P@@]NcXNs[' for input: '[PH+][N-]7[P@@]NcXNs['\n",
      "[19:30:38] SMILES Parse Error: unclosed ring for input: '[C]3[P@]S.[c-]'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: =X[N-]O#2X[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '=X[N-]O#2X[' for input: '=X[N-]O#2X['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: C[BH3-][B-][B-1][NH3+]Cl[BH3-]X[P@@]2p[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'C[BH3-][B-][B-1][NH3+]Cl[BH3-]X[P@@]2p[' for input: 'C[BH3-][B-][B-1][NH3+]Cl[BH3-]X[P@@]2p['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: \\[P@H][Na+][P@H][B-1]Cl[S@]3[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '\\[P@H][Na+][P@H][B-1]Cl[S@]3[' for input: '\\[P@H][Na+][P@H][B-1]Cl[S@]3['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: unclosed ring for input: 'Br=2[BH3-]3N'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: sI4S[O-1]O[nH]([Si][C@@][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'sI4S[O-1]O[nH]([Si][C@@][' for input: 'sI4S[O-1]O[nH]([Si][C@@]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: c[c-][P@@][NH3+]5nS[H]1[N].[S@][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'c[c-][P@@][NH3+]5nS[H]1[N].[S@][' for input: 'c[c-][P@@][NH3+]5nS[H]1[N].[S@]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: #[P@]C[C@][NH3+][N-1][P@H][N-1]#-[BH3-]3[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '#[P@]C[C@][NH3+][N-1][P@H][N-1]#-[BH3-]3[' for input: '#[P@]C[C@][NH3+][N-1][P@H][N-1]#-[BH3-]3['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: n[N-1]s[K+][C@H]I[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'n[N-1]s[K+][C@H]I[' for input: 'n[N-1]s[K+][C@H]I['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [C@@][Na+][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[C@@][Na+][' for input: '[C@@][Na+]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: ClClX[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'ClClX[' for input: 'ClClX['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: P[nH][N-]p5[PH]F[PH]=[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'P[nH][N-]p5[PH]F[PH]=[' for input: 'P[nH][N-]p5[PH]F[PH]=['\n",
      "[19:30:38] SMILES Parse Error: unclosed ring for input: '[Na+]/7[N]o[P@H][PH+]1[N-1]O[Si][N-1]'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [C]o5[nH]Fp[Na+]3[P@H][C@@][P@@]4[C@@H]sPOB6/p[S@]4N3[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[C]o5[nH]Fp[Na+]3[P@H][C@@][P@@]4[C@@H]sPOB6/p[S@]4N3[' for input: '[C]o5[nH]Fp[Na+]3[P@H][C@@][P@@]4[C@@H]sPOB6/p[S@]4N3['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: N4\\NoClBr/B7[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'N4\\NoClBr/B7[' for input: 'N4\\NoClBr/B7['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [N][n+][N][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[N][n+][N][' for input: '[N][n+][N]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: X[N-][BH3-][C]ClnI[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'X[N-][BH3-][C]ClnI[' for input: 'X[N-][BH3-][C]ClnI['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: 6[PH]\\[C][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '6[PH]\\[C][' for input: '6[PH]\\[C]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: 76[H][H]o*[Si]O[P@]X1[C@@H][C@@][C@@H][C@@]2=-[S@]P[Si][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '76[H][H]o*[Si]O[P@]X1[C@@H][C@@][C@@H][C@@]2=-[S@]P[Si][' for input: '76[H][H]o*[Si]O[P@]X1[C@@H][C@@][C@@H][C@@]2=-[S@]P[Si]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: Br[C@@H][Na+][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'Br[C@@H][Na+][' for input: 'Br[C@@H][Na+]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: o[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'o[' for input: 'o['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [S@@][N]pc[S@]2Br[H]SonB3[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[S@@][N]pc[S@]2Br[H]SonB3[' for input: '[S@@][N]pc[S@]2Br[H]SonB3['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [B-]7[P@@]-[N]=53F[P@H][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[B-]7[P@@]-[N]=53F[P@H][' for input: '[B-]7[P@@]-[N]=53F[P@H]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [C@][PH][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[C@][PH][' for input: '[C@][PH]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [Na+]p[C][n+]\\7[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[Na+]p[C][n+]\\7[' for input: '[Na+]p[C][n+]\\7['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: =[C@@]7\\o[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '=[C@@]7\\o[' for input: '=[C@@]7\\o['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: C[PH+][C@@H][P+][K+][C@@H]N([\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'C[PH+][C@@H][P+][K+][C@@H]N([' for input: 'C[PH+][C@@H][P+][K+][C@@H]N(['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: o[N+]Cl[N-1]#[B-]XcBpo7Br[PH]5Brp[C@@H][P+][Na+]O)[O-][B-][S@@][K+]\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'o[N+]Cl[N-1]#[B-]XcBpo7Br[PH]5Brp[C@@H][P+][Na+]O)[O-][B-][S@@][K+]' for input: 'o[N+]Cl[N-1]#[B-]XcBpo7Br[PH]5Brp[C@@H][P+][Na+]O)[O-][B-][S@@][K+]'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: .B[N+]=[P@H]=[N+][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '.B[N+]=[P@H]=[N+][' for input: '.B[N+]=[P@H]=[N+]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [O-]S3#[P+][S@][B-1]O#[NH3+]XPpI[P@]/[nH]6[nH][c-][P+][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[O-]S3#[P+][S@][B-1]O#[NH3+]XPpI[P@]/[nH]6[nH][c-][P+][' for input: '[O-]S3#[P+][S@][B-1]O#[NH3+]XPpI[P@]/[nH]6[nH][c-][P+]['\n",
      "[19:30:38] SMILES Parse Error: extra open parentheses for input: 'oClP[C@H][P@H](PBrF3[PH+]P'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [Na+][C@].[P+][c-][N-][B-1][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[Na+][C@].[P+][c-][N-][B-1][' for input: '[Na+][C@].[P+][c-][N-][B-1]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: extra close parentheses while parsing: [H]C[O-][S@@]=[B-][B-][N-])[P+][N][PH+][BH3-]n31[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[H]C[O-][S@@]=[B-][B-][N-])[P+][N][PH+][BH3-]n31[' for input: '[H]C[O-][S@@]=[B-][B-][N-])[P+][N][PH+][BH3-]n31['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: 23o[Na+][B-1][B-][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '23o[Na+][B-1][B-][' for input: '23o[Na+][B-1][B-]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: #2[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '#2[' for input: '#2['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: \\[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '\\[' for input: '\\['\n",
      "[19:30:38] non-ring atom 0 marked aromatic\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [N]S[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[N]S[' for input: '[N]S['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: 6/16)[C][B-][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '6/16)[C][B-][' for input: '6/16)[C][B-]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: C[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'C[' for input: 'C['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [C@][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[C@][' for input: '[C@]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: FO[P+]5[C@@H][P@@]Cl[P@]/)Cl[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'FO[P+]5[C@@H][P@@]Cl[P@]/)Cl[' for input: 'FO[P+]5[C@@H][P@@]Cl[P@]/)Cl['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: =[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '=[' for input: '=['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: sp[NH3+]OBrOO[N+][S@@][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'sp[NH3+]OBrOO[N+][S@@][' for input: 'sp[NH3+]OBrOO[N+][S@@]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: -[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '-[' for input: '-['\n",
      "[19:30:38] SMILES Parse Error: extra close parentheses while parsing: [C@@H][C@@]Br[B-][N+][P+][NH3+]2)#)([N][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[C@@H][C@@]Br[B-][N+][P+][NH3+]2)#)([N][' for input: '[C@@H][C@@]Br[B-][N+][P+][NH3+]2)#)([N]['\n",
      "[19:30:38] SMILES Parse Error: unclosed ring for input: 'N#2[N-][N]5'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: O[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'O[' for input: 'O['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: #c[C@@H][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '#c[C@@H][' for input: '#c[C@@H]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: 5[C@]-[P@@][N][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '5[C@]-[P@@][N][' for input: '5[C@]-[P@@][N]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: Br#[P@@]3[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'Br#[P@@]3[' for input: 'Br#[P@@]3['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [K+][N][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[K+][N][' for input: '[K+][N]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: =[K+][N-1]\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '=[K+][N-1]' for input: '=[K+][N-1]'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: [nH]3[B-].[K+][S@]([\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '[nH]3[B-].[K+][S@]([' for input: '[nH]3[B-].[K+][S@](['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: 7[K+][H]o[PH][H]Cl[C@@H]\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '7[K+][H]o[PH][H]Cl[C@@H]' for input: '7[K+][H]o[PH][H]Cl[C@@H]'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: CC[S@][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'CC[S@][' for input: 'CC[S@]['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: 6[P@H]([C@@][NH3+]5[C@@H][P+][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '6[P@H]([C@@][NH3+]5[C@@H][P+][' for input: '6[P@H]([C@@][NH3+]5[C@@H][P+]['\n",
      "[19:30:38] SMILES Parse Error: extra close parentheses while parsing: S[Na+][N-])[B-][Si]4[N-1]5.-\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'S[Na+][N-])[B-][Si]4[N-1]5.-' for input: 'S[Na+][N-])[B-][Si]4[N-1]5.-'\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: #[\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES '#[' for input: '#['\n",
      "[19:30:38] SMILES Parse Error: syntax error while parsing: BN[P@H]F51[C@H][H]F(5[O-][P@H][N+][S@])S[P@]P[P+](1[C@@H]N[C@@H]5BO[B-1][B-1][C@@][S@@]41[PH]6N[PH][PH][P+][N][\n",
      "[19:30:38] SMILES Parse Error: Failed parsing SMILES 'BN[P@H]F51[C@H][H]F(5[O-][P@H][N+][S@])S[P@]P[P+](1[C@@H]N[C@@H]5BO[B-1][B-1][C@@][S@@]41[PH]6N[PH][PH][P+][N][' for input: 'BN[P@H]F51[C@H][H]F(5[O-][P@H][N+][S@])S[P@]P[P+](1[C@@H]N[C@@H]5BO[B-1][B-1][C@@][S@@]41[PH]6N[PH][PH][P+][N]['\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "load_path = 'model_pre_weights1.h5'  \n",
    "model.load_weights(load_path)\n",
    "\n",
    "# 执行测试\n",
    "test_different_sampling_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 与核心片段拼接成完整的手性磷酸催化剂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_smiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mFragment_processing\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## 尝试在多个位点插入(*)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 加载 SMILES 数据\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# df_smiles_gen = pd.read_csv('unique_generated_smiles.csv')\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m smiles_gen_list \u001b[38;5;241m=\u001b[39m \u001b[43mgenerated_smiles\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# smiles_gen_list = df_smiles_gen['Generated_SMILES'].tolist()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 定义核心结构\u001b[39;00m\n\u001b[1;32m     10\u001b[0m core_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOP1(Oc2c(c3c(CCCC4)c4cc(*)c3O1)c(CCCC5)c5cc2(*))=O\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOP1(Oc2c(c3c4ccccc4cc(*)c3O1)c5ccccc5cc2(*))=O\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOP1(Oc2c(*)cc3c(CCCC3)c2C4c5c(CC(C4O1)(*))cccc5)=O\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO=P1(Oc(cc2)c(c(c(O1)cc3)c4c3cc(*)cc4)c5c2cc(*)cc5)O\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_smiles' is not defined"
     ]
    }
   ],
   "source": [
    "import Fragment_processing\n",
    "\n",
    "## 尝试在多个位点插入(*)\n",
    "# 加载 SMILES 数据\n",
    "# df_smiles_gen = pd.read_csv('unique_generated_smiles.csv')\n",
    "smiles_gen_list = generated_smiles\n",
    "# smiles_gen_list = df_smiles_gen['Generated_SMILES'].tolist()\n",
    "\n",
    "# 定义核心结构\n",
    "core_list = [\n",
    "    \"OP1(Oc2c(c3c(CCCC4)c4cc(*)c3O1)c(CCCC5)c5cc2(*))=O\",\n",
    "    \"OP1(Oc2c(c3c4ccccc4cc(*)c3O1)c5ccccc5cc2(*))=O\",\n",
    "    \"OP1(Oc2c(*)cc3c(CCCC3)c2C4c5c(CC(C4O1)(*))cccc5)=O\",\n",
    "    \"O=P1(Oc(cc2)c(c(c(O1)cc3)c4c3cc(*)cc4)c5c2cc(*)cc5)O\",\n",
    "]\n",
    "\n",
    "# 生成片段库\n",
    "frag_list = generate_fragment_library(smiles_gen_list)\n",
    "\n",
    "# 生成多核心多片段组合\n",
    "multi_results, core_indices = multi_core_multi_frag_join(core_list, frag_list, use_symmetric=True)\n",
    "\n",
    "# 保存结果\n",
    "# output_file = 'multi_core.csv'\n",
    "# df = save_to_csv(multi_results, core_indices, core_list, output_file)\n",
    "\n",
    "# print(f\"已生成 {len(multi_results)} 个分子组合，并已保存到 {output_file}\")\n",
    "# 1. 从 multi_core.csv 读取生成的 SMILES\n",
    "df_valid = multi_results\n",
    "df_valid = pd.read_csv('multi_core.csv')\n",
    "if 'SMILES' not in df_valid.columns:\n",
    "    raise KeyError(\"multi_core.csv 中未找到 'SMILES' 列\")\n",
    "gen_smiles = df_valid['SMILES'].dropna().tolist()\n",
    "\n",
    "# 2. 调用主处理流程：清理、验证、并保存 valid/invalid SMILES\n",
    "valid_smiles, invalid_smiles = process_generated_smiles(gen_smiles)\n",
    "\n",
    "# 打印一些统计信息\n",
    "print(\"\\n处理结果统计:\")\n",
    "print(f\"总SMILES数量: {len(gen_smiles)}\")\n",
    "print(f\"有效SMILES数量: {len(valid_smiles)}\")\n",
    "print(f\"无效SMILES数量: {len(invalid_smiles)}\")\n",
    "# print(f\"去重后SMILES数量: {len(final_smiles)}\")\n",
    "\n",
    "# 打印前5个有效SMILES\n",
    "print(\"\\n前5个有效SMILES:\")\n",
    "for i, smiles in enumerate(valid_smiles[:5]):\n",
    "    print(f\"{i+1}. {smiles}\")\n",
    "# 假设 valid_smiles 是生成的SMILES列表\n",
    "# 假设 smiles_list 是初始输入的SMILES列表\n",
    "\n",
    "# 1. 比较SMILES\n",
    "# 加载 SMILES 数据\n",
    "df_1075 = pd.read_csv('1075_cat.csv')  # 假设 'all_c.csv' 文件路径正确\n",
    "smiles_list = df_1075['Catalyst'].tolist()  # 提取 Catalyst 列\n",
    "results_df, results_unique_df, results_duplicate_df = compare_with_initial_smiles(valid_smiles, smiles_list)\n",
    "\n",
    "# 2. 分析结果\n",
    "total = len(results_df)\n",
    "duplicates = results_df['Is_Duplicate'].sum()\n",
    "unique = total - duplicates\n",
    "print(f\"唯一SMILES数量: {unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 强化学习部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate_random_sample对应的强化学习部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('./scscore15/scscore-master/scscore')\n",
    "from standalone_model_numpy import SCScorer\n",
    "from itertools import product\n",
    "import Fragment_processing\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append(os.path.abspath('MMHRP-GCL-Code-main'))\n",
    "from utils.rxn import *\n",
    "from utils.molecule import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from models.MMHRP_GCL import *\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# 封装一个surrogate_regressor模型，定义一个接口\n",
    "class SurrogateRegressor:\n",
    "    def __init__(self, smiles, reactant_combine):\n",
    "        self.smiles = smiles \n",
    "        self.reactant_combine = 'O=C(/N=C/c1ccc(C(F)(F)F)cc1)c1ccccc1.CCS.CCSC(NC(=O)c1ccccc1)c1ccc(C(F)(F)F)cc1'\n",
    "        \n",
    "    def predictor(self, smiles, reactant_combine):\n",
    "        \"\"\"\n",
    "        results_unique_df: 每个元素是完整催化剂的SMILES\n",
    "        返回： (smiles, predicted_values_tensor)\n",
    "        reactant_combine='O=C(/N=C/c1ccc(C(F)(F)F)cc1)c1ccccc1.CCS.CCSC(NC(=O)c1ccccc1)c1ccc(C(F)(F)F)cc1'\n",
    "        \"\"\"\n",
    "        # 拆分 reactant_combine\n",
    "        parts = reactant_combine.split('.')\n",
    "        if len(parts) != 3:\n",
    "            raise ValueError(f\"reactant_combine 的格式应为 'Imine.Thiol.Product'，但现在得到 {len(parts)} 部分：{parts}\")\n",
    "\n",
    "        imine = parts[0]\n",
    "        thiol = parts[1]\n",
    "        product = parts[2]\n",
    "        \n",
    "        combinations = []\n",
    "        for s in smiles:\n",
    "            combined_smiles = f\"{imine}.{thiol}.{product}.{s}\"\n",
    "            combinations.append({\n",
    "                'Catalyst': s,\n",
    "                'Imine': imine,\n",
    "                'Thiol': thiol,\n",
    "                'product': product\n",
    "            })\n",
    "        \n",
    "        expanded_df = pd.DataFrame(combinations)\n",
    "        new_data = expanded_df\n",
    "        expanded_df.to_csv(\"Combinations.csv\", index=False)\n",
    "        \n",
    "        def check_smiles_validity(smi):\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            return mol is not None\n",
    "        \n",
    "        valid_rows = []\n",
    "        invalid_indices = []\n",
    "        \n",
    "        for idx, row in new_data.iterrows():\n",
    "            smi_list = [row[\"Catalyst\"], row[\"Imine\"], row[\"Thiol\"], row[\"product\"]]\n",
    "            if all(check_smiles_validity(smi) for smi in smi_list):\n",
    "                valid_rows.append(row)\n",
    "            else:\n",
    "                invalid_indices.append(idx)\n",
    "        \n",
    "        # 重新构建合法数据 DataFrame\n",
    "        new_data = pd.DataFrame(valid_rows).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # 模型加载\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "       \n",
    "        # 模型超参数（要和训练时一致！）\n",
    "        num_feature = 8\n",
    "        smi_inputsize = 128\n",
    "        \n",
    "        # vocab 文件路径\n",
    "        vocab_type = \"AT\"\n",
    "        vocab_path = \"./MMHRP-GCL-Code-main/utils/%s_vocab.txt\" % vocab_type\n",
    "        \n",
    "        # 模型保存路径\n",
    "        model_path = \"./MMHRP-GCL-Code-main/exp/AT/AT_MMHRP-GCL_rs=3/model.pth\"\n",
    "        \n",
    "        # ---------------------- 加载模型 ----------------------\n",
    "        model = MMHRP_GCL(\n",
    "            GraphEncoder={\n",
    "                \"NodeFeatNum\": num_feature,\n",
    "                \"Channels\": [32, 64],\n",
    "                \"Heads\": 4\n",
    "            },\n",
    "            TextEncoder={\n",
    "                \"SmiFeatNum\": smi_inputsize,\n",
    "                \"Heads\": 4,\n",
    "                \"BigruChannels\": [128, 128, 128],\n",
    "                \"BigruNumlayer\": 1\n",
    "            },\n",
    "            Decoder={\n",
    "                \"Heads\": 4,\n",
    "                \"Channels\": [1000, 500, 100]\n",
    "            },\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        model = torch.load(model_path, map_location=device)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(\"模型已加载成功！\")\n",
    "        \n",
    "        # 数据处理\n",
    "        rxn_dataset = []\n",
    "        rxn_RxnSmi = []\n",
    "        max_len = -1\n",
    "       \n",
    "        # 确定max_len\n",
    "        for idx in range(new_data.shape[0]):\n",
    "            RxnSmi = get_AT_RxnSmi(new_data.iloc[idx, :])\n",
    "            max_len = max(max_len, len(RxnSmi))\n",
    "            RxnSmi = \" \".join(smi_tokenizer(RxnSmi))\n",
    "            rxn_RxnSmi.append(RxnSmi)\n",
    "        \n",
    "        # 构建数据集\n",
    "        for idx in range(new_data.shape[0]):\n",
    "            meta = []\n",
    "            \n",
    "            # 反应物-产物图\n",
    "            rea1 = new_data.loc[idx][\"Imine\"]\n",
    "            rea2 = new_data.loc[idx][\"Thiol\"]\n",
    "            prod = new_data.loc[idx][\"product\"]\n",
    "            rea_pro_graph = smis_to_graph([rea1, rea2, prod])\n",
    "            # 确保节点特征维度正确\n",
    "            if rea_pro_graph.x.size(1) != num_feature:\n",
    "                rea_pro_graph.x = torch.nn.functional.pad(rea_pro_graph.x, (0, num_feature - rea_pro_graph.x.size(1)))\n",
    "            meta.append(rea_pro_graph)\n",
    "            \n",
    "            # 催化剂图\n",
    "            cat = new_data.loc[idx][\"Catalyst\"]\n",
    "            cat_graph = smis_to_graph([cat])\n",
    "            # 确保节点特征维度正确\n",
    "            if cat_graph.x.size(1) != num_feature:\n",
    "                cat_graph.x = torch.nn.functional.pad(cat_graph.x, (0, num_feature - cat_graph.x.size(1)))\n",
    "            meta.append(cat_graph)\n",
    "            \n",
    "            # RxnSmi张量\n",
    "            RxnSmi_vec = RxnSmi_to_tensor(\n",
    "                RxnSmi=rxn_RxnSmi[idx],\n",
    "                maxlen_=max_len,\n",
    "                victor_size=smi_inputsize,\n",
    "                file=vocab_path\n",
    "            )\n",
    "            meta.append(RxnSmi_vec)\n",
    "            \n",
    "            # 输出（如果存在）\n",
    "            if \"Output\" in new_data.columns:\n",
    "                meta.append(torch.tensor(new_data.loc[idx][\"Output\"], dtype=torch.float))\n",
    "            \n",
    "            rxn_dataset.append(meta)\n",
    "        \n",
    "        # 创建DataLoader\n",
    "        test_loader = DataLoader(rxn_dataset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        # 模型预测\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_values = []\n",
    "            \n",
    "            for batch_data in test_loader:\n",
    "                # 确保数据在正确的设备上\n",
    "                ReaPro_data = batch_data[0].to(device)\n",
    "                CatSol_data = batch_data[1].to(device)\n",
    "                RxnSmi_data = batch_data[2].to(device)\n",
    "                \n",
    "                # 确保输入维度正确\n",
    "                if ReaPro_data.x.size(1) != num_feature:\n",
    "                    ReaPro_data.x = torch.nn.functional.pad(ReaPro_data.x, (0, num_feature - ReaPro_data.x.size(1)))\n",
    "                if CatSol_data.x.size(1) != num_feature:\n",
    "                    CatSol_data.x = torch.nn.functional.pad(CatSol_data.x, (0, num_feature - CatSol_data.x.size(1)))\n",
    "                \n",
    "                x = [ReaPro_data, CatSol_data, RxnSmi_data]\n",
    "                pred = model.forward(x).cpu().detach().numpy()\n",
    "                # predictions.extend(pred.tolist())#这个地方最后输出的是[]列表形式\n",
    "                predictions.extend(pred.flatten().tolist())#这样输出的就是数字形式\n",
    "                \n",
    "                if len(batch_data) == 4:\n",
    "                    true = batch_data[3].detach().numpy()\n",
    "                    true_values.extend(true.tolist())\n",
    "        \n",
    "        # 保存结果\n",
    "        output_df = new_data.copy()\n",
    "        output_df[\"Predicted_Output\"] = predictions\n",
    "        \n",
    "        return smiles, output_df, predictions\n",
    "\n",
    "# def get_reward(smiles, invalid_reward=0.0):\n",
    "def smiles_to_ecfp(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(mol, radius=3, nBits=2048)\n",
    "    \n",
    "def get_reward(smiles, surrogate_regressor, reactant_combine, invalid_reward=0.0):\n",
    "    rewards = np.zeros([len(smiles)])\n",
    "    \n",
    "    # 如果有代理预测模型\n",
    "    if surrogate_regressor is not None:\n",
    "        molecule_smiles, molecule_output_df, predicted_value = surrogate_regressor.predictor(smiles, reactant_combine) \n",
    "        predicted_value = tensor_to_array(predicted_value)\n",
    "    else:\n",
    "        predicted_value = [0.0 for _ in range(len(smiles))]  # 假设默认值为0.0\n",
    "\n",
    "\n",
    "    for i in range(len(smiles)):\n",
    "        sample = smiles[i]\n",
    "        if sample == '':\n",
    "            rewards[i] = -2\n",
    "            continue\n",
    "        else:\n",
    "            a = sample.find('.')\n",
    "            smile_ = sample[:a]\n",
    "            mol = Chem.MolFromSmiles(smile_)\n",
    "            if mol is None:\n",
    "                rewards[i] = -2\n",
    "                continue\n",
    "            else:\n",
    "                charge = 0\n",
    "                for atom in mol.GetAtoms():\n",
    "                    atom_charge = atom.GetFormalCharge()\n",
    "                    if atom_charge != 0:\n",
    "                        charge = atom_charge \n",
    "                if charge != 0:\n",
    "                    rewards[i] = 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if Descriptors.MolWt(mol) > 1200:\n",
    "                        rewards[i] = 2\n",
    "                        continue\n",
    "                        \n",
    "                    else:\n",
    "                        sm, SCscore = scscore_model.get_score_from_smi(smile_)\n",
    "                        if SCscore > 4:                                                   \n",
    "                            rewards[i] = 2 + (5.0-(SCscore)*2)\n",
    "                        else:\n",
    "                            s = int((predicted_value[i]-50)/5)\n",
    "                            rewards[i] = 6.5 + ((2*s)+1)\n",
    "   \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored variables from scscore15/scscore-master/models/full_reaxys_model_1024bool/model.ckpt-10654.as_numpy.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<standalone_model_numpy.SCScorer at 0x7f592993f130>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scscore\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 添加 scscore 路径（让 Python 能找到 SCScorer）\n",
    "sys.path.append('./scscore15/scscore-master/scscore')\n",
    "\n",
    "# ✅ 导入 SCScorer\n",
    "from standalone_model_numpy import SCScorer\n",
    "\n",
    "# ✅ 设置模型路径\n",
    "scscore_model_path = os.path.join(\n",
    "    'scscore15', 'scscore-master', 'models', 'full_reaxys_model_1024bool',\n",
    "    'model.ckpt-10654.as_numpy.json.gz'\n",
    ")\n",
    "\n",
    "# ✅ 初始化并加载模型\n",
    "scscore_model = SCScorer()\n",
    "scscore_model.restore(scscore_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from Fragment_processing import *\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append(os.path.join(os.getcwd(), 'MMHRP-GCL-Code-main'))\n",
    "sys.path.append('./MMHRP-GCL-Code-main')\n",
    "from utils.rxn import *\n",
    "from utils.molecule import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from models.MMHRP_GCL import *\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# 封装一个surrogate_regressor模型，定义一个接口\n",
    "class SurrogateRegressor:\n",
    "    def __init__(self, smiles, reactant_combine):\n",
    "        self.smiles = smiles \n",
    "        self.reactant_combine = 'O=C(/N=C/c1ccc(C(F)(F)F)cc1)c1ccccc1.CCS.CCSC(NC(=O)c1ccccc1)c1ccc(C(F)(F)F)cc1'\n",
    "        \n",
    "    def predictor(self, smiles, reactant_combine):\n",
    "        \"\"\"\n",
    "        results_unique_df: 每个元素是完整催化剂的SMILES\n",
    "        返回： (smiles, predicted_values_tensor)\n",
    "        reactant_combine='O=C(/N=C/c1ccc(C(F)(F)F)cc1)c1ccccc1.CCS.CCSC(NC(=O)c1ccccc1)c1ccc(C(F)(F)F)cc1'\n",
    "        \"\"\"\n",
    "        # 拆分 reactant_combine\n",
    "        parts = reactant_combine.split('.')\n",
    "        if len(parts) != 3:\n",
    "            raise ValueError(f\"reactant_combine 的格式应为 'Imine.Thiol.Product'，但现在得到 {len(parts)} 部分：{parts}\")\n",
    "\n",
    "        imine = parts[0]\n",
    "        thiol = parts[1]\n",
    "        product = parts[2]\n",
    "        \n",
    "        combinations = []\n",
    "        for s in smiles:\n",
    "            combined_smiles = f\"{imine}.{thiol}.{product}.{s}\"\n",
    "            combinations.append({\n",
    "                'Catalyst': s,\n",
    "                'Imine': imine,\n",
    "                'Thiol': thiol,\n",
    "                'product': product\n",
    "            })\n",
    "        \n",
    "        expanded_df = pd.DataFrame(combinations)\n",
    "        new_data = expanded_df\n",
    "        expanded_df.to_csv(\"Combinations.csv\", index=False)\n",
    "        \n",
    "        def check_smiles_validity(smi):\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            return mol is not None\n",
    "        \n",
    "        valid_rows = []\n",
    "        invalid_indices = []\n",
    "        \n",
    "        for idx, row in new_data.iterrows():\n",
    "            smi_list = [row[\"Catalyst\"], row[\"Imine\"], row[\"Thiol\"], row[\"product\"]]\n",
    "            if all(check_smiles_validity(smi) for smi in smi_list):\n",
    "                valid_rows.append(row)\n",
    "            else:\n",
    "                invalid_indices.append(idx)\n",
    "        \n",
    "        # 重新构建合法数据 DataFrame\n",
    "        new_data = pd.DataFrame(valid_rows).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # 模型加载\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "       \n",
    "        # 模型超参数（要和训练时一致！）\n",
    "        num_feature = 8\n",
    "        smi_inputsize = 128\n",
    "        \n",
    "        # vocab 文件路径\n",
    "        vocab_type = \"AT\"\n",
    "        vocab_path = \"./MMHRP-GCL-Code-main/utils/%s_vocab.txt\" % vocab_type\n",
    "        \n",
    "        # 模型保存路径\n",
    "        model_path = \"./MMHRP-GCL-Code-main/exp/AT/AT_MMHRP-GCL_rs=3/model.pth\"\n",
    "        \n",
    "        # ---------------------- 加载模型 ----------------------\n",
    "        model = MMHRP_GCL(\n",
    "            GraphEncoder={\n",
    "                \"NodeFeatNum\": num_feature,\n",
    "                \"Channels\": [32, 64],\n",
    "                \"Heads\": 4\n",
    "            },\n",
    "            TextEncoder={\n",
    "                \"SmiFeatNum\": smi_inputsize,\n",
    "                \"Heads\": 4,\n",
    "                \"BigruChannels\": [128, 128, 128],\n",
    "                \"BigruNumlayer\": 1\n",
    "            },\n",
    "            Decoder={\n",
    "                \"Heads\": 4,\n",
    "                \"Channels\": [1000, 500, 100]\n",
    "            },\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        model = torch.load(model_path, map_location=device)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(\"模型已加载成功！\")\n",
    "        \n",
    "        # 数据处理\n",
    "        rxn_dataset = []\n",
    "        rxn_RxnSmi = []\n",
    "        max_len = -1\n",
    "       \n",
    "        # 确定max_len\n",
    "        for idx in range(new_data.shape[0]):\n",
    "            RxnSmi = get_AT_RxnSmi(new_data.iloc[idx, :])\n",
    "            max_len = max(max_len, len(RxnSmi))\n",
    "            RxnSmi = \" \".join(smi_tokenizer(RxnSmi))\n",
    "            rxn_RxnSmi.append(RxnSmi)\n",
    "        \n",
    "        # 构建数据集\n",
    "        for idx in range(new_data.shape[0]):\n",
    "            meta = []\n",
    "            \n",
    "            # 反应物-产物图\n",
    "            rea1 = new_data.loc[idx][\"Imine\"]\n",
    "            rea2 = new_data.loc[idx][\"Thiol\"]\n",
    "            prod = new_data.loc[idx][\"product\"]\n",
    "            rea_pro_graph = smis_to_graph([rea1, rea2, prod])\n",
    "            # 确保节点特征维度正确\n",
    "            if rea_pro_graph.x.size(1) != num_feature:\n",
    "                rea_pro_graph.x = torch.nn.functional.pad(rea_pro_graph.x, (0, num_feature - rea_pro_graph.x.size(1)))\n",
    "            meta.append(rea_pro_graph)\n",
    "            \n",
    "            # 催化剂图\n",
    "            cat = new_data.loc[idx][\"Catalyst\"]\n",
    "            cat_graph = smis_to_graph([cat])\n",
    "            # 确保节点特征维度正确\n",
    "            if cat_graph.x.size(1) != num_feature:\n",
    "                cat_graph.x = torch.nn.functional.pad(cat_graph.x, (0, num_feature - cat_graph.x.size(1)))\n",
    "            meta.append(cat_graph)\n",
    "            \n",
    "            # RxnSmi张量\n",
    "            RxnSmi_vec = RxnSmi_to_tensor(\n",
    "                RxnSmi=rxn_RxnSmi[idx],\n",
    "                maxlen_=max_len,\n",
    "                victor_size=smi_inputsize,\n",
    "                file=vocab_path\n",
    "            )\n",
    "            meta.append(RxnSmi_vec)\n",
    "            \n",
    "            # 输出（如果存在）\n",
    "            if \"Output\" in new_data.columns:\n",
    "                meta.append(torch.tensor(new_data.loc[idx][\"Output\"], dtype=torch.float))\n",
    "            \n",
    "            rxn_dataset.append(meta)\n",
    "        \n",
    "        # 创建DataLoader\n",
    "        test_loader = DataLoader(rxn_dataset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        # 模型预测\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_values = []\n",
    "            \n",
    "            for batch_data in test_loader:\n",
    "                # 确保数据在正确的设备上\n",
    "                ReaPro_data = batch_data[0].to(device)\n",
    "                CatSol_data = batch_data[1].to(device)\n",
    "                RxnSmi_data = batch_data[2].to(device)\n",
    "                \n",
    "                # 确保输入维度正确\n",
    "                if ReaPro_data.x.size(1) != num_feature:\n",
    "                    ReaPro_data.x = torch.nn.functional.pad(ReaPro_data.x, (0, num_feature - ReaPro_data.x.size(1)))\n",
    "                if CatSol_data.x.size(1) != num_feature:\n",
    "                    CatSol_data.x = torch.nn.functional.pad(CatSol_data.x, (0, num_feature - CatSol_data.x.size(1)))\n",
    "                \n",
    "                x = [ReaPro_data, CatSol_data, RxnSmi_data]\n",
    "                pred = model.forward(x).cpu().detach().numpy()\n",
    "                # predictions.extend(pred.tolist())#这个地方最后输出的是[]列表形式\n",
    "                predictions.extend(pred.flatten().tolist())#这样输出的就是数字形式\n",
    "                \n",
    "                if len(batch_data) == 4:\n",
    "                    true = batch_data[3].detach().numpy()\n",
    "                    true_values.extend(true.tolist())\n",
    "        \n",
    "        # 保存结果\n",
    "        output_df = new_data.copy()\n",
    "        output_df[\"Predicted_Output\"] = predictions\n",
    "        \n",
    "        return smiles, output_df, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_reward(smiles, invalid_reward=0.0):\n",
    "def smiles_to_ecfp(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(mol, radius=3, nBits=2048)\n",
    "    \n",
    "def get_reward(smiles, surrogate_regressor, reactant_combine, invalid_reward=0.0):\n",
    "    rewards = np.zeros([len(smiles)])\n",
    "    \n",
    "    # 如果有代理预测模型\n",
    "    if surrogate_regressor is not None:\n",
    "        molecule_smiles, molecule_output_df, predicted_value = surrogate_regressor.predictor(smiles, reactant_combine) \n",
    "        predicted_value = tensor_to_array(predicted_value)\n",
    "    else:\n",
    "        predicted_value = [0.0 for _ in range(len(smiles))]  # 假设默认值为0.0\n",
    "\n",
    "\n",
    "    for i in range(len(smiles)):\n",
    "        sample = smiles[i]\n",
    "        if sample == '':\n",
    "            rewards[i] = -2\n",
    "            continue\n",
    "        else:\n",
    "            a = sample.find('.')\n",
    "            smile_ = sample[:a]\n",
    "            mol = Chem.MolFromSmiles(smile_)\n",
    "            if mol is None:\n",
    "                rewards[i] = -2\n",
    "                continue\n",
    "            else:\n",
    "                charge = 0\n",
    "                for atom in mol.GetAtoms():\n",
    "                    atom_charge = atom.GetFormalCharge()\n",
    "                    if atom_charge != 0:\n",
    "                        charge = atom_charge \n",
    "                if charge != 0:\n",
    "                    rewards[i] = 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if Descriptors.MolWt(mol) > 1200:\n",
    "                        rewards[i] = 2\n",
    "                        continue\n",
    "                        \n",
    "                    else:\n",
    "                        sm, SCscore = scscore_model.get_score_from_smi(smile_)\n",
    "                        if SCscore > 4:                                                   \n",
    "                            rewards[i] = 2 + (5.0-(SCscore)*2)\n",
    "                        else:\n",
    "                            s = int((predicted_value[i]-50)/5)\n",
    "                            rewards[i] = 6.5 + ((2*s)+1)\n",
    "   \n",
    "    return rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### 这个是cursor生成的：random样本生成 ###### 可运行\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "# from functions_rl import gen_firstatom_frag, join_frag, usable_frag\n",
    "\n",
    "class ReinforcementRandomTF(object):\n",
    "    def __init__(self, generator, predictor, get_reward):\n",
    "        super(ReinforcementRandomTF, self).__init__()\n",
    "        self.generator = generator  # CVAE解码器，策略函数 π_θ\n",
    "        self.predictor = predictor  # 辅助模型\n",
    "        self.get_reward = get_reward  # 奖励函数 R(s,a)\n",
    "\n",
    "    def policy_gradient(self, vocab, tokenize, map_tokens_to_ids,\n",
    "                        n_batch=4000, gamma=0.99,\n",
    "                        std_smiles=False, grad_clipping=None,\n",
    "                        reactant_combine='O=C(/N=C/c1ccc(C(F)(F)F)cc1)c1ccccc1.CCS.CCSC(NC(=O)c1ccccc1)c1ccc(C(F)(F)F)cc1',\n",
    "                        sub1 = '*c1cc2ccccc2c2c1OP(=O)(O)Oc1c(*)cc3ccccc3c1-2',\n",
    "                        sub2 = 'O=C(/N=C/c1ccc(C(F)(F)F)cc1)c1ccccc1.CCS',\n",
    "                        learning_rate=1e-3,\n",
    "                        max_length=165,\n",
    "                        core_list = [\"OP1(Oc2c(c3c(CCCC4)c4cc(*)c3O1)c(CCCC5)c5cc2(*))=O\",\n",
    "                                     \"OP1(Oc2c(c3c4ccccc4cc(*)c3O1)c5ccccc5cc2(*))=O\",\n",
    "                                     \"OP1(Oc2c(*)cc3c(CCCC3)c2C4c5c(CC(C4O1)(*))cccc5)=O\",\n",
    "                                     \"O=P1(Oc(cc2)c(c(c(O1)cc3)c4c3cc(*)cc4)c5c2cc(*)cc5)O\"\n",
    "                                    ]\n",
    "                       ):\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        rl_loss = 0.0\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # 确保生成器处于训练模式\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # 拆分 reactant_combine\n",
    "        parts = reactant_combine.split('.')\n",
    "        if len(parts) != 3:\n",
    "            raise ValueError(f\"reactant_combine 的格式应为 'Imine.Thiol.Product'，但现在得到 {len(parts)} 部分：{parts}\")\n",
    "\n",
    "        # 准备条件向量\n",
    "        C = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "        C = np.tile(C, (n_batch, 1))\n",
    "        start_codon = np.array([[vocab['X']]] * n_batch)\n",
    "        chars = [None] * len(vocab)\n",
    "        for k, v in vocab.items():\n",
    "            chars[v] = k\n",
    "        stop_token_id = vocab['E']\n",
    "\n",
    "        # 生成样本\n",
    "        trajectories = self.generator.generate_random_sample(\n",
    "            C=C,\n",
    "            start_codon=start_codon,\n",
    "            char=chars,\n",
    "            max_len=max_length,\n",
    "            return_smiles=True,\n",
    "            stop_token_id=stop_token_id\n",
    "        )\n",
    "        \n",
    "        if not trajectories:\n",
    "            raise ValueError(\"generate_random_sample 返回为空，无法生成片段\")\n",
    "\n",
    "        # 处理生成的样本\n",
    "        frag_list = generate_fragment_library(trajectories)\n",
    "\n",
    "        # 生成多核心多片段组合\n",
    "        multi_results, core_indices = multi_core_multi_frag_join(core_list, frag_list, use_symmetric=True)\n",
    "        df_valid = multi_results\n",
    "        df_valid = pd.read_csv('multi_core.csv')\n",
    "        if 'SMILES' not in df_valid.columns:\n",
    "            raise KeyError(\"multi_core.csv 中未找到 'SMILES' 列\")\n",
    "        gen_smiles = df_valid['SMILES'].dropna().tolist()\n",
    "        \n",
    "        # 调用主处理流程：清理、验证、并保存 valid/invalid SMILES\n",
    "        valid_smiles, invalid_smiles = process_generated_smiles(gen_smiles)\n",
    "        df_1075 = pd.read_csv('1075_cat.csv')  # 假设 'all_c.csv' 文件路径正确\n",
    "        smiles_list = df_1075['Catalyst'].tolist()  # 提取 Catalyst 列\n",
    "        results_df, results_unique_df, results_duplicate_df = compare_with_initial_smiles(valid_smiles, smiles_list)\n",
    "             \n",
    "        samples = results_unique_df\n",
    "\n",
    "        # 计算奖励\n",
    "        rewards = self.get_reward(samples, self.predictor, reactant_combine)\n",
    "        rewards = np.array(rewards, dtype=np.float64)\n",
    "            \n",
    "        def accumulate_loss(smile, reward, gamma, latent_vector, cond_vector, seq_length):\n",
    "            tokens = tokenize(smile)\n",
    "            traj_tokens = map_tokens_to_ids(tokens)\n",
    "            traj_tensor = tf.convert_to_tensor(traj_tokens, dtype=tf.int32)\n",
    "            loss = 0.0\n",
    "            discounted = reward\n",
    "\n",
    "            batch_size = 1\n",
    "            # 初始化隐藏状态\n",
    "            hidden_state = [[\n",
    "                tf.zeros((batch_size, self.generator.unit_size)),\n",
    "                tf.zeros((batch_size, self.generator.unit_size))\n",
    "            ] for _ in range(self.generator.n_rnn_layer)]\n",
    "\n",
    "            # 逐token生成和计算损失\n",
    "            for t in range(len(traj_tokens) - 1):\n",
    "                # 当前状态：已生成的序列\n",
    "                current_input = tf.expand_dims(traj_tensor[t], axis=0)  # shape: [1, 1]\n",
    "\n",
    "                # 使用解码器预测下一个token的概率分布\n",
    "                logits, _, hidden_state = self.generator(\n",
    "                    [current_input, latent_vector, cond_vector, tf.constant([1], dtype=tf.int32)],\n",
    "                    state=hidden_state,\n",
    "                    training=True\n",
    "                )\n",
    "\n",
    "                # 计算当前时间步的损失\n",
    "                log_probs = tf.nn.log_softmax(logits[0, -1, :], axis=-1)  # 取最后一个时间步的预测\n",
    "                next_char = traj_tensor[t + 1]  # 实际的下一个token\n",
    "                loss += log_probs[next_char] * discounted\n",
    "                discounted *= gamma\n",
    "\n",
    "            return -loss\n",
    "\n",
    "        # 计算策略梯度\n",
    "        unique = list(set(trajectories))\n",
    "        counter = np.zeros(len(unique))\n",
    "\n",
    "        C_tensor = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "        cond_vector = tf.convert_to_tensor(C_tensor[None, :], dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "            for k, smi in enumerate(trajectories):\n",
    "                if not smi:  # 跳过空字符串\n",
    "                    continue\n",
    "                idx = unique.index(smi)\n",
    "                counter[idx] += 1\n",
    "                adjusted_reward = rewards[k] * (0.75 ** (counter[idx] - 1))\n",
    "\n",
    "                latent_vector = tf.random.normal(shape=(1, self.generator.latent_size))\n",
    "                tokens = tokenize(smi)\n",
    "                traj_tokens = map_tokens_to_ids(tokens)\n",
    "                seq_length = len(traj_tokens)\n",
    "\n",
    "                loss = accumulate_loss(smi, adjusted_reward, gamma, latent_vector, cond_vector, seq_length)\n",
    "                total_loss = total_loss + loss\n",
    "                total_reward += adjusted_reward\n",
    "\n",
    "#             total_loss = total_loss / tf.constant(n_batch, dtype=tf.float32)\n",
    "\n",
    "            # 计算平均损失\n",
    "            valid_samples = sum(1 for smi in trajectories if smi)\n",
    "            if valid_samples > 0:\n",
    "                total_loss = total_loss / tf.constant(valid_samples, dtype=tf.float32)\n",
    "            else:\n",
    "                return 0.0, 0.0, 0.0  # 如果没有有效样本，返回零值\n",
    "\n",
    "        # 应用梯度\n",
    "        grads = tape.gradient(total_loss, self.generator.trainable_variables)\n",
    "        if grad_clipping is not None:\n",
    "            grads = [tf.clip_by_norm(g, grad_clipping) if g is not None else None for g in grads]\n",
    "        optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
    "\n",
    "        avg_reward = total_reward / n_batch\n",
    "        return avg_reward, total_loss.numpy(), 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练强化学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    grammar = 'smiles'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "new_vocab, new_chars, new_labels, new_tokenize, new_map_tokens_to_ids = load_new_vocab('reactions_5.csv', args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迁移学习权重已加载\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [Si][Na+][N][B-]Br[nH]Br[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[Si][Na+][N][B-]Br[nH]Br[' for input: '[Si][Na+][N][B-]Br[nH]Br['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: s[N+]5c[B-1]n*#[N]=SCs[P@@][C@H].F1I[P+][H][B-1]P[N-][C@@][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES 's[N+]5c[B-1]n*#[N]=SCs[P@@][C@H].F1I[P+][H][B-1]P[N-][C@@][' for input: 's[N+]5c[B-1]n*#[N]=SCs[P@@][C@H].F1I[P+][H][B-1]P[N-][C@@]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: .[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '.[' for input: '.['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: n#[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES 'n#[' for input: 'n#['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: 7S[S@@]*[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '7S[S@@]*[' for input: '7S[S@@]*['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: )\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES ')' for input: ')'\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: In[C@@][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES 'In[C@@][' for input: 'In[C@@]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [P+][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[P+][' for input: '[P+]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [N][N-1]Cl5c#-S[Si]B7[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[N][N-1]Cl5c#-S[Si]B7[' for input: '[N][N-1]Cl5c#-S[Si]B7['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [NH3+][n+][Si][nH]C3I[S@]c[B-]N[PH+]=[N+][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[NH3+][n+][Si][nH]C3I[S@]c[B-]N[PH+]=[N+][' for input: '[NH3+][n+][Si][nH]C3I[S@]c[B-]N[PH+]=[N+]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: 5c*[N+]I[P@H][O-][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '5c*[N+]I[P@H][O-][' for input: '5c*[N+]I[P@H][O-]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: 7[BH3-]o([\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '7[BH3-]o([' for input: '7[BH3-]o(['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: 4[N-][O-]\\N[nH][O-1]3[N-]5[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '4[N-][O-]\\N[nH][O-1]3[N-]5[' for input: '4[N-][O-]\\N[nH][O-1]3[N-]5['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: s[N]p[nH][H][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES 's[N]p[nH][H][' for input: 's[N]p[nH][H]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: .C\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '.C' for input: '.C'\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [O-][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[O-][' for input: '[O-]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [P@H]5[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[P@H]5[' for input: '[P@H]5['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: PN[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES 'PN[' for input: 'PN['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: cNBn7[P@@]=[S@@][P+]6[P@H][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES 'cNBn7[P@@]=[S@@][P+]6[P@H][' for input: 'cNBn7[P@@]=[S@@][P+]6[P@H]['\n",
      "[19:51:21] SMILES Parse Error: extra close parentheses while parsing: [C@@H])[N-1][N][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[C@@H])[N-1][N][' for input: '[C@@H])[N-1][N]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [O-1][O-1][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[O-1][O-1][' for input: '[O-1][O-1]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: n[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES 'n[' for input: 'n['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [S@@][O-1][P+][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[S@@][O-1][P+][' for input: '[S@@][O-1][P+]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: s[N-]o[\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES 's[N-]o[' for input: 's[N-]o['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [N]s[N-][C@@H]S[B-1]P[Si][\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[N]s[N-][C@@H]S[B-1]P[Si][' for input: '[N]s[N-][C@@H]S[B-1]P[Si]['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: (*)[Na+]\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '(*)[Na+]' for input: '(*)[Na+]'\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [(*)Na+]\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[(*)Na+]' for input: '[(*)Na+]'\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [N(*)a+]\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[N(*)a+]' for input: '[N(*)a+]'\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [Na(*)+]\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[Na(*)+]' for input: '[Na(*)+]'\n",
      "[19:51:21] SMILES Parse Error: syntax error while parsing: [Na+(*)]\n",
      "[19:51:21] SMILES Parse Error: Failed parsing SMILES '[Na+(*)]' for input: '[Na+(*)]'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始SMILES数量: 1116\n",
      "有效SMILES数量: 1116\n",
      "无效SMILES数量: 0\n",
      "标准化生成的SMILES...\n",
      "标准化初始SMILES...\n",
      "\n",
      "统计信息:\n",
      "总生成SMILES数量: 1116\n",
      "重复SMILES数量: 0\n",
      "唯一SMILES数量: 1116\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 4. 强化学习训练循环\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 32\u001b[0m     avg_reward, rl_loss, mean_pred_val \u001b[38;5;241m=\u001b[39m \u001b[43mrl_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clipping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clipping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_tokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ 添加\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tokens_to_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_map_tokens_to_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ 添加\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOP1(Oc2c(c3c(CCCC4)c4cc(*)c3O1)c(CCCC5)c5cc2(*))=O\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOP1(Oc2c(c3c4ccccc4cc(*)c3O1)c5ccccc5cc2(*))=O\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOP1(Oc2c(*)cc3c(CCCC3)c2C4c5c(CC(C4O1)(*))cccc5)=O\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mO=P1(Oc(cc2)c(c(c(O1)cc3)c4c3cc(*)cc4)c5c2cc(*)cc5)O\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Avg Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RL Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrl_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Mean Pred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_pred_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[72], line 83\u001b[0m, in \u001b[0;36mReinforcementRandomTF.policy_gradient\u001b[0;34m(self, vocab, tokenize, map_tokens_to_ids, n_batch, gamma, std_smiles, grad_clipping, reactant_combine, sub1, sub2, learning_rate, max_length, core_list)\u001b[0m\n\u001b[1;32m     80\u001b[0m samples \u001b[38;5;241m=\u001b[39m results_unique_df\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 计算奖励\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreactant_combine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(rewards, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate_loss\u001b[39m(smile, reward, gamma, latent_vector, cond_vector, seq_length):\n",
      "Cell \u001b[0;32mIn[71], line 20\u001b[0m, in \u001b[0;36mget_reward\u001b[0;34m(smiles, surrogate_regressor, reactant_combine, invalid_reward)\u001b[0m\n\u001b[1;32m     16\u001b[0m     predicted_value \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(smiles))]  \u001b[38;5;66;03m# 假设默认值为0.0\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(smiles)):\n\u001b[0;32m---> 20\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43msmiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     22\u001b[0m         rewards[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# # 1. 初始化词表和模型\n",
    "# # 重新构建模型（用迁移学习结构）\n",
    "# model = load_and_fine_tune_model(\n",
    "#     old_vocab, new_vocab, 'model-notl-weights1.h5',\n",
    "#     X_sample=X_s, C_sample=C_s, L_sample=L_s,\n",
    "#     X_new_sample=X_new_s, C_new_sample=C_new_s, L_new_sample=L_new_s,\n",
    "# )\n",
    "\n",
    "# 加载迁移学习权重\n",
    "model.load_weights('NOTLmodel_weights2.h5')\n",
    "print(\"迁移学习权重已加载\")\n",
    "new_vocab={'#': 0, '(': 1, ')': 2, '-': 3, '.': 4, '/': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '=': 13, 'B': 14, 'Br': 15, 'C': 16, 'Cl': 17, 'E': 18, 'F': 19, 'I': 20, 'N': 21, 'O': 22, 'P': 23, 'S': 24, 'X': 25, '[ Co +2]': 26, '[ Co +]': 27, '[ Cr ]': 28, '[ Fe ]': 29, '[ Ir +]': 30, '[ Ni +2]': 31, '[ Pd +2]': 32, '[ Pd +]': 33, '[ Rh +]': 34, '[ Rh ]': 35, '[ Ru +2]': 36, '[ Ru +]': 37, '[B-1]': 38, '[B-]': 39, '[BH3-]': 40, '[C@@H]': 41, '[C@@]': 42, '[C@H]': 43, '[C@]': 44, '[C]': 45, '[H]': 46, '[K+]': 47, '[N+]': 48, '[N-1]': 49, '[N-]': 50, '[NH3+]': 51, '[N]': 52, '[Na+]': 53, '[O-1]': 54, '[O-]': 55, '[P+]': 56, '[P@@]': 57, '[P@H]': 58, '[P@]': 59, '[PH+]': 60, '[PH]': 61, '[S@@]': 62, '[S@]': 63, '[Si]': 64, '[c-]': 65, '[n+]': 66, '[nH]': 67, '\\\\': 68, 'c': 69, 'n': 70, 'o': 71, 'p': 72, 's': 73,'*': 74}                \n",
    "\n",
    "# 这个smiles是催化剂\n",
    "# smiles = results_unique_df[\"Standardized_SMILES\"].dropna().unique().tolist()\n",
    "\n",
    "# 2. 初始化强化学习训练器\n",
    "rl_trainer = ReinforcementRandomTF(\n",
    "    generator=model.decoder,\n",
    "    predictor=None,  # 如果用的话，传入预测模型实例\n",
    "    get_reward=get_reward\n",
    ")\n",
    "\n",
    "# 3. 训练参数\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "grad_clipping = 5.0\n",
    "\n",
    "# 4. 强化学习训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    avg_reward, rl_loss, mean_pred_val = rl_trainer.policy_gradient(\n",
    "        vocab=new_vocab,\n",
    "        n_batch=batch_size,\n",
    "        gamma=gamma,\n",
    "        grad_clipping=grad_clipping,\n",
    "        max_length=model.seq_length,\n",
    "        tokenize=new_tokenize,  # ✅ 添加\n",
    "        map_tokens_to_ids=new_map_tokens_to_ids,  # ✅ 添加\n",
    "        core_list = [\"OP1(Oc2c(c3c(CCCC4)c4cc(*)c3O1)c(CCCC5)c5cc2(*))=O\",\n",
    "                     \"OP1(Oc2c(c3c4ccccc4cc(*)c3O1)c5ccccc5cc2(*))=O\",\n",
    "                     \"OP1(Oc2c(*)cc3c(CCCC3)c2C4c5c(CC(C4O1)(*))cccc5)=O\",\n",
    "                     \"O=P1(Oc(cc2)c(c(c(O1)cc3)c4c3cc(*)cc4)c5c2cc(*)cc5)O\"\n",
    "                    ]\n",
    "    )\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg Reward: {avg_reward:.4f}, RL Loss: {rl_loss:.4f}, Mean Pred: {mean_pred_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate_topk_sampling对应的强化学习部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored variables from scscore15\\scscore-master\\models\\full_reaxys_model_1024bool\\model.ckpt-10654.as_numpy.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<standalone_model_numpy.SCScorer at 0x1a784c5eb20>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scscore\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 添加 scscore 路径（让 Python 能找到 SCScorer）\n",
    "sys.path.append('./scscore15/scscore-master/scscore')\n",
    "\n",
    "# ✅ 导入 SCScorer\n",
    "from standalone_model_numpy import SCScorer\n",
    "\n",
    "# ✅ 设置模型路径\n",
    "scscore_model_path = os.path.join(\n",
    "    'scscore15', 'scscore-master', 'models', 'full_reaxys_model_1024bool',\n",
    "    'model.ckpt-10654.as_numpy.json.gz'\n",
    ")\n",
    "\n",
    "# ✅ 初始化并加载模型\n",
    "scscore_model = SCScorer()\n",
    "scscore_model.restore(scscore_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_reward(smiles, invalid_reward=0.0):\n",
    "def get_reward(smiles, surrogate_regressor=None, invalid_reward=0.0):\n",
    "    \n",
    "#     molecule_smiles, predicted_value = surrogate_regressor.predictor(smiles, seed_tl, batch_size, reaction_dataset, train_aug, valid, current_path, drp_out, sigm_g) \n",
    "#     predicted_value = tensor_to_array(predicted_value)\n",
    "    # 如果有代理预测模型\n",
    "    if surrogate_regressor is not None:\n",
    "        molecule_smiles, predicted_value = surrogate_regressor.predictor(\n",
    "            smiles, seed_tl, batch_size, reaction_dataset,\n",
    "            train_aug, valid, current_path, drp_out, sigm_g\n",
    "        )\n",
    "        predicted_value = tensor_to_array(predicted_value)\n",
    "    else:\n",
    "        predicted_value = [0.0 for _ in range(len(smiles))]  # 假设默认值为0.0\n",
    "\n",
    "    rewards = np.zeros([len(smiles)])\n",
    "    for i in range(len(smiles)):\n",
    "        sample = smiles[i]\n",
    "        if sample == '':\n",
    "            rewards[i] = -2\n",
    "            continue\n",
    "        else:\n",
    "            a = sample.find('.')\n",
    "            smile_ = sample[:a]\n",
    "            mol = Chem.MolFromSmiles(smile_)\n",
    "            if mol is None:\n",
    "                rewards[i] = -2\n",
    "                continue\n",
    "            else:\n",
    "                charge = 0\n",
    "                for atom in mol.GetAtoms():\n",
    "                    atom_charge = atom.GetFormalCharge()\n",
    "                    if atom_charge != 0:\n",
    "                        charge = atom_charge \n",
    "                if charge != 0:\n",
    "                    rewards[i] = 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if Descriptors.MolWt(mol) > 1200:\n",
    "                        rewards[i] = 2\n",
    "                        continue\n",
    "                        \n",
    "                    else:\n",
    "                        sm, SCscore = scscore_model.get_score_from_smi(smile_)\n",
    "                        if SCscore > 4.0:                                                   \n",
    "                            rewards[i] = 4 + ((5.0-SCscore)*2)\n",
    "                        else:\n",
    "                            # s = int((predicted_value[i]-50)/5)\n",
    "                            s = int((1000-50)/5)\n",
    "                            rewards[i] = 6.5 + ((2*s)+1)\n",
    "   \n",
    "    return rewards\n",
    "\n",
    "def get_pred_val(smiles, surrogate_regressor):#使用代理回归预测模型对反应进行预测，得到ΔΔG的预测值。\n",
    "    generated_novel = []\n",
    "    for j in range(len(smiles)):\n",
    "        if_smile = Chem.MolFromSmiles(smiles[j])\n",
    "        if if_smile is not None:\n",
    "            generated_novel.append(smiles[j])  \n",
    "    unique_components = list(np.unique(generated_novel))\n",
    "    mol, predicted_tensor = surrogate_regressor.predictor(unique_components, seed_tl, batch_size, reaction_dataset, train_aug, valid, current_path, drp_out, sigm_g)\n",
    "    predicted_array = tensor_to_array(predicted_tensor)\n",
    "    return predicted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from functions_rl import gen_firstatom_frag, join_frag, usable_frag\n",
    "\n",
    "class ReinforcementTopkTF(object):\n",
    "    def __init__(self, generator, predictor, get_reward, get_pred_val):\n",
    "        super(ReinforcementTopkTF, self).__init__()\n",
    "        self.generator = generator  # CVAE解码器，策略函数 π_θ\n",
    "        self.predictor = predictor  # 辅助模型\n",
    "        self.get_reward = get_reward  # 奖励函数 R(s,a)\n",
    "        self.get_pred_val = get_pred_val  # 额外预测\n",
    "\n",
    "    def policy_gradient(self, vocab, tokenize, map_tokens_to_ids, \n",
    "                        n_batch=4000, gamma=0.99,\n",
    "                        std_smiles=False, grad_clipping=None,\n",
    "                        core_smi='OP1(Oc2c(c3c4ccccc4cc(*)c3O1)c5ccccc5cc2(*))=O',\n",
    "                        others='COc1ccc(cc1)/C=N/C(=O)c1ccccc1.Sc1ccccc1',\n",
    "                        learning_rate=1e-3,\n",
    "                        sub1='COC(=O)c1nc(-c2ccc(Cl)cc2F)cc(N)c1Cl', \n",
    "                        sub2='O=[N+]([O-])[O-]',\n",
    "                        max_length=100,\n",
    "                        k=5,  # 新增参数：top-k采样的k值\n",
    "                        **kwargs):\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        rl_loss = 0.0\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # 确保生成器处于训练模式\n",
    "        self.generator.trainable = True\n",
    "        \n",
    "        # 准备条件向量\n",
    "        C = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "        C = np.tile(C, (n_batch, 1))\n",
    "        start_codon = np.array([[vocab['X']]] * n_batch)\n",
    "        chars = [None] * len(vocab)\n",
    "        for k, v in vocab.items():\n",
    "            chars[v] = k\n",
    "        stop_token_id = vocab['E']\n",
    "\n",
    "        # 使用top-k采样生成样本\n",
    "        trajectories = self.generator.generate_topk_sampling(\n",
    "            C=C,\n",
    "            start_codon=start_codon,\n",
    "            char=chars,\n",
    "            k=k,  # 使用top-k采样\n",
    "            max_len=max_length,\n",
    "            return_smiles=True,\n",
    "            stop_token_id=stop_token_id\n",
    "        )\n",
    "\n",
    "        # 处理生成的样本\n",
    "        samples = []\n",
    "        for smi in trajectories:\n",
    "            if Chem.MolFromSmiles(smi) is None:\n",
    "                samples.append('')\n",
    "            else:\n",
    "                fragment = gen_firstatom_frag(smi)\n",
    "                mol = core_smi\n",
    "                for _ in range(core_smi.count('(*)')):\n",
    "                    mol = join_frag(mol, fragment)\n",
    "                    mol = usable_frag(mol)\n",
    "                full_mol = mol + '.' + others\n",
    "                samples.append(full_mol)\n",
    "\n",
    "        # 计算奖励\n",
    "        rewards = self.get_reward(samples, self.predictor, **kwargs)\n",
    "        rewards = np.array(rewards, dtype=np.float64)\n",
    "            \n",
    "        def accumulate_loss(smile, reward, gamma, latent_vector, cond_vector, seq_length):\n",
    "            tokens = tokenize(smile)\n",
    "            traj_tokens = map_tokens_to_ids(tokens)\n",
    "            traj_tensor = tf.convert_to_tensor(traj_tokens, dtype=tf.int32)\n",
    "            loss = 0.0\n",
    "            discounted = reward\n",
    "\n",
    "            batch_size = 1\n",
    "            # 初始化隐藏状态\n",
    "            hidden_state = [[\n",
    "                tf.zeros((batch_size, self.generator.unit_size)),\n",
    "                tf.zeros((batch_size, self.generator.unit_size))\n",
    "            ] for _ in range(self.generator.n_rnn_layer)]\n",
    "\n",
    "            # 逐token生成和计算损失\n",
    "            for t in range(len(traj_tokens) - 1):\n",
    "                # 当前状态：已生成的序列\n",
    "                current_input = tf.expand_dims(traj_tensor[t], axis=0)  # shape: [1, 1]\n",
    "\n",
    "                # 使用解码器预测下一个token的概率分布\n",
    "                logits, _, hidden_state = self.generator(\n",
    "                    [current_input, latent_vector, cond_vector, tf.constant([1], dtype=tf.int32)],\n",
    "                    state=hidden_state,\n",
    "                    training=True\n",
    "                )\n",
    "\n",
    "                # 计算当前时间步的损失\n",
    "                log_probs = tf.nn.log_softmax(logits[0, -1, :], axis=-1)  # 取最后一个时间步的预测\n",
    "                next_char = traj_tensor[t + 1]  # 实际的下一个token\n",
    "                loss += log_probs[next_char] * discounted\n",
    "                discounted *= gamma\n",
    "\n",
    "            return -loss\n",
    "\n",
    "        # 计算策略梯度\n",
    "        unique = list(set(trajectories))\n",
    "        counter = np.zeros(len(unique))\n",
    "\n",
    "        C_tensor = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "        cond_vector = tf.convert_to_tensor(C_tensor[None, :], dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "            for k, smi in enumerate(trajectories):\n",
    "                if not smi:  # 跳过空字符串\n",
    "                    continue\n",
    "                idx = unique.index(smi)\n",
    "                counter[idx] += 1\n",
    "                adjusted_reward = rewards[k] * (0.75 ** (counter[idx] - 1))\n",
    "\n",
    "                latent_vector = tf.random.normal(shape=(1, self.generator.latent_size))\n",
    "                tokens = tokenize(smi)\n",
    "                traj_tokens = map_tokens_to_ids(tokens)\n",
    "                seq_length = len(traj_tokens)\n",
    "\n",
    "                loss = accumulate_loss(smi, adjusted_reward, gamma, latent_vector, cond_vector, seq_length)\n",
    "                total_loss = total_loss + loss\n",
    "                total_reward += adjusted_reward\n",
    "\n",
    "            # 计算平均损失\n",
    "            valid_samples = sum(1 for smi in trajectories if smi)\n",
    "            if valid_samples > 0:\n",
    "                total_loss = total_loss / tf.constant(valid_samples, dtype=tf.float32)\n",
    "            else:\n",
    "                return 0.0, 0.0, 0.0  # 如果没有有效样本，返回零值\n",
    "\n",
    "        # 应用梯度\n",
    "        grads = tape.gradient(total_loss, self.generator.trainable_variables)\n",
    "        if grad_clipping is not None:\n",
    "            grads = [tf.clip_by_norm(g, grad_clipping) if g is not None else None for g in grads]\n",
    "        optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
    "\n",
    "        avg_reward = total_reward / n_batch\n",
    "        return avg_reward, total_loss.numpy(), 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练强化学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResourceExhaustedError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     avg_reward, rl_loss, mean_pred_val \u001b[38;5;241m=\u001b[39m \u001b[43mrl_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clipping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clipping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ 添加\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tokens_to_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_tokens_to_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ 添加\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43msub1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCOC(=O)c1nc(-c2ccc(Cl)cc2F)cc(N)c1Cl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ 指定底物1（你可以改成你的实际底物）\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43msub2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mO=[N+]([O-])[O-]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# ✅ 指定底物2（你可以改成你的实际底物）\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Avg Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RL Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrl_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Mean Pred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_pred_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 43\u001b[0m, in \u001b[0;36mReinforcementTopkTF.policy_gradient\u001b[1;34m(self, vocab, tokenize, map_tokens_to_ids, n_batch, gamma, std_smiles, grad_clipping, core_smi, others, learning_rate, sub1, sub2, max_length, k, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 使用top-k采样生成样本\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_topk_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_codon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_codon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 使用top-k采样\u001b[39;49;00m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_smiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_token_id\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 处理生成的样本\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 148\u001b[0m, in \u001b[0;36mDecoder.generate_topk_sampling\u001b[1;34m(self, C, start_codon, char, k, max_len, return_smiles, stop_token_id)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Top-k 采样\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m topk_values, topk_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [batch, k]\u001b[39;00m\n\u001b[0;32m    149\u001b[0m topk_probs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(topk_values)  \u001b[38;5;66;03m# 对 top-k 值进行 softmax\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Value for attr 'Tk' of string is not in the list of allowed values: int16, int32, int64\n\t; NodeDef: {{node TopKV2}}; Op<name=TopKV2; signature=input:T, k:Tk -> values:T, indices:index_type; attr=sorted:bool,default=true; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tk:type,default=DT_INT32,allowed=[DT_INT16, DT_INT32, DT_INT64]; attr=index_type:type,default=DT_INT32,allowed=[DT_INT16, DT_INT32, DT_INT64]> [Op:TopKV2]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     39\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mResourceExhaustedError\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m内存不足错误: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m尝试减小批量大小或使用梯度累积\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ResourceExhaustedError' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. 初始化词表和模型\n",
    "model = Model(vocab)\n",
    "\n",
    "# 2. 初始化强化学习训练器\n",
    "rl_trainer = ReinforcementTopkTF(\n",
    "    generator=model.decoder,\n",
    "    predictor=None,  # 如果用的话传入我的预测模型实例\n",
    "    get_reward=get_reward,\n",
    "    get_pred_val=get_pred_val\n",
    ")\n",
    "\n",
    "# 3. 训练参数\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "grad_clipping = 5.0\n",
    "\n",
    "# 4. 强化学习训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    try:\n",
    "        avg_reward, rl_loss, mean_pred_val = rl_trainer.policy_gradient(\n",
    "            vocab=vocab,\n",
    "            n_batch=batch_size,\n",
    "            gamma=gamma,\n",
    "            grad_clipping=grad_clipping,\n",
    "            max_length=model.seq_length,\n",
    "            tokenize=tokenize,  # ✅ 添加\n",
    "            map_tokens_to_ids=map_tokens_to_ids,  # ✅ 添加\n",
    "            k=5,\n",
    "            sub1=\"COC(=O)c1nc(-c2ccc(Cl)cc2F)cc(N)c1Cl\",  # ✅ 指定底物1（你可以改成你的实际底物）\n",
    "            sub2=\"O=[N+]([O-])[O-]\"   # ✅ 指定底物2（你可以改成你的实际底物）\n",
    "        )\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Avg Reward: {avg_reward:.4f}, RL Loss: {rl_loss:.4f}, Mean Pred: {mean_pred_val:.4f}\")\n",
    "\n",
    "        # 清理内存\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except ResourceExhaustedError as e:\n",
    "        print(f\"内存不足错误: {e}\")\n",
    "        print(\"尝试减小批量大小或使用梯度累积\")\n",
    "        break\n",
    "        \n",
    "    # 5. 根据需要保存模型\n",
    "    # model.save_weights(f\"model_epoch_{epoch+1}.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate_topp_sampling对应的强化学习部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored variables from scscore15\\scscore-master\\models\\full_reaxys_model_1024bool\\model.ckpt-10654.as_numpy.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<standalone_model_numpy.SCScorer at 0x1a780e60a00>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scscore\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 添加 scscore 路径（让 Python 能找到 SCScorer）\n",
    "sys.path.append('./scscore15/scscore-master/scscore')\n",
    "\n",
    "# ✅ 导入 SCScorer\n",
    "from standalone_model_numpy import SCScorer\n",
    "\n",
    "# ✅ 设置模型路径\n",
    "scscore_model_path = os.path.join(\n",
    "    'scscore15', 'scscore-master', 'models', 'full_reaxys_model_1024bool',\n",
    "    'model.ckpt-10654.as_numpy.json.gz'\n",
    ")\n",
    "\n",
    "# ✅ 初始化并加载模型\n",
    "scscore_model = SCScorer()\n",
    "scscore_model.restore(scscore_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_reward(smiles, invalid_reward=0.0):\n",
    "def get_reward(smiles, surrogate_regressor=None, invalid_reward=0.0):\n",
    "    \n",
    "#     molecule_smiles, predicted_value = surrogate_regressor.predictor(smiles, seed_tl, batch_size, reaction_dataset, train_aug, valid, current_path, drp_out, sigm_g) \n",
    "#     predicted_value = tensor_to_array(predicted_value)\n",
    "    # 如果有代理预测模型\n",
    "    if surrogate_regressor is not None:\n",
    "        molecule_smiles, predicted_value = surrogate_regressor.predictor(\n",
    "            smiles, seed_tl, batch_size, reaction_dataset,\n",
    "            train_aug, valid, current_path, drp_out, sigm_g\n",
    "        )\n",
    "        predicted_value = tensor_to_array(predicted_value)\n",
    "    else:\n",
    "        predicted_value = [0.0 for _ in range(len(smiles))]  # 假设默认值为0.0\n",
    "\n",
    "    rewards = np.zeros([len(smiles)])\n",
    "    for i in range(len(smiles)):\n",
    "        sample = smiles[i]\n",
    "        if sample == '':\n",
    "            rewards[i] = -2\n",
    "            continue\n",
    "        else:\n",
    "            a = sample.find('.')\n",
    "            smile_ = sample[:a]\n",
    "            mol = Chem.MolFromSmiles(smile_)\n",
    "            if mol is None:\n",
    "                rewards[i] = -2\n",
    "                continue\n",
    "            else:\n",
    "                charge = 0\n",
    "                for atom in mol.GetAtoms():\n",
    "                    atom_charge = atom.GetFormalCharge()\n",
    "                    if atom_charge != 0:\n",
    "                        charge = atom_charge \n",
    "                if charge != 0:\n",
    "                    rewards[i] = 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if Descriptors.MolWt(mol) > 1200:\n",
    "                        rewards[i] = 2\n",
    "                        continue\n",
    "                        \n",
    "                    else:\n",
    "                        sm, SCscore = scscore_model.get_score_from_smi(smile_)\n",
    "                        if SCscore > 4.0:                                                   \n",
    "                            rewards[i] = 4 + ((5.0-SCscore)*2)\n",
    "                        else:\n",
    "                            # s = int((predicted_value[i]-50)/5)\n",
    "                            s = int((1000-50)/5)\n",
    "                            rewards[i] = 6.5 + ((2*s)+1)\n",
    "   \n",
    "    return rewards\n",
    "\n",
    "def get_pred_val(smiles, surrogate_regressor):#使用代理回归预测模型对反应进行预测，得到ΔΔG的预测值。\n",
    "    generated_novel = []\n",
    "    for j in range(len(smiles)):\n",
    "        if_smile = Chem.MolFromSmiles(smiles[j])\n",
    "        if if_smile is not None:\n",
    "            generated_novel.append(smiles[j])  \n",
    "    unique_components = list(np.unique(generated_novel))\n",
    "    mol, predicted_tensor = surrogate_regressor.predictor(unique_components, seed_tl, batch_size, reaction_dataset, train_aug, valid, current_path, drp_out, sigm_g)\n",
    "    predicted_array = tensor_to_array(predicted_tensor)\n",
    "    return predicted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from functions_rl import gen_firstatom_frag, join_frag, usable_frag\n",
    "\n",
    "class ReinforcementToppTF(object):\n",
    "    def __init__(self, generator, predictor, get_reward, get_pred_val):\n",
    "        super(ReinforcementToppTF, self).__init__()\n",
    "        self.generator = generator  # CVAE解码器，策略函数 π_θ\n",
    "        self.predictor = predictor  # 辅助模型\n",
    "        self.get_reward = get_reward  # 奖励函数 R(s,a)\n",
    "        self.get_pred_val = get_pred_val  # 额外预测\n",
    "\n",
    "    def policy_gradient(self, vocab, tokenize, map_tokens_to_ids, \n",
    "                        n_batch=4000, gamma=0.99,\n",
    "                        std_smiles=False, grad_clipping=None,\n",
    "                        core_smi='OP1(Oc2c(c3c4ccccc4cc(*)c3O1)c5ccccc5cc2(*))=O',\n",
    "                        others='COc1ccc(cc1)/C=N/C(=O)c1ccccc1.Sc1ccccc1',\n",
    "                        learning_rate=1e-3,\n",
    "                        sub1='COC(=O)c1nc(-c2ccc(Cl)cc2F)cc(N)c1Cl', \n",
    "                        sub2='O=[N+]([O-])[O-]',\n",
    "                        max_length=100,\n",
    "                        p=0.95,  # 新增参数：top-p采样的p值\n",
    "                        **kwargs):\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        rl_loss = 0.0\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # 确保生成器处于训练模式\n",
    "        self.generator.trainable = True\n",
    "        \n",
    "        # 准备条件向量\n",
    "        C = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "        C = np.tile(C, (n_batch, 1))\n",
    "        start_codon = np.array([[vocab['X']]] * n_batch)\n",
    "        chars = [None] * len(vocab)\n",
    "        for k, v in vocab.items():\n",
    "            chars[v] = k\n",
    "        stop_token_id = vocab['E']\n",
    "\n",
    "        # 使用top-p采样生成样本\n",
    "        trajectories = self.generator.generate_topp_sampling(\n",
    "            C=C,\n",
    "            start_codon=start_codon,\n",
    "            char=chars,\n",
    "            p=p,  # 使用top-p采样\n",
    "            max_len=max_length,\n",
    "            return_smiles=True,\n",
    "            stop_token_id=stop_token_id\n",
    "        )\n",
    "\n",
    "        # 处理生成的样本\n",
    "        samples = []\n",
    "        for smi in trajectories:\n",
    "            if Chem.MolFromSmiles(smi) is None:\n",
    "                samples.append('')\n",
    "            else:\n",
    "                fragment = gen_firstatom_frag(smi)\n",
    "                mol = core_smi\n",
    "                for _ in range(core_smi.count('(*)')):\n",
    "                    mol = join_frag(mol, fragment)\n",
    "                    mol = usable_frag(mol)\n",
    "                full_mol = mol + '.' + others\n",
    "                samples.append(full_mol)\n",
    "\n",
    "        # 计算奖励\n",
    "        rewards = self.get_reward(samples, self.predictor, **kwargs)\n",
    "        rewards = np.array(rewards, dtype=np.float64)\n",
    "            \n",
    "        def accumulate_loss(smile, reward, gamma, latent_vector, cond_vector, seq_length):\n",
    "            tokens = tokenize(smile)\n",
    "            traj_tokens = map_tokens_to_ids(tokens)\n",
    "            traj_tensor = tf.convert_to_tensor(traj_tokens, dtype=tf.int32)\n",
    "            loss = 0.0\n",
    "            discounted = reward\n",
    "\n",
    "            batch_size = 1\n",
    "            # 初始化隐藏状态\n",
    "            hidden_state = [[\n",
    "                tf.zeros((batch_size, self.generator.unit_size)),\n",
    "                tf.zeros((batch_size, self.generator.unit_size))\n",
    "            ] for _ in range(self.generator.n_rnn_layer)]\n",
    "\n",
    "            # 逐token生成和计算损失\n",
    "            for t in range(len(traj_tokens) - 1):\n",
    "                # 当前状态：已生成的序列\n",
    "                current_input = tf.expand_dims(traj_tensor[t], axis=0)  # shape: [1, 1]\n",
    "\n",
    "                # 使用解码器预测下一个token的概率分布\n",
    "                logits, _, hidden_state = self.generator(\n",
    "                    [current_input, latent_vector, cond_vector, tf.constant([1], dtype=tf.int32)],\n",
    "                    state=hidden_state,\n",
    "                    training=True\n",
    "                )\n",
    "\n",
    "                # 计算当前时间步的损失\n",
    "                log_probs = tf.nn.log_softmax(logits[0, -1, :], axis=-1)  # 取最后一个时间步的预测\n",
    "                next_char = traj_tensor[t + 1]  # 实际的下一个token\n",
    "                loss += log_probs[next_char] * discounted\n",
    "                discounted *= gamma\n",
    "\n",
    "            return -loss\n",
    "\n",
    "        # 计算策略梯度\n",
    "        unique = list(set(trajectories))\n",
    "        counter = np.zeros(len(unique))\n",
    "\n",
    "        C_tensor = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "        cond_vector = tf.convert_to_tensor(C_tensor[None, :], dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "            for k, smi in enumerate(trajectories):\n",
    "                if not smi:  # 跳过空字符串\n",
    "                    continue\n",
    "                idx = unique.index(smi)\n",
    "                counter[idx] += 1\n",
    "                adjusted_reward = rewards[k] * (0.75 ** (counter[idx] - 1))\n",
    "\n",
    "                latent_vector = tf.random.normal(shape=(1, self.generator.latent_size))\n",
    "                tokens = tokenize(smi)\n",
    "                traj_tokens = map_tokens_to_ids(tokens)\n",
    "                seq_length = len(traj_tokens)\n",
    "\n",
    "                loss = accumulate_loss(smi, adjusted_reward, gamma, latent_vector, cond_vector, seq_length)\n",
    "                total_loss = total_loss + loss\n",
    "                total_reward += adjusted_reward\n",
    "\n",
    "            # 计算平均损失\n",
    "            valid_samples = sum(1 for smi in trajectories if smi)\n",
    "            if valid_samples > 0:\n",
    "                total_loss = total_loss / tf.constant(valid_samples, dtype=tf.float32)\n",
    "            else:\n",
    "                return 0.0, 0.0, 0.0  # 如果没有有效样本，返回零值\n",
    "\n",
    "        # 应用梯度\n",
    "        grads = tape.gradient(total_loss, self.generator.trainable_variables)\n",
    "        if grad_clipping is not None:\n",
    "            grads = [tf.clip_by_norm(g, grad_clipping) if g is not None else None for g in grads]\n",
    "        optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
    "\n",
    "        avg_reward = total_reward / n_batch\n",
    "        return avg_reward, total_loss.numpy(), 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练强化学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:14:26] SMILES Parse Error: syntax error while parsing: nn[K+][Na+]/[C@H][P+][NH3+]7[c-]1[N][Si][C@H][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES 'nn[K+][Na+]/[C@H][P+][NH3+]7[c-]1[N][Si][C@H][' for input: 'nn[K+][Na+]/[C@H][P+][NH3+]7[c-]1[N][Si][C@H]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: -[P@@][PH]N.[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '-[P@@][PH]N.[' for input: '-[P@@][PH]N.['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [C@H][N].po[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[C@H][N].po[' for input: '[C@H][N].po['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: S5[N][PH+][N-]Ps[C@@]3[PH]*[B-][N]N[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES 'S5[N][PH+][N-]Ps[C@@]3[PH]*[B-][N]N[' for input: 'S5[N][PH+][N-]Ps[C@@]3[PH]*[B-][N]N['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [O-1]XI[S@]\\[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[O-1]XI[S@]\\[' for input: '[O-1]XI[S@]\\['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 3o2[P@H][B-1][N][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '3o2[P@H][B-1][N][' for input: '3o2[P@H][B-1][N]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 3Bn.N7[C@@H]3n[N-][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '3Bn.N7[C@@H]3n[N-][' for input: '3Bn.N7[C@@H]3n[N-]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: PP[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES 'PP[' for input: 'PP['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [N+][nH][n+][C@H][P@H][C@H]12[K+][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[N+][nH][n+][C@H][P@H][C@H]12[K+][' for input: '[N+][nH][n+][C@H][P@H][C@H]12[K+]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: c[S@@][H]Br[B-1][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES 'c[S@@][H]Br[B-1][' for input: 'c[S@@][H]Br[B-1]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [P+]/[Si]2[C@@H][P@@]S[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[P+]/[Si]2[C@@H][P@@]S[' for input: '[P+]/[Si]2[C@@H][P@@]S['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [C@@]c[S@@][K+][N-1][P@@][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[C@@]c[S@@][K+][N-1][P@@][' for input: '[C@@]c[S@@][K+][N-1][P@@]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: I[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES 'I[' for input: 'I['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 1[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '1[' for input: '1['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [P+][S@]n[P@@]6\\[N]S(22[O-1][PH][n+]([NH3+]C[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[P+][S@]n[P@@]6\\[N]S(22[O-1][PH][n+]([NH3+]C[' for input: '[P+][S@]n[P@@]6\\[N]S(22[O-1][PH][n+]([NH3+]C['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 1B[n+][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '1B[n+][' for input: '1B[n+]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [Na+]X[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[Na+]X[' for input: '[Na+]X['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: X2CN[Si][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES 'X2CN[Si][' for input: 'X2CN[Si]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 73[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '73[' for input: '73['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: .[P@@]S[P@][C][N][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '.[P@@]S[P@][C][N][' for input: '.[P@@]S[P@][C][N]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 52#[nH]S)[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '52#[nH]S)[' for input: '52#[nH]S)['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: F[H][O-1][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES 'F[H][O-1][' for input: 'F[H][O-1]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 7[Na+][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '7[Na+][' for input: '7[Na+]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [N-][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[N-][' for input: '[N-]['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 7F4[C@]#=Br[Si]SO2\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '7F4[C@]#=Br[Si]SO2' for input: '7F4[C@]#=Br[Si]SO2'\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: 2[PH+][S@]ClXN#4[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '2[PH+][S@]ClXN#4[' for input: '2[PH+][S@]ClXN#4['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: nsN()1[Si][P@H][P+]O[\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES 'nsN()1[Si][P@H][P+]O[' for input: 'nsN()1[Si][P@H][P+]O['\n",
      "[15:14:26] SMILES Parse Error: syntax error while parsing: [B-][\n",
      "[15:14:26] SMILES Parse Error: Failed parsing SMILES '[B-][' for input: '[B-]['\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Avg Reward: 0.0000, RL Loss: 0.0000, Mean Pred: 0.0000\n",
      "Epoch 2/2 - Avg Reward: 0.0000, RL Loss: 0.0000, Mean Pred: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:15:37] SMILES Parse Error: syntax error while parsing: P=[C@H]CS3F[O-1]\\P[S@@][S@@]F[P@@][B-][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES 'P=[C@H]CS3F[O-1]\\P[S@@][S@@]F[P@@][B-][' for input: 'P=[C@H]CS3F[O-1]\\P[S@@][S@@]F[P@@][B-]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [c-][C@@H][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[c-][C@@H][' for input: '[c-][C@@H]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: /[O-1]X).6[S@@][P@@]S[K+][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '/[O-1]X).6[S@@][P@@]S[K+][' for input: '/[O-1]X).6[S@@][P@@]S[K+]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: F3sXS([P+][BH3-]2s/[C@@H]6[B-][P@][c-][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES 'F3sXS([P+][BH3-]2s/[C@@H]6[B-][P@][c-][' for input: 'F3sXS([P+][BH3-]2s/[C@@H]6[B-][P@][c-]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [n+]NPIO[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[n+]NPIO[' for input: '[n+]NPIO['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: *[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '*[' for input: '*['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: 2\\[n+]FsX/[c-][PH][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '2\\[n+]FsX/[c-][PH][' for input: '2\\[n+]FsX/[c-][PH]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: =*[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '=*[' for input: '=*['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [C@H]-pBr[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[C@H]-pBr[' for input: '[C@H]-pBr['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: 3O[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '3O[' for input: '3O['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: NB[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES 'NB[' for input: 'NB['\n",
      "[15:15:37] SMILES Parse Error: unclosed ring for input: 'N6c[B-1]1'\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [P@@]P6Br[C@H].-)N[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[P@@]P6Br[C@H].-)N[' for input: '[P@@]P6Br[C@H].-)N['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [K+]N[S@@][P@H]Br([H]#[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[K+]N[S@@][P@H]Br([H]#[' for input: '[K+]N[S@@][P@H]Br([H]#['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: #[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '#[' for input: '#['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [P@@][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[P@@][' for input: '[P@@]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [NH3+][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[NH3+][' for input: '[NH3+]['\n",
      "[15:15:37] SMILES Parse Error: extra close parentheses while parsing: [S@]Cl[PH+])[c-][C@H][N-1]X4=[PH+][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[S@]Cl[PH+])[c-][C@H][N-1]X4=[PH+][' for input: '[S@]Cl[PH+])[c-][C@H][N-1]X4=[PH+]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [BH3-]\\(o[N]\\IcsP[K+][c-].[C@][P@]\\S[B-1]n[P+]n\\[H]Cl[S@][H][C@@H]BrX6[P+]*[S@][PH+]F[K+]n5B[C@@H][PH][K+]2nXFo[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[BH3-]\\(o[N]\\IcsP[K+][c-].[C@][P@]\\S[B-1]n[P+]n\\[H]Cl[S@][H][C@@H]BrX6[P+]*[S@][PH+]F[K+]n5B[C@@H][PH][K+]2nXFo[' for input: '[BH3-]\\(o[N]\\IcsP[K+][c-].[C@][P@]\\S[B-1]n[P+]n\\[H]Cl[S@][H][C@@H]BrX6[P+]*[S@][PH+]F[K+]n5B[C@@H][PH][K+]2nXFo['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [N-1][Si]s[B-][B-1]5[Na+][P@H]5[S@@](3.6[P+][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[N-1][Si]s[B-][B-1]5[Na+][P@H]5[S@@](3.6[P+][' for input: '[N-1][Si]s[B-][B-1]5[Na+][P@H]5[S@@](3.6[P+]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [K+]2[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[K+]2[' for input: '[K+]2['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: /[BH3-]S-[N]=IX[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '/[BH3-]S-[N]=IX[' for input: '/[BH3-]S-[N]=IX['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: o[C@@H]*[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES 'o[C@@H]*[' for input: 'o[C@@H]*['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [C@@H][B-][C@@H][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[C@@H][B-][C@@H][' for input: '[C@@H][B-][C@@H]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[' for input: '['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: \\*\\[c-][c-].[\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '\\*\\[c-][c-].[' for input: '\\*\\[c-][c-].['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: [K+][P@@][S@]O(.Br[O-][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '[K+][P@@][S@]O(.Br[O-][' for input: '[K+][P@@][S@]O(.Br[O-]['\n",
      "[15:15:37] SMILES Parse Error: syntax error while parsing: 5.s[H][NH3+][\n",
      "[15:15:37] SMILES Parse Error: Failed parsing SMILES '5.s[H][NH3+][' for input: '5.s[H][NH3+]['\n"
     ]
    }
   ],
   "source": [
    "# 1. 初始化词表和模型\n",
    "model = Model(vocab)\n",
    "\n",
    "# 2. 初始化强化学习训练器\n",
    "rl_trainer = ReinforcementToppTF(\n",
    "    generator=model.decoder,\n",
    "    predictor=None,\n",
    "    get_reward=get_reward,\n",
    "    get_pred_val=get_pred_val\n",
    ")\n",
    "\n",
    "# 3. 训练参数\n",
    "num_epochs = 2\n",
    "batch_size = 32  # 建议使用较小的批量大小以避免内存问题\n",
    "gamma = 0.99\n",
    "grad_clipping = 5.0\n",
    "p = 0.95  # top-p 采样的概率阈值\n",
    "\n",
    "# 4. 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    avg_reward, rl_loss, mean_pred_val = rl_trainer.policy_gradient(\n",
    "        vocab=vocab,\n",
    "        n_batch=batch_size,\n",
    "        gamma=gamma,\n",
    "        grad_clipping=grad_clipping,\n",
    "        max_length=model.seq_length,\n",
    "        tokenize=tokenize,\n",
    "        map_tokens_to_ids=map_tokens_to_ids,\n",
    "        p=p,  # 指定 top-p 采样的概率阈值\n",
    "        sub1=\"COC(=O)c1nc(-c2ccc(Cl)cc2F)cc(N)c1Cl\",\n",
    "        sub2=\"O=[N+]([O-])[O-]\"\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg Reward: {avg_reward:.4f}, RL Loss: {rl_loss:.4f}, Mean Pred: {mean_pred_val:.4f}\")\n",
    "\n",
    "    # 5. 根据需要保存模型\n",
    "    # model.save_weights(f\"model_epoch_{epoch+1}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate对应的强化学习部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_reward(smiles, invalid_reward=0.0):\n",
    "def get_reward(smiles, surrogate_regressor=None, invalid_reward=0.0):\n",
    "    \n",
    "#     molecule_smiles, predicted_value = surrogate_regressor.predictor(smiles, seed_tl, batch_size, reaction_dataset, train_aug, valid, current_path, drp_out, sigm_g) \n",
    "#     predicted_value = tensor_to_array(predicted_value)\n",
    "    # 如果有代理预测模型\n",
    "    if surrogate_regressor is not None:\n",
    "        molecule_smiles, predicted_value = surrogate_regressor.predictor(\n",
    "            smiles, seed_tl, batch_size, reaction_dataset,\n",
    "            train_aug, valid, current_path, drp_out, sigm_g\n",
    "        )\n",
    "        predicted_value = tensor_to_array(predicted_value)\n",
    "    else:\n",
    "        predicted_value = [0.0 for _ in range(len(smiles))]  # 假设默认值为0.0\n",
    "\n",
    "    rewards = np.zeros([len(smiles)])\n",
    "    for i in range(len(smiles)):\n",
    "        sample = smiles[i]\n",
    "        if sample == '':\n",
    "            rewards[i] = -2\n",
    "            continue\n",
    "        else:\n",
    "            a = sample.find('.')\n",
    "            smile_ = sample[:a]\n",
    "            mol = Chem.MolFromSmiles(smile_)\n",
    "            if mol is None:\n",
    "                rewards[i] = -2\n",
    "                continue\n",
    "            else:\n",
    "                charge = 0\n",
    "                for atom in mol.GetAtoms():\n",
    "                    atom_charge = atom.GetFormalCharge()\n",
    "                    if atom_charge != 0:\n",
    "                        charge = atom_charge \n",
    "                if charge != 0:\n",
    "                    rewards[i] = 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if Descriptors.MolWt(mol) > 1200:\n",
    "                        rewards[i] = 2\n",
    "                        continue\n",
    "                        \n",
    "                    else:\n",
    "                        sm, SCscore = scscore_model.get_score_from_smi(smile_)\n",
    "                        if SCscore > 4.0:                                                   \n",
    "                            rewards[i] = 4 + ((5.0-SCscore)*2)\n",
    "                        else:\n",
    "                            # s = int((predicted_value[i]-50)/5)\n",
    "                            s = int((1000-50)/5)\n",
    "                            rewards[i] = 6.5 + ((2*s)+1)\n",
    "   \n",
    "    return rewards\n",
    "\n",
    "def get_pred_val(smiles, surrogate_regressor):#使用代理回归预测模型对反应进行预测，得到ΔΔG的预测值。\n",
    "    generated_novel = []\n",
    "    for j in range(len(smiles)):\n",
    "        if_smile = Chem.MolFromSmiles(smiles[j])\n",
    "        if if_smile is not None:\n",
    "            generated_novel.append(smiles[j])  \n",
    "    unique_components = list(np.unique(generated_novel))\n",
    "    mol, predicted_tensor = surrogate_regressor.predictor(unique_components, seed_tl, batch_size, reaction_dataset, train_aug, valid, current_path, drp_out, sigm_g)\n",
    "    predicted_array = tensor_to_array(predicted_tensor)\n",
    "    return predicted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored variables from scscore15\\scscore-master\\models\\full_reaxys_model_1024bool\\model.ckpt-10654.as_numpy.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<standalone_model_numpy.SCScorer at 0x1a707847820>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scscore\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 添加 scscore 路径（让 Python 能找到 SCScorer）\n",
    "sys.path.append('./scscore15/scscore-master/scscore')\n",
    "\n",
    "# ✅ 导入 SCScorer\n",
    "from standalone_model_numpy import SCScorer\n",
    "\n",
    "# ✅ 设置模型路径\n",
    "scscore_model_path = os.path.join(\n",
    "    'scscore15', 'scscore-master', 'models', 'full_reaxys_model_1024bool',\n",
    "    'model.ckpt-10654.as_numpy.json.gz'\n",
    ")\n",
    "\n",
    "# ✅ 初始化并加载模型\n",
    "scscore_model = SCScorer()\n",
    "scscore_model.restore(scscore_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from functions_rl import gen_firstatom_frag, join_frag, usable_frag\n",
    "\n",
    "class ReinforcementGreedyTF(object):\n",
    "    def __init__(self, generator, predictor, get_reward, get_pred_val):\n",
    "        super(ReinforcementGreedyTF, self).__init__()\n",
    "        self.generator = generator  # CVAE解码器，策略函数 π_θ\n",
    "        self.predictor = predictor  # 辅助模型\n",
    "        self.get_reward = get_reward  # 奖励函数 R(s,a)\n",
    "        self.get_pred_val = get_pred_val  # 额外预测\n",
    "\n",
    "    def policy_gradient(self, vocab, tokenize, map_tokens_to_ids, \n",
    "                        n_batch=4000, gamma=0.99,\n",
    "                        std_smiles=False, grad_clipping=None,\n",
    "                        core_smi='OP1(Oc2c(c3c4ccccc4cc(*)c3O1)c5ccccc5cc2(*))=O',\n",
    "                        others='COc1ccc(cc1)/C=N/C(=O)c1ccccc1.Sc1ccccc1',\n",
    "                        learning_rate=1e-3,\n",
    "                        sub1='COC(=O)c1nc(-c2ccc(Cl)cc2F)cc(N)c1Cl', \n",
    "                        sub2='O=[N+]([O-])[O-]',\n",
    "                        max_length=100,\n",
    "                        **kwargs):\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        rl_loss = 0.0\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # 确保生成器处于训练模式\n",
    "        self.generator.trainable = True\n",
    "        \n",
    "        # 准备条件向量\n",
    "        C = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "        C = np.tile(C, (n_batch, 1))\n",
    "        start_codon = np.array([[vocab['X']]] * n_batch)\n",
    "        chars = [None] * len(vocab)\n",
    "        for k, v in vocab.items():\n",
    "            chars[v] = k\n",
    "\n",
    "        # 生成样本 - 使用generate_molecules函数\n",
    "        trajectories = generate_molecules(\n",
    "            model=self.generator,\n",
    "            sub1=sub1,\n",
    "            sub2=sub2,\n",
    "            vocab=vocab,\n",
    "            chars=chars,\n",
    "            tokenize=tokenize,\n",
    "            map_tokens_to_ids=map_tokens_to_ids,\n",
    "            num_samples=n_batch,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        # 处理生成的样本\n",
    "        samples = []\n",
    "        for smi in trajectories:\n",
    "            if Chem.MolFromSmiles(smi) is None:\n",
    "                samples.append('')\n",
    "            else:\n",
    "                fragment = gen_firstatom_frag(smi)\n",
    "                mol = core_smi\n",
    "                for _ in range(core_smi.count('(*)')):\n",
    "                    mol = join_frag(mol, fragment)\n",
    "                    mol = usable_frag(mol)\n",
    "                full_mol = mol + '.' + others\n",
    "                samples.append(full_mol)\n",
    "\n",
    "        # 计算奖励\n",
    "        rewards = self.get_reward(samples, self.predictor, **kwargs)\n",
    "        rewards = np.array(rewards, dtype=np.float64)\n",
    "            \n",
    "        def accumulate_loss(smile, reward, gamma, latent_vector, cond_vector, seq_length):\n",
    "            tokens = tokenize(smile)\n",
    "            traj_tokens = map_tokens_to_ids(tokens)\n",
    "            traj_tensor = tf.convert_to_tensor(traj_tokens, dtype=tf.int32)\n",
    "            loss = 0.0\n",
    "            discounted = reward\n",
    "\n",
    "            batch_size = 1\n",
    "            # 初始化隐藏状态\n",
    "            hidden_state = [[\n",
    "                tf.zeros((batch_size, self.generator.unit_size)),\n",
    "                tf.zeros((batch_size, self.generator.unit_size))\n",
    "            ] for _ in range(self.generator.n_rnn_layer)]\n",
    "\n",
    "            # 逐token生成和计算损失\n",
    "            for t in range(len(traj_tokens) - 1):\n",
    "                # 当前状态：已生成的序列\n",
    "                current_input = tf.expand_dims(traj_tensor[t], axis=0)  # shape: [1, 1]\n",
    "                current_input = tf.expand_dims(current_input, axis=1)  # shape: [1, 1]\n",
    "                # 使用解码器预测下一个token的概率分布\n",
    "                logits, _, hidden_state = self.generator(\n",
    "                    [current_input, latent_vector, cond_vector, tf.constant([1], dtype=tf.int32)],\n",
    "                    state=hidden_state,\n",
    "                    training=True\n",
    "                )\n",
    "\n",
    "                # 计算当前时间步的损失\n",
    "                log_probs = tf.nn.log_softmax(logits[0, -1, :], axis=-1)  # 取最后一个时间步的预测\n",
    "                next_char = traj_tensor[t + 1]  # 实际的下一个token\n",
    "                loss += log_probs[next_char] * discounted\n",
    "                discounted *= gamma\n",
    "\n",
    "            return -loss\n",
    "\n",
    "        # 计算策略梯度\n",
    "        unique = list(set(trajectories))\n",
    "        counter = np.zeros(len(unique))\n",
    "\n",
    "        C_tensor = prepare_condition(sub1, sub2, vocab, max_length, tokenize, map_tokens_to_ids)\n",
    "        cond_vector = tf.convert_to_tensor(C_tensor[None, :], dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "            for k, smi in enumerate(trajectories):\n",
    "                if not smi:  # 跳过空字符串\n",
    "                    continue\n",
    "                idx = unique.index(smi)\n",
    "                counter[idx] += 1\n",
    "                adjusted_reward = rewards[k] * (0.75 ** (counter[idx] - 1))\n",
    "\n",
    "                latent_vector = tf.random.normal(shape=(1, self.generator.latent_size))\n",
    "                tokens = tokenize(smi)\n",
    "                traj_tokens = map_tokens_to_ids(tokens)\n",
    "                seq_length = len(traj_tokens)\n",
    "\n",
    "                loss = accumulate_loss(smi, adjusted_reward, gamma, latent_vector, cond_vector, seq_length)\n",
    "                total_loss = total_loss + loss\n",
    "                total_reward += adjusted_reward\n",
    "\n",
    "            # 计算平均损失\n",
    "            valid_samples = sum(1 for smi in trajectories if smi)\n",
    "            if valid_samples > 0:\n",
    "                total_loss = total_loss / tf.constant(valid_samples, dtype=tf.float32)\n",
    "            else:\n",
    "                return 0.0, 0.0, 0.0  # 如果没有有效样本，返回零值\n",
    "\n",
    "        # 应用梯度\n",
    "        grads = tape.gradient(total_loss, self.generator.trainable_variables)\n",
    "        if grad_clipping is not None:\n",
    "            grads = [tf.clip_by_norm(g, grad_clipping) if g is not None else None for g in grads]\n",
    "        optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
    "\n",
    "        avg_reward = total_reward / n_batch\n",
    "        return avg_reward, total_loss.numpy(), 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练强化学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 咋还内存不足了呢？？？ #####\n",
    "# 1. 初始化词表和模型\n",
    "model = Model(vocab)\n",
    "\n",
    "# 2. 初始化强化学习训练器\n",
    "rl_trainer = ReinforcementGreedyTF(\n",
    "    generator=model.decoder,\n",
    "    predictor=None,  # 如果用的话传入我的预测模型实例\n",
    "    get_reward=get_reward,\n",
    "    get_pred_val=get_pred_val\n",
    ")\n",
    "\n",
    "# 3. 训练参数\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "grad_clipping = 5.0\n",
    "\n",
    "# 4. 强化学习训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    avg_reward, rl_loss, mean_pred_val = rl_trainer.policy_gradient(\n",
    "        vocab=vocab,\n",
    "        n_batch=batch_size,\n",
    "        gamma=gamma,\n",
    "        grad_clipping=grad_clipping,\n",
    "        max_length=model.seq_length,\n",
    "        tokenize=tokenize,  # ✅ 添加\n",
    "        map_tokens_to_ids=map_tokens_to_ids,  # ✅ 添加\n",
    "        sub1=\"COC(=O)c1nc(-c2ccc(Cl)cc2F)cc(N)c1Cl\",  # ✅ 指定底物1（你可以改成你的实际底物）\n",
    "        sub2=\"O=[N+]([O-])[O-]\"   # ✅ 指定底物2（你可以改成你的实际底物）\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg Reward: {avg_reward:.4f}, RL Loss: {rl_loss:.4f}, Mean Pred: {mean_pred_val:.4f}\")\n",
    "\n",
    "    # 5. 根据需要保存模型\n",
    "    # model.save_weights(f\"model_epoch_{epoch+1}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 往后是之前遗留的，暂时不要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(vocab)\n",
    "chars += ('E',)  # End of smiles\n",
    "chars += ('X',)  # Start of smiles\n",
    "\n",
    "# 原始数据预处理用于 scaler（仅用于归一化最后一维的数值部分）\n",
    "C_train_np = C_train.numpy()\n",
    "C_val_np = C_val.numpy()\n",
    "fit_data = np.vstack([C_train_np, C_val_np])\n",
    "print('fit_data shape:', fit_data.shape)\n",
    "\n",
    "# 只对最后一列进行归一化\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(fit_data[:, -1:])  # 只对最后一列归一化\n",
    "\n",
    "trials = []\n",
    "num_iterations = 10    \n",
    "num_trials = 10\n",
    "\n",
    "for iter in range(num_iterations):\n",
    "    trial = []\n",
    "    \n",
    "    # 获取两个 SMILES 字符串和值范围\n",
    "    smiles_str1, smiles_str2, value_range = range_list2[0]\n",
    "    \n",
    "    # 将两个 SMILES 转换为序列\n",
    "    encoded_smiles1 = map_smiles_to_sequence([smiles_str1], vocab)[0]\n",
    "    encoded_smiles2 = map_smiles_to_sequence([smiles_str2], vocab)[0]\n",
    "    \n",
    "    # 生成随机值并归一化\n",
    "    random_value = np.random.uniform(value_range[0], value_range[1])\n",
    "    norm_value = scaler.transform(np.array([[random_value]]))[0][0]\n",
    "    \n",
    "    # 组合条件向量：SMILES序列1 + SMILES序列2 + 归一化后的随机值\n",
    "    C = np.concatenate([encoded_smiles1, encoded_smiles2, [norm_value]], dtype=float)\n",
    "    \n",
    "    # 确保 C 的维度正确\n",
    "    C = C.reshape(1, -1)  # 重塑为 (1, 2*seq_length+1)\n",
    "    C = np.tile(C, (num_trials, 1))  \n",
    "    \n",
    "    print('The conditions are:', C) # 打印条件C，这样可以追踪每次生成的条件。\n",
    "    # 创建起始密码子，确保维度正确\n",
    "    start_codon = np.array([np.array(list(map(vocab.get, 'X'))) for _ in range(num_trials)])\n",
    "    start_codon = start_codon.reshape(num_trials, 1)  # 重塑为 (num_trials, 1)\n",
    "    \n",
    "    # 生成分子\n",
    "    smiles_generated = []\n",
    "    x = model.generate(C, start_codon)\n",
    "    \n",
    "    # 转换为 SMILES\n",
    "    for i in range(C.shape[0]):\n",
    "        smiles_generated += [convert_to_smiles(x[i].numpy(), chars)]\n",
    "    \n",
    "    trial += list([s.split('E')[0] for s in smiles_generated])\n",
    "    trials += trial\n",
    "    print('Number of generated smiles : ', len(trial)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_condition_from_training_data(train_sub1_arr, train_sub2_arr, train_out_arr):\n",
    "    \"\"\"\n",
    "    从训练数据中随机选一条样本构造条件向量 C\n",
    "    \"\"\"\n",
    "    idx = np.random.randint(len(train_sub1_arr))  # 随机选择一个索引\n",
    "    sub1_seq = train_sub1_arr[idx]   # shape: (L_sub,)\n",
    "    sub2_seq = train_sub2_arr[idx]   # shape: (L_sub,)\n",
    "    output_val = train_out_arr[idx]  # shape: (1,)\n",
    "\n",
    "    # 拼接形成条件向量: shape = (2*L_sub + 1,)\n",
    "    C_np = np.concatenate([sub1_seq, sub2_seq, output_val])\n",
    "    C = tf.convert_to_tensor([C_np], dtype=tf.float32)  # shape: (1, 2*L_sub + 1)\n",
    "\n",
    "    return C\n",
    "C_sample = generate_condition_from_training_data(train_sub1_arr, train_sub2_arr, train_out_arr)\n",
    "print(\"Generated C (from training data):\", C_sample.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对验证集进行重构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_val, z_mean_val, z_logvar_val = model.encoder([X_val, C_val, L_val], training=False)\n",
    "# 编码器将输入的SMILES序列映射到潜在空间，得到潜在变量 z 及其均值 z_mean 和对数方差 z_logvar。\n",
    "Y_hat_val, Y_hat_logits_val, state_val = model.decoder([X_val, z_val, C_val, L_val], state=None, training=False)\n",
    "# Y_hat_val, Y_hat_logits, z_mean_val, z_log_var_val, z_val, state_val = model.call(\n",
    "                    # [X_val, C_val, L_val], training=False)# add training true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_hat_val.shape\n",
    "Y_hat_val[:5]\n",
    "# shape=(5, 300, 67),5表示有5个分子,300表示长度是300,67表示我的vocab长度是67\n",
    "# Y_hat_val = Y_hat_val.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  将 Y_hat_val 中的最大概率索引找出来\n",
    "Y_hat_val_tokens = tf.argmax(Y_hat_val, axis=-1).numpy()\n",
    "Y_hat_val_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_smiles(Y_hat_val, vocab):\n",
    "    \"\"\"\n",
    "    把Y_hat_val的token序列还原成SMILES字符串\n",
    "    \"\"\"\n",
    "    # 反向映射：index -> token\n",
    "    idx2token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # 存储还原后的SMILES列表\n",
    "    smiles_list = []\n",
    "\n",
    "    for sample in Y_hat_val:  # Y_hat_val: (batch_size, seq_len)\n",
    "        smiles = []\n",
    "        for idx in sample:\n",
    "            idx = int(idx)\n",
    "            token = idx2token.get(idx, '')  # 保险起见，未知idx默认空\n",
    "            if token in ['X', 'E']:  # 如果有Start和End标记，直接跳过\n",
    "                continue\n",
    "            smiles.append(token)\n",
    "        smiles_str = ''.join(smiles)\n",
    "        smiles_list.append(smiles_str)\n",
    "\n",
    "    return smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还原SMILES\n",
    "smiles_list = tokens_to_smiles(Y_hat_val_tokens, vocab)\n",
    "\n",
    "# 打印前5个结果\n",
    "for i, smiles in enumerate(smiles_list[:5]):\n",
    "    print(f'Sample {i+1}: {smiles}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_smiles_to_csv(smiles_list, output_file):\n",
    "    \"\"\"\n",
    "    保存SMILES列表到CSV文件，每行一个SMILES\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(smiles_list, columns=['SMILES'])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f'SMILES已保存到 {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_smiles_to_csv(smiles_list, output_file='val_predicted_smiles_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "def smiles_from_mol(mol_list):\n",
    "    \"\"\"\n",
    "    这个是你可能已经有的辅助函数，把分子对象还原成SMILES\n",
    "    \"\"\"\n",
    "    return [Chem.MolToSmiles(m) for m in mol_list]\n",
    "    \n",
    "def validity(smiles_trial):\n",
    "    \"\"\"\n",
    "    This function takes a list of smiles and returns a list of valid smiles\n",
    "    \"\"\"\n",
    "    mol_trial = [Chem.MolFromSmiles(s) for s in smiles_trial]\n",
    "    if len(mol_trial) !=0:\n",
    "        print('Fraction of valid smiles: ', 1 - mol_trial.count(None)/len(mol_trial))\n",
    "        mol_trial = [m for m in mol_trial if m is not None]\n",
    "        smiles_valid = smiles_from_mol(mol_trial)\n",
    "\n",
    "        perc_valid = len(smiles_valid)/len(smiles_trial)*100\n",
    "        print('Number of valid smiles: ', len(smiles_valid))\n",
    "        print('Percentage of valid smiles: ', perc_valid)\n",
    "\n",
    "        return smiles_valid, perc_valid\n",
    "    else:\n",
    "        print('No valid smiles')\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例调用：\n",
    "smiles_valid, perc_valid = validity(smiles_list)\n",
    "# smiles_valid是合法的SMILES，perc_valid是合法SMILES的百分比\n",
    "# 打印或者保存合法SMILES\n",
    "print(f'合法SMILES数量: {len(smiles_valid)}')\n",
    "print(f'合法SMILES比例: {perc_valid:.2f}%')\n",
    "\n",
    "# 也可以保存到CSV：\n",
    "import pandas as pd\n",
    "pd.DataFrame(smiles_valid, columns=['Valid_SMILES']).to_csv('valid_smiles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolToMolFile\n",
    "\n",
    "# 假设你有3个SMILES\n",
    "smiles_list = [\n",
    "    \"C1=CC=C2C(=C1)C=C(C3=C2C4=C(C(=CC5=CC=CC=C54)OP4l(=O)OOP3)OP(=O)(O3)O)O(OO)OO\",\n",
    "    \"CC1=CC(=CC(=C1)C(=))F)F)CC3=C3C(=C4CCCCC4=C5)C5=C6CCCCC6=CC(=C5)P(=O)(O3)O)CC7=CC(=CC(=C7)C)C(F)(F)F\",\n",
    "    \"C1=CC2=C(C=CC1=C2C2=C(C=CC4=C4C=CC(=C5)=5)OP(=O)(O3)O)C=C===CCCCCCCCCC77\"]\n",
    "\n",
    "# 逐个转换并保存为mol文件\n",
    "for i, smiles in enumerate(smiles_list, start=1):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        MolToMolFile(mol, f'smiles_{i}.mol')  # 保存为mol文件\n",
    "        print(f'SMILES {i} 已保存为 smiles_{i}.mol')\n",
    "    else:\n",
    "        print(f'SMILES {i} 无法解析为分子对象，跳过')\n",
    "\n",
    "print(\"转换完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, MolToMolFile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 你的SMILES列表\n",
    "smiles_list = [\n",
    "    \"C1=CC=C2C(=C1)C=C(C3=C2C4=C(C(=CC5=CC=CC=C54)OP(=O)(O3)O)OP(=O)(O)O)OO\",\n",
    "    \"CC1=CC(=CC(=C1)C(=)F)F)CC3=C3C(=C4CCCCC4=C5)C5=C6CCCCC6=CC(=C5)P(=O)(O3)O)CC7=CC(=CC(=C7)C)C(F)(F)F\",\n",
    "    \"C1=CC2=C(C=CC1=C2C2=C(C=CC4=C4C=CC(=C5)=5)OP(=O)(O3)O)C=C=CCCCCCCCCC77\"\n",
    "]\n",
    "\n",
    "# 逐个处理\n",
    "for i, smiles in enumerate(smiles_list, start=1):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    if mol is not None:\n",
    "        # 保存为mol文件\n",
    "        MolToMolFile(mol, f'smiles_{i}.mol')\n",
    "        print(f'SMILES {i} 已保存为 smiles_{i}.mol')\n",
    "        \n",
    "        # 绘制分子结构并显示\n",
    "        img = Draw.MolToImage(mol, size=(400, 400))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'SMILES {i}')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f'SMILES {i} 无法解析为分子对象，跳过')\n",
    "\n",
    "print(\"转换和显示完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv(\"valid_smiles.csv\")\n",
    "\n",
    "# 提取 SMILES 列\n",
    "smiles_list = df[\"Valid_SMILES\"].dropna().tolist()\n",
    "\n",
    "# 解析 SMILES 为分子对象\n",
    "mols = [Chem.MolFromSmiles(smiles) for smiles in smiles_list if Chem.MolFromSmiles(smiles) is not None]\n",
    "\n",
    "# 设置一行 5 个\n",
    "img = Draw.MolsToGridImage(mols, molsPerRow=5, subImgSize=(300, 300), legends=[f\"Mol {i+1}\" for i in range(len(mols))])\n",
    "\n",
    "# **方法 1：直接显示（适用于 Jupyter Notebook）**\n",
    "from IPython.display import display\n",
    "display(img)  # 在 Jupyter Notebook 里显示\n",
    "\n",
    "# **方法 2：转换 PIL Image 为 NumPy 数组，并用 plt.imshow() 显示**\n",
    "img_array = np.array(img)  # 转换为 NumPy 数组\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img_array)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_list2 = [(\"*c1cc2ccccc2c2c1OP(=O)(O)Oc1c(*)cc3ccccc3c1-2\",\"O=C(/N=C/c1ccc(C(F)(F)F)cc1)c1ccccc1.CCS\", (1, 1.5))]\n",
    "C = range_list2\n",
    "trials = []\n",
    "num_iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_smiles(vector, char):\n",
    "    \"\"\"\n",
    "    Convert a vector to a SMILES string\n",
    "    \"\"\"\n",
    "    list_char = list(char)\n",
    "    vector = vector.astype(int)\n",
    "    return \"\".join(map(lambda x: list_char[x], vector)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate molecules\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "\n",
    "# # 使用固定的词汇表\n",
    "# vocab =  {'#': 0, '(': 1, ')': 2, '-': 3, '.': 4, '/': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '=': 14, 'B': 15, 'Br': 16, 'C': 17, 'Cl': 18, 'E': 19, 'F': 20, 'I': 21, 'N': 22, 'O': 23, 'P': 24, 'S': 25, 'X': 26, '[ Ag +2]': 27, '[ Ag +]': 28, '[ Ag -]': 29, '[ Ag ]': 30, '[ Au ]': 31, '[ Ba +2]': 32, '[ Be +2]': 33, '[ Ca +2]': 34, '[ Ca ]': 35, '[ Ce +3]': 36, '[ Ce +4]': 37, '[ Co +2]': 38, '[ Co ]': 39, '[ Cr +3]': 40, '[ Cr +4]': 41, '[ Cr +6]': 42, '[ Cr ]': 43, '[ Cs +]': 44, '[ Cu +2]': 45, '[ Cu -]': 46, '[ Cu ]': 47, '[ Dy +3]': 48, '[ Fe +2]': 49, '[ Fe +3]': 50, '[ Fe ]': 51, '[ Ir ]': 52, '[ La +3]': 53, '[ Mg +2]': 54, '[ Mg +]': 55, '[ Mg ]': 56, '[ Mn +2]': 57, '[ Mn +4]': 58, '[ Mn ]': 59, '[ Mo ]': 60, '[ Nd +3]': 61, '[ Ni +2]': 62, '[ Ni H6-5]': 63, '[ Ni ]': 64, '[ Pd +2]': 65, '[ Pd +]': 66, '[ Pd -2]': 67, '[ Pd -4]': 68, '[ Pd -]': 69, '[ Pd ]': 70, '[ Pt +2]': 71, '[ Pt ]': 72, '[ Re ]': 73, '[ Rh +]': 74, '[ Rh ]': 75, '[ Ru +2]': 76, '[ Ru ]': 77, '[ Sc +3]': 78, '[ Ta ]': 79, '[ Ti +2]': 80, '[ Ti +3]': 81, '[ Ti +4]': 82, '[ Ti ]': 83, '[ V +2]': 84, '[ V ]': 85, '[ W ]': 86, '[ Y b+3]': 87, '[ Zn +2]': 88, '[ Zn ]': 89, '[ Zr +4]': 90, '[125I]': 91, '[2H-]': 92, '[3H]': 93, '[Al+3]': 94, '[Al+]': 95, '[Al-]': 96, '[AlH-]': 97, '[AlH2-]': 98, '[AlH3]': 99, '[AlH4-]': 100, '[AlH]': 101, '[Al]': 102, '[As]': 103, '[B+3]': 104, '[B-]': 105, '[BH-]': 106, '[BH3-]': 107, '[BH4-]': 108, '[Bi+3]': 109, '[Br+2]': 110, '[Br-]': 111, '[C+4]': 112, '[C+]': 113, '[C-]': 114, '[C@@H]': 115, '[C@@]': 116, '[C@H]': 117, '[C@]': 118, '[CH2-]': 119, '[CH]': 120, '[Cl+2]': 121, '[Cl+3]': 122, '[Cl-]': 123, '[F-]': 124, '[H+]': 125, '[H-]': 126, '[H]': 127, '[Hg]': 128, '[I+2]': 129, '[I+3]': 130, '[I-]': 131, '[IH2+]': 132, '[In+3]': 133, '[In]': 134, '[K+]': 135, '[K]': 136, '[Li+]': 137, '[Li]': 138, '[N+]': 139, '[N-]': 140, '[NH+]': 141, '[NH-]': 142, '[NH2+]': 143, '[NH2-]': 144, '[NH3+]': 145, '[NH4+]': 146, '[Na+]': 147, '[Na]': 148, '[O+]': 149, '[O-2]': 150, '[O-]': 151, '[OH-]': 152, '[P+3]': 153, '[P+]': 154, '[P-]': 155, '[PH+]': 156, '[PH2]': 157, '[PH3]': 158, '[PH4+]': 159, '[PH]': 160, '[Pb+2]': 161, '[Pb+4]': 162, '[S+]': 163, '[S-2]': 164, '[S-]': 165, '[S@@]': 166, '[S@]': 167, '[SH-]': 168, '[SH]': 169, '[SeH]': 170, '[Se]': 171, '[SiH2]': 172, '[SiH3]': 173, '[SiH4]': 174, '[SiH]': 175, '[Si]': 176, '[Sn+2]': 177, '[Sn+4]': 178, '[Sn+]': 179, '[SnH]': 180, '[Sn]': 181, '[Tl+3]': 182, '[Tl+]': 183, '[c-]': 184, '[cH-]': 185, '[n+]': 186, '[n-]': 187, '[nH+]': 188, '[nH]': 189, '[se]': 190, '\\\\': 191, 'c': 192, 'n': 193, 'o': 194, 'p': 195, 's': 196}\n",
    "\n",
    "# chars = list(vocab)\n",
    "# chars += ('E',)  # End of smiles\n",
    "# chars += ('X',)  # Start of smiles\n",
    "\n",
    "# # 原始数据预处理用于 scaler（仅用于归一化最后一维的数值部分）\n",
    "# C_train_np = C_train.numpy()\n",
    "# C_val_np = C_val.numpy()\n",
    "# fit_data = np.vstack([C_train_np, C_val_np])\n",
    "# print('fit_data shape:', fit_data.shape)\n",
    "\n",
    "# # 只对最后一列进行归一化\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(fit_data[:, -1:])  # 只对最后一列归一化\n",
    "\n",
    "# trials = []\n",
    "# num_iterations = 10\n",
    "# num_trials = 10\n",
    "\n",
    "# for iter in range(num_iterations):\n",
    "#     trial = []\n",
    "    \n",
    "#     # 获取 SMILES 字符串和值范围\n",
    "#     smiles_str, value_range = range_list2[0]\n",
    "    \n",
    "#     # 将 SMILES 转换为序列\n",
    "#     encoded_smiles = map_smiles_to_sequence([smiles_str], vocab)[0]\n",
    "    \n",
    "#     # 生成随机值并归一化\n",
    "#     random_value = np.random.uniform(value_range[0], value_range[1])\n",
    "#     norm_value = scaler.transform(np.array([[random_value]]))[0][0]\n",
    "    \n",
    "#     # 组合条件向量：SMILES序列 + 归一化后的随机值\n",
    "#     C = np.array(encoded_smiles + [norm_value], dtype=float)\n",
    "    \n",
    "#     # 确保 C 的维度正确 (100, 1)\n",
    "#     C = C.reshape(1, -1)  # 重塑为 (1, 101)\n",
    "#     C = np.tile(C, (num_trials, 1))  \n",
    "    \n",
    "#     print('The conditions are:', C) # 打印条件C，这样可以追踪每次生成的条件。\n",
    "#     # 创建起始密码子，确保维度正确\n",
    "#     start_codon = np.array([np.array(list(map(vocab.get, 'X'))) for _ in range(num_trials)])\n",
    "#     start_codon = start_codon.reshape(num_trials, 1)  # 重塑为 (num_trials, 1)\n",
    "    \n",
    "#     # 生成分子\n",
    "#     smiles_generated = []\n",
    "#     x = model.generate(C, start_codon)\n",
    "    \n",
    "#     # 转换为 SMILES\n",
    "#     for i in range(C.shape[0]):\n",
    "#         smiles_generated += [convert_to_smiles(x[i].numpy(), chars)]\n",
    "    \n",
    "#     trial += list([s.split('E')[0] for s in smiles_generated])\n",
    "#     trials += trial\n",
    "#     print('Number of generated smiles : ', len(trial))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate molecules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# 使用固定的词汇表\n",
    "vocab={'#': 0, '(': 1, ')': 2, '-': 3, '.': 4, '/': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '=': 14, 'B': 15, 'Br': 16, 'C': 17, 'Cl': 18, 'E': 19, 'F': 20, 'I': 21, 'N': 22, 'O': 23, 'P': 24, 'S': 25, 'X': 26, '[ Ag +2]': 27, '[ Ag +]': 28, '[ Ag -]': 29, '[ Ag ]': 30, '[ Au ]': 31, '[ Ba +2]': 32, '[ Be +2]': 33, '[ Ca +2]': 34, '[ Ca ]': 35, '[ Ce +3]': 36, '[ Co +2]': 37, '[ Co ]': 38, '[ Cr +3]': 39, '[ Cr +4]': 40, '[ Cr +6]': 41, '[ Cr ]': 42, '[ Cs +]': 43, '[ Cu +2]': 44, '[ Cu -]': 45, '[ Cu ]': 46, '[ Dy +3]': 47, '[ Fe +2]': 48, '[ Fe +3]': 49, '[ Fe ]': 50, '[ Ir ]': 51, '[ La +3]': 52, '[ Mg +2]': 53, '[ Mg +]': 54, '[ Mg ]': 55, '[ Mn +2]': 56, '[ Mn +4]': 57, '[ Mn ]': 58, '[ Nd +3]': 59, '[ Ni +2]': 60, '[ Ni H6-5]': 61, '[ Ni ]': 62, '[ Pd +2]': 63, '[ Pd +]': 64, '[ Pd -2]': 65, '[ Pd ]': 66, '[ Pt ]': 67, '[ Re ]': 68, '[ Rh ]': 69, '[ Ru ]': 70, '[ Sc +3]': 71, '[ Ta ]': 72, '[ Ti +2]': 73, '[ Ti +3]': 74, '[ Ti +4]': 75, '[ Ti ]': 76, '[ V +2]': 77, '[ V ]': 78, '[ W ]': 79, '[ Y b+3]': 80, '[ Zn +2]': 81, '[ Zn ]': 82, '[ Zr +4]': 83, '[125I]': 84, '[2H-]': 85, '[3H]': 86, '[Al+3]': 87, '[Al+]': 88, '[Al-]': 89, '[AlH-]': 90, '[AlH2-]': 91, '[AlH3]': 92, '[AlH4-]': 93, '[AlH]': 94, '[Al]': 95, '[As]': 96, '[B+3]': 97, '[B-]': 98, '[BH-]': 99, '[BH3-]': 100, '[BH4-]': 101, '[Bi+3]': 102, '[Br+2]': 103, '[Br-]': 104, '[C+4]': 105, '[C+]': 106, '[C-]': 107, '[C@@H]': 108, '[C@@]': 109, '[C@H]': 110, '[C@]': 111, '[CH2-]': 112, '[CH]': 113, '[Cl+2]': 114, '[Cl+3]': 115, '[Cl-]': 116, '[F-]': 117, '[H+]': 118, '[H-]': 119, '[H]': 120, '[Hg]': 121, '[I+2]': 122, '[I+3]': 123, '[I-]': 124, '[IH2+]': 125, '[In+3]': 126, '[In]': 127, '[K+]': 128, '[K]': 129, '[Li+]': 130, '[Li]': 131, '[N+]': 132, '[N-]': 133, '[NH+]': 134, '[NH-]': 135, '[NH2+]': 136, '[NH2-]': 137, '[NH3+]': 138, '[NH4+]': 139, '[Na+]': 140, '[Na]': 141, '[O+]': 142, '[O-2]': 143, '[O-]': 144, '[OH-]': 145, '[P+3]': 146, '[P+]': 147, '[P-]': 148, '[PH+]': 149, '[PH2]': 150, '[PH3]': 151, '[PH4+]': 152, '[PH]': 153, '[Pb+2]': 154, '[Pb+4]': 155, '[S+]': 156, '[S-2]': 157, '[S-]': 158, '[S@@]': 159, '[S@]': 160, '[SH-]': 161, '[SH]': 162, '[SeH]': 163, '[Se]': 164, '[SiH2]': 165, '[SiH3]': 166, '[SiH4]': 167, '[SiH]': 168, '[Si]': 169, '[Sn+2]': 170, '[Sn+4]': 171, '[Sn+]': 172, '[SnH]': 173, '[Sn]': 174, '[Tl+3]': 175, '[Tl+]': 176, '[c-]': 177, '[cH-]': 178, '[n+]': 179, '[n-]': 180, '[nH+]': 181, '[nH]': 182, '[se]': 183, '\\\\': 184, 'c': 185, 'n': 186, 'o': 187, 'p': 188, 's': 189}\n",
    "\n",
    "chars = list(vocab)\n",
    "chars += ('E',)  # End of smiles\n",
    "chars += ('X',)  # Start of smiles\n",
    "\n",
    "# 原始数据预处理用于 scaler（仅用于归一化最后一维的数值部分）\n",
    "C_train_np = C_train.numpy()\n",
    "C_val_np = C_val.numpy()\n",
    "fit_data = np.vstack([C_train_np, C_val_np])\n",
    "print('fit_data shape:', fit_data.shape)\n",
    "\n",
    "# 只对最后一列进行归一化\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(fit_data[:, -1:])  # 只对最后一列归一化\n",
    "\n",
    "trials = []\n",
    "num_iterations = 10    \n",
    "num_trials = 10\n",
    "\n",
    "for iter in range(num_iterations):\n",
    "    trial = []\n",
    "    \n",
    "    # 获取两个 SMILES 字符串和值范围\n",
    "    smiles_str1, smiles_str2, value_range = range_list2[0]\n",
    "    \n",
    "    # 将两个 SMILES 转换为序列\n",
    "    encoded_smiles1 = map_smiles_to_sequence([smiles_str1], vocab)[0]\n",
    "    encoded_smiles2 = map_smiles_to_sequence([smiles_str2], vocab)[0]\n",
    "    \n",
    "    # 生成随机值并归一化\n",
    "    random_value = np.random.uniform(value_range[0], value_range[1])\n",
    "    norm_value = scaler.transform(np.array([[random_value]]))[0][0]\n",
    "    \n",
    "    # 组合条件向量：SMILES序列1 + SMILES序列2 + 归一化后的随机值\n",
    "    C = np.concatenate([encoded_smiles1, encoded_smiles2, [norm_value]], dtype=float)\n",
    "    \n",
    "    # 确保 C 的维度正确\n",
    "    C = C.reshape(1, -1)  # 重塑为 (1, 2*seq_length+1)\n",
    "    C = np.tile(C, (num_trials, 1))  \n",
    "    \n",
    "    print('The conditions are:', C) # 打印条件C，这样可以追踪每次生成的条件。\n",
    "    # 创建起始密码子，确保维度正确\n",
    "    start_codon = np.array([np.array(list(map(vocab.get, 'X'))) for _ in range(num_trials)])\n",
    "    start_codon = start_codon.reshape(num_trials, 1)  # 重塑为 (num_trials, 1)\n",
    "    \n",
    "    # 生成分子\n",
    "    smiles_generated = []\n",
    "    x = model.generate(C, start_codon)\n",
    "    \n",
    "    # 转换为 SMILES\n",
    "    for i in range(C.shape[0]):\n",
    "        smiles_generated += [convert_to_smiles(x[i].numpy(), chars)]\n",
    "    \n",
    "    trial += list([s.split('E')[0] for s in smiles_generated])\n",
    "    trials += trial\n",
    "    print('Number of generated smiles : ', len(trial)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate molecules\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# C_train_np = C_train.numpy()\n",
    "# C_val_np = C_val.numpy()\n",
    "# fit_data = np.vstack([C_train_np, C_val_np])\n",
    "# print('fit_data shape:', fit_data.shape)  # (222+56, 2) = (278, 2)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(fit_data)\n",
    "\n",
    "# trials = []\n",
    "# num_iterations = 10\n",
    "# num_trials = 10\n",
    "\n",
    "# # parser.add_argument('--num_trials', type=int, default=3000, help='Number of trials')\n",
    "# for iter in range(num_iterations):\n",
    "#     trial = []\n",
    "#     C = []\n",
    "#     # if args.model_type == 'tuning':\n",
    "#     #     C = range_list2\n",
    "#     #     C = np.array(C).T\n",
    "#     # else:\n",
    "#     for i in range(len(range_list2)):\n",
    "#         C.append(np.random.randint(range_list2[i][0], range_list2[i][1], 1)[0])\n",
    "        \n",
    "#     print('The conditions are:', C) # 打印条件C，这样可以追踪每次生成的条件。\n",
    "\n",
    "#     # Normalize C\n",
    "#     C = np.tile(C, (num_trials, 1))\n",
    "\n",
    "#     C = np.round(scaler.transform(C), 2)\n",
    "#     start_codon = np.array([np.array(list(map(vocab.get, 'X')))for _ in range(num_trials)])\n",
    "#     smiles_generated = []\n",
    "#     x = model.generate (C, start_codon)\n",
    "\n",
    "#     # Convert to smiles usinf from utils convert_to_smiles\n",
    "#     for i in range(C.shape[0]):\n",
    "#         smiles_generated += [convert_to_smiles(x[i].numpy(), chars)]\n",
    "\n",
    "#     trial += list([s.split('E')[0] for s in smiles_generated])\n",
    "    \n",
    "\n",
    "\n",
    "#     trials += trial\n",
    "#     print('Number of generated smiles : ', len(trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存到new_molecules.csv\n",
    "df = pd.DataFrame({'SMILES': trials})\n",
    "df.to_csv('new_molecules.csv', index=False)\n",
    "\n",
    "print('All generated molecules saved to new_molecules.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 接着训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 假设 C 和 start_codon 已经被定义并准备好\n",
    "# C = np.random.random(size=(batch_size, C_dim))  # 条件向量\n",
    "# start_codon = np.array([start_index])  # 起始密码子，通常是一个整数索引\n",
    "C = np.tile(C, (num_trials, 1))\n",
    "start_codon = start_codon.reshape(num_trials, 1)\n",
    "# 调用 generate 函数生成新的化合物\n",
    "generated_compounds = model.generate(C, start_codon)\n",
    "\n",
    "# 将生成的化合物序列转换为 SMILES 或其他表示\n",
    "smiles_list = [decode_smiles(compound) for compound in generated_compounds]\n",
    "\n",
    "# 输出生成的 SMILES 字符串\n",
    "for smile in smiles_list:\n",
    "    print(\"生成的化合物 SMILES:\", smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_clean_history1.pkl', 'rb') as file:\n",
    "    history_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' model/encoder/embedding/embeddings:0': Shape mismatch.The variable shape (75, 200), and the assigned value shape (74, 200) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3. 加载权重\u001b[39;00m\n\u001b[1;32m      2\u001b[0m load_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_pre_weights1.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型权重已从 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 加载\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# # 2. 重新定义优化器（如果需要）\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model.optimizer = tf.keras.optimize1rs.Adam(learning_rate=model.lr)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 3. 继续训练模型\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/src/backend.py:4361\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[0;34m(variable, value)\u001b[0m\n\u001b[1;32m   4358\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[1;32m   4359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4360\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[0;32m-> 4361\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign value to variable ' model/encoder/embedding/embeddings:0': Shape mismatch.The variable shape (75, 200), and the assigned value shape (74, 200) are incompatible."
     ]
    }
   ],
   "source": [
    "# 3. 加载权重\n",
    "load_path = 'model_pre_weights1.h5'\n",
    "model.load_weights(load_path)\n",
    "print(f\"模型权重已从 {load_path} 加载\")\n",
    "# # 2. 重新定义优化器（如果需要）\n",
    "# model.optimizer = tf.keras.optimize1rs.Adam(learning_rate=model.lr)\n",
    "\n",
    "# 3. 继续训练模型\n",
    "history = model.train(\n",
    "    dataset_train=(X_train, Y_train, C_train, L_train),\n",
    "    dataset_val=(X_val, Y_val, C_val, L_val),\n",
    "    epochs=200,  # 设置新的训练轮数\n",
    "    patience=50  # 设置早停机制的容忍度\n",
    ")\n",
    " \n",
    "print(\"模型训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重已保存至 model_allgen_ mmd_weights17.h5\n"
     ]
    }
   ],
   "source": [
    "save_path = 'model_allgen_ mmd_weights17.h5'  # HDF5 格式保存权重文件\n",
    "\n",
    "# 保存模型权重\n",
    "model.save_weights(save_path)\n",
    "print(f\"模型权重已保存至 {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle  \n",
    "\n",
    "# 使用pickle保存history对象\n",
    "with open('training_clean_history2.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(\"History object saved using pickle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 加载权重\n",
    "load_path = 'model_weights1.h5'\n",
    "model.load_weights(load_path)\n",
    "print(f\"模型权重已从 {load_path} 加载\")\n",
    "\n",
    "# 送入encoder，获取z_mean\n",
    "latent_features,z_mean, z_logvar = model.encoder([X_train, C_train, L_train])\n",
    "latent_features_val,z_mean_val, z_logvar_val = model.encoder([X_val, C_val, L_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降维\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "latent_2d = pca.fit_transform(latent_features)\n",
    "val_latent_2d = pca.transform(latent_features_val)\n",
    "\n",
    "print(f'潜在空间表示的形状: {latent_features.shape}')  # (样本数, latent_dim)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# 训练集-黑色点\n",
    "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
    "            c='black', label='Train Set', alpha=0.7, s=20)\n",
    "\n",
    "# 验证集-蓝色点\n",
    "plt.scatter(val_latent_2d[:, 0], val_latent_2d[:, 1], \n",
    "            c='dodgerblue', label='Validation Set', alpha=0.7, s=20)\n",
    "\n",
    "plt.title('Visualization of the latent space for the training set and validation set')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 加载权重\n",
    "load_path = 'model_weights57s.h5'\n",
    "model.load_weights(load_path)\n",
    "print(f\"模型权重已从 {load_path} 加载\")\n",
    "\n",
    "# 送入encoder，获取z_mean\n",
    "latent_features,z_mean, z_logvar = model.decoder([X_train, C_train, L_train])\n",
    "latent_features_val,z_mean_val, z_logvar_val = model.decoder([X_val, C_val, L_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降维\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "latent_2d = pca.fit_transform(latent_features)\n",
    "val_latent_2d = pca.transform(latent_features_val)\n",
    "\n",
    "print(f'潜在空间表示的形状: {latent_features.shape}')  # (样本数, latent_dim)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "#可视化\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.scatter(latent_2d[:, 0], latent_2d[:, 1], alpha=0.7)\n",
    "# 训练集-黑色点\n",
    "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
    "            c='black', label='Train Set', alpha=0.7, s=20)\n",
    "# 验证集-蓝色点\n",
    "plt.scatter(val_latent_2d[:, 0], val_latent_2d[:, 1], \n",
    "            c='dodgerblue', label='Validation Set', alpha=0.7, s=20)\n",
    "\n",
    "plt.title('Visualization of the latent space for the training set and validation set')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
